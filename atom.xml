<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[张宁网·源创库]]></title>
  <link href="https://zning.me/atom.xml" rel="self"/>
  <link href="https://zning.me/"/>
  <updated>2021-08-13T18:43:55+08:00</updated>
  <id>https://zning.me/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.mweb.im/">MWeb</generator>
  
  <entry>
    <title type="html"><![CDATA[面向白嫖(Baipiao-oriented)的DevOps：将定时更新任务扔给Github Actions去做]]></title>
    <link href="https://zning.me/16288505655950.html"/>
    <updated>2021-08-13T18:29:25+08:00</updated>
    <id>https://zning.me/16288505655950.html</id>
    <content type="html"><![CDATA[
<p>最近有个需求，由于最近在某平台更新内容，其提供的订阅机制为RSS订阅分发，不过其中带有的一些额外的信息有点尴尬，并不能直接发送到其他同类型内容平台分发，因此想到了通过程序先将不适宜信息处理后，定期更新RSS文件到自己的Github Pages上，然后其他同类型内容平台获取的RSS为自己的Github Pages上托管的RSS文件。</p>

<span id="more"></span><!-- more -->

<p>于是，需求比较明确了：</p>

<ul>
<li>可以托管并执行程序，不仅可以获取RSS文件内容，又可以通过程序修改；</li>
<li>可以定时更新任务，并且根据更新任务内容push到仓库指定分支。</li>
</ul>

<p>我第一时间想起了Github Actions这个工具，众所周知，这个工具自2019年内测到向公众开放后，众多从业者将自己的博客的编译工作从本地线下扔到了Github Actions上；一些从业者所写的前端小玩意儿，都可以很轻松的通过Github Actions将工程迅速部署，提高了效率以及节约了成本。甚至一些有后端的工程，只需要寻（bai）找（piao）一个后端存储数据的地儿，就可以低成本甚至零成本搭建个人主页，这简直是一个十分有效提高生产率的东西啊！</p>

<p>不过说起来，其实Github Actions与Jenkins的功能是一致的，都是一个CI/CD的DevOps生产力工具，只不过Github Actions以其简单易用一经推出就受到了十分的欢迎。</p>

<p>虽然一直有知道这个功能，但是苦于没什么场景可以使用，今天这个需求正好让我能够使用Github Actions，零成本的解决一下需求。</p>

<p>首先，准备一下更新RSS的主程序<code>convertRSS.py</code>和<code>requirements.txt</code>：</p>

<pre><code class="language-python">import requests
import os, sys
import argparse

# the main get content codes
def get_url(url):
    # some server requests need headers, otherwise return empty
    headers = {
        &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.131 Safari/537.36&#39;,
    }
    page = requests.get(url, headers = headers).content
    # convert to str within utf-8 encoding
    page = page.decode(&#39;utf-8&#39;)
    return page

# the main convert codes
def convert_content(content):
    content = content.replace(&quot;张宁网·源创库&quot;,&quot;张宁网·源创库&quot;)
    return content

if __name__ == &#39;__main__&#39;:
    # need system args input
    arg_prsr = argparse.ArgumentParser()
    arg_prsr.add_argument(&#39;--url&#39;, required=True, type=str, default=&quot;&quot;, help = &#39;Required. Which one RSS Xml do you want to convert.&#39;)
    args = arg_prsr.parse_args(sys.argv[1:])
    url = args.url

    # place a flag to judge should write file to disk or not
    flag = False

    url_content = convert_content(get_url(url))
    url_list = url.split(&quot;/&quot;)

    if os.path.exists(url_list[-1]):
        with open(url_list[-1], &#39;r&#39;, encoding=&#39;utf-8&#39;) as file:
            if(file.read()!=url_content):
                flag = True
    else:
        # if the file dose not exist, the flag should placed True to convert first time
        flag = True

    if flag:
        # writing to disk codes
        with open(url_list[-1], &#39;w&#39;, encoding=&#39;utf-8&#39;) as file:
            file.write(url_content)
        print(&quot;The RSS Xml had update, convert done.&quot;)
        sys.exit(0)
    else:
        print(&quot;The RSS Xml have no change, convert abort.&quot;)
        sys.exit(1)
</code></pre>

<p>这里解释下我的程序的逻辑：通过系统执行传参<code>--url</code>将需要处理的RSS XML URL传值进入，后通过函数<code>get_url()</code>完成源文件的获取并转换成字符串，紧接着通过<code>convert_content()</code>函数处理替换源文件字符串。完成后紧接着判断本地是否有该文件，如果没有则直接创建，如果有则判断本地文件是否与处理替换后文件内容是否相同，如果不相同则覆盖修改本地文件完成更新，并以<code>0</code>返回值正常退出，如果相同则无需更新，以<code>1</code>返回值返回退出程序。</p>

<p>至于为什么设计这两个返回值，文末会讲到。</p>

<p>下面是该程序的<code>requirements.txt</code>，显然得，不再赘述：</p>

<pre><code class="language-text">requests==2.25.1
</code></pre>

<p>之后Github Actions创建一个仓库，把这俩文件放进去。</p>

<p>另外是需要去Settings-Developer settings-Personal access tokens（链接：<a href="https://github.com/settings/tokens">https://github.com/settings/tokens</a>）里面生成一个密钥，将这个密钥添加到仓库设置的Actions secrets（链接：<a href="https://github.com/$%7Busername%7D/$%7Breponame%7D/settings/secrets/actions">https://github.com/${username}/${reponame}/settings/secrets/actions</a>），在Actions secrets的名字即是配置文件引入的变量名，这里我的仓库叫<code>CONVERT_TOKEN</code>。</p>

<p><img src="media/16288505655950/FireShot%20Capture%20016%20-%20New%20personal%20access%20token%20-%20github.com.png" alt="FireShot Capture 016 - New personal access token - github.com"/></p>

<p><img src="media/16288505655950/FireShot%20Capture%20017%20-%20Actions%20secrets%20-%20github.com.png" alt="FireShot Capture 017 - Actions secrets - github.com"/></p>

<p>文件和密钥均备妥后，点击菜单标签Actions，后点击<code>set up a workflow yourself</code>，进入创建自己的工作流。</p>

<p>工作流创建文件使用的是YAML语法，具体使用文档可以查阅：<a href="https://docs.github.com/cn/actions">https://docs.github.com/cn/actions</a> 这里不再赘述更多用法，直接贴出本示例项目创建的工作流文件<code>convert-rss.yml</code>：</p>

<pre><code class="language-yaml">name: &#39;Convert RSS Actions&#39;

on:
  schedule:
    - cron: &#39;0,15,30,45 * * * *&#39;

jobs:
  build:

    runs-on: ubuntu-latest

    steps:
    - name: Checkout from repo
      uses: actions/checkout@main
      with:
        ref: main

    - name: Install Python latest
      uses: actions/setup-python@main
      with:
        python-version: &#39;3.x&#39;
        architecture: &#39;x64&#39;

    - name: Install dependencies
      run: |
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
    - name: Doing fetch
      run: python convertRSS.py --url https://zning.me/atom.xml

    - name: Commit files
      run: |
        git config --local user.email &quot;zhn038@gmail.com&quot;
        git config --local user.name &quot;ZNing&quot;
        git add .
        git commit -m &quot;提交RSS XML更新 $(date &quot;+%Y-%m-%d %H:%M:%S&quot;)&quot;
    - name: Push changes
      uses: ad-m/github-push-action@master
      with:
        github_token: ${{ secrets.CONVERT_TOKEN }}
        branch: main
</code></pre>

<p>Github会在该分支仓库里创建<code>.github/workflow</code>文件夹，并在里面创建工作流文件<code>convert-rss.yml</code>，文件名没有任何要求，Github Actions会对该文件夹下所有<code>.yml</code>进行解析与执行。</p>

<p>这里稍微解释一下我自己这个工作流文件所干的事情：</p>

<ul>
<li>工作流名称为<code>Convert RSS Actions</code>；</li>
<li>工作流为定时计划工作流，工作时间是每隔15分钟一次；</li>
<li>工作流build有6个任务，顺序执行，每个步骤的具体是：
<ol>
<li>检出main仓库；</li>
<li>安装最新版Python 3；</li>
<li>根据仓库是否有<code>requirements.txt</code>安装Python依赖；</li>
<li>执行Python程序，对RSS XML进行处理；</li>
<li>对更新的RSS XML进行提交；</li>
<li>对更新的RSS XML进行推送至指定分支。</li>
</ol></li>
</ul>

<p><img src="media/16288505655950/FireShot%20Capture%20009%20-%20Actions%20%C2%B7%20zning1994_github-actions-examples%20-%20github.com.png" alt="FireShot Capture 009 - Actions · zning1994_github-actions-examples - github.com"/></p>

<p><img src="media/16288505655950/FireShot%20Capture%20012%20-%20New%20File%20-%20github.com.png" alt="FireShot Capture 012 - New File - github.com"/></p>

<p>创建好后，就等着Github Actions自己按计划调起Actions工作即可，不过根据实际测试，由于这玩意儿全球从业者估计使用的都挺多的，所以实际执行时间并不会完全按照设定的时间走，一般会晚个8-12分钟。</p>

<blockquote>
<p>仓库所有内容其实都可以本地创建，其中Actions的定义文件在本地创建<code>.github/workflow</code>文件夹后放置提交推送给Github也是可以的，只不过我这边为了演示方便，就不再在本地做操作了，直接在Github网页上直接将所有步骤都解决了。</p>
</blockquote>

<p><img src="media/16288505655950/FireShot%20Capture%20019%20-%20Actions%20%C2%B7%20zning1994_github-actions-examples%20-%20github.com.png" alt="FireShot Capture 019 - Actions · zning1994_github-actions-examples - github.com"/></p>

<p><img src="media/16288505655950/FireShot%20Capture%20020%20-%20Update%20convert-rss.yml%20%C2%B7%20zning1994_github-actions-examples@b2e36a9_%20-%20github.com.png" alt="FireShot Capture 020 - Update convert-rss.yml · zning1994_github-actions-examples@b2e36a9_ - github.com"/></p>

<p>这里解释一下刚才Python返回值的问题，因为我需要区分两种状态情况，而Github Actions是可以根据返回值判断该节点运行正常与否（详见：<a href="https://docs.github.com/cn/actions/creating-actions/setting-exit-codes-for-actions">https://docs.github.com/cn/actions/creating-actions/setting-exit-codes-for-actions</a>）只要是0就认为是正常，非0则是异常，且异常不会继续执行接下来的节点。</p>

<p>而我的需求其实就正好是，如果有更新则提交并推送仓库，如果没有则终止即可。因此上面的Python程序才会设置如此的返回值。</p>

<blockquote>
<p>不过可能是我Python程序有点小问题，发现如果内容相同的时候，后台程序也会提交更新，但是这个情况出现的很随机，有的时候又比对相同1值退出，正常返回错误。目前不太清楚这是为什么，欢迎知道的大佬在评论区留言。</p>
</blockquote>

<p><img src="media/16288505655950/FireShot%20Capture%20022%20-%20%E6%8F%90%E4%BA%A4RSS%20XML%E6%9B%B4%E6%96%B0%202021-08-13%2007_50_01%20%C2%B7%20zning1994_github-actions-examples@e_%20-%20github.com.png" alt="FireShot Capture 022 - 提交RSS XML更新 2021-08-13 07_50_01 · zning1994_github-actions-examples@e_ - github.com"/></p>

<p><img src="media/16288505655950/FireShot%20Capture%20022%20-%20%E6%8F%90%E4%BA%A4RSS%20XML%E6%9B%B4%E6%96%B0%202021-08-13%2007_50_01%20%C2%B7%20zning1994_github-actions-examples@e_%20-%20github.com%20-2-.png" alt="FireShot Capture 022 - 提交RSS XML更新 2021-08-13 07_50_01 · zning1994_github-actions-examples@e_ - github.com -2-"/></p>

<p>最后，本文示例程序正常开源，协议MIT，欢迎自取使用：</p>

<p>Github：<a href="https://github.com/zning1994/github-actions-examples">https://github.com/zning1994/github-actions-examples</a></p>

<p>Gitee镜像：<a href="https://gitee.com/zning/github-actions-examples">https://gitee.com/zning/github-actions-examples</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[快速通过中转方式下载GitHub/GitLab仓库]]></title>
    <link href="https://zning.me/16237631542826.html"/>
    <updated>2021-06-15T21:19:14+08:00</updated>
    <id>https://zning.me/16237631542826.html</id>
    <content type="html"><![CDATA[
<p>近期由于众所周知的原因，GitHub又不是特别好使，已经影响到了正常的pull/clone和push操作了。不过还好我的push主要还是在自己的网站更新上，但是pull/clone可是容易让人耽误事儿的啊。为此我经常使用Gitee（原称“码云”）做仓库中转。</p>

<p>本文就简单的介绍一下如何使用Gitee中转GitHub/GiLab仓库吧。</p>

<span id="more"></span><!-- more -->

<blockquote>
<p><strong>注意：</strong> 本操作仅限于从GitHub Clone仓库操作，不能Push。如果是Push受阻，您可能需要提高一定的上网技术才能解决该问题了。</p>
</blockquote>

<p>首先，登陆<code>gitee.com</code>，没有账号可以先注册，如果有开源中国账号的可以直接关联注册。</p>

<p>登陆后，在左上角<code>+</code>处找到<code>从GitHub/GitLab导入仓库</code>，单击进入。</p>

<p><img src="media/16237631542826/%E6%88%AA%E5%B1%8F2021-06-15%2021.39.18.png" alt="截屏2021-06-15 21.39.18"/></p>

<p>在如下显示的页面，在Git仓库地址粘贴你需要clone的仓库地址，Gitee会自动去GitHub拉取信息，如果你需要拉取的仓库在Gitee已有同学拉取过且设置为开源，他还会提示你该仓库已被拉取的地址。图上我以拉取最近微服务消息队列新宠Apache Pulsar为例演示。</p>

<p><img src="media/16237631542826/16237648054426.jpg" alt=""/></p>

<blockquote>
<p><strong>注意：</strong> 他人拉取的仓库不一定是最新的，有可能人家在半年前拉取过就没再更新过，你需要自己点进去看是否是你的需要。</p>
</blockquote>

<p>如果没有需要调整或者输入的，点击<code>导入</code>按钮即可。等待Gitee给你拉取完仓库即可。</p>

<p><img src="media/16237631542826/%E6%88%AA%E5%B1%8F2021-06-15%2021.48.40.png" alt="截屏2021-06-15 21.48.40"/></p>

<p>拉取完毕，在你的主页里就有这个仓库啦~</p>

<p><img src="media/16237631542826/%E6%9C%AA%E6%A0%87%E9%A2%98-1.png" alt="未标题-1"/></p>

<p>Gitee仓库（左）和GitHub仓库（右）对比如上图。这样你在本地再clone你Gitee的仓库即可，例如我的这个仓库：</p>

<pre><code class="language-shell">git clone git@gitee.com:zning/pulsar.git
</code></pre>

<p>仓库路径右侧有个刷新图标，如果你过段时间仓库更新，需要再次clone或者需要pull的话，可以点击该按钮先更新自己的Gitee仓库，后在本地pull即可。</p>

<blockquote>
<p><strong>注意：</strong> Gitee仓库对于更新操作是，删除仓库内容重新全量克隆，而不是执行<code>git pull</code>命令，所以谨记不要在这个仓库上提交任何分支和任何修改，以免丢失信息。</p>
</blockquote>

<p>Gitee除了可以clone任意的GitHub仓库以外，也可以关联自己的GitHub账号，快速将自己GitHub仓库clone到Gitee。相当方便。</p>

<p><img src="media/16237631542826/Pasted%20Image%202021-06-15%2022-01-58.png" alt="Pasted Image 2021-06-15 22-01-58"/></p>

<blockquote>
<p><strong>PS：</strong>最后声明，这篇文章早在一年前的7月就打算写了，奈何一直拖着没交付。[Doge] 当初是答应同事要写的，现在把这个事儿补上。[Doge]</p>

<p><img src="media/16237631542826/IMG_8872.jpg" alt="IMG_8872"/></p>

<p><img src="media/16237631542826/Screenshot_2021-06-15-21-29-06-841_com.tencent.mm.jpg" alt="Screenshot_2021-06-15-21-29-06-841_com.tencent.mm"/></p>

<p>今晚文章交付了，看大佬考完试的交付物了。[Doge]</p>
</blockquote>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Google Chrome 91开始封禁10080端口 基金从业资格考试报名系统受影响 附临时解决方案]]></title>
    <link href="https://zning.me/16236378724281.html"/>
    <updated>2021-06-14T10:31:12+08:00</updated>
    <id>https://zning.me/16236378724281.html</id>
    <content type="html"><![CDATA[
<p>今日是个好日子，端午节。在这个插艾草、盼安康的日子里，本周六的基金从业考试开始打印准考证了。</p>

<p>我报名了6月的这次考试，本来到点登录打印准考证是个再平常不过的事情，结果……</p>

<span id="more"></span><!-- more -->

<p><img src="media/16236378724281/%E6%88%AA%E5%B1%8F2021-06-14%2010.17.42.png" alt="截屏2021-06-14 10.17.42"/></p>

<p>沃日？什么鬼？网站挂了？不过我发现一个很奇怪的地方，就是访问报错。<code>ERR_UNSAFE_PORT</code>，从来没见过的报错。</p>

<p>于是我网上一查，发现Google Chrome其实早在2014年就维护了一个“默认非安全端口列表”以拦截通过浏览器发起的不安全请求。这个列表有点长，其实是一些高危服务所在端口，例如22和23。但是……查了下，一堆文章提供的端口列表并没有<code>10080</code>端口啊。查了下Chrome源代码</p>

<p><a href="https://src.chromium.org/viewvc/chrome/trunk/src/net/base/net_util.cc?view=markup#l68">https://src.chromium.org/viewvc/chrome/trunk/src/net/base/net_util.cc?view=markup#l68</a> </p>

<p>上面好像也没有10080。是哪里出问题了吗？</p>

<p>后来我搜了下资料，原来Google Chrome刚刚在91版本更新了端口的Block List，将10080端口加入了。</p>

<p><img src="media/16236378724281/%E6%88%AA%E5%B1%8F2021-06-14%2010.29.46.png" alt="截屏2021-06-14 10.29.46"/></p>

<p>究其原因，是Google Chrome社区开发者早在年初就讨论的关于防止10080端口在NAT Slipstreaming 2.0攻击中被滥用的问题。四月确认更新源码封禁，并在近期91版本实施。</p>

<p><img src="media/16236378724281/%E6%88%AA%E5%B1%8F2021-06-14%2010.48.11.png" alt="截屏2021-06-14 10.48.11"/></p>

<p>比较尴尬的是，中国证券投资基金业协会基金从业资格考试报名网站就是用的10080，这就导致了文章一开头出现的错误。</p>

<p>临时的解决方案其实很简单，就是<strong>更换浏览器</strong>。虽然网上有可以直接强制打开不安全端口的方法，但是这增加了安全风险，如果不是专业人士十分不建议去跟风操作。目前低于91版本的Chrome，以及Safari和Edge还是可以使用该端口访问网站，所以影响还是可以解决的。</p>

<p>（看社区所反馈，Firefox已经跟Chrome同步封禁，因为没有具体再搜版本号因此没有写Firefox）</p>

<p>但是目前从开源社区讨论的情况来看，10080端口集体封禁是大势所趋，希望中国证券投资基金业协会早点更新下基金从业资格考试报名网站，修改下访问端口，以保证未来的考试工作正常进行。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python Django近期总结 - 2021-06-06]]></title>
    <link href="https://zning.me/16229817791761.html"/>
    <updated>2021-06-06T20:16:19+08:00</updated>
    <id>https://zning.me/16229817791761.html</id>
    <content type="html"><![CDATA[
<p>周五公司给买的键盘到货了，毕竟我自己不太会买这么贵的键盘……不过这个键盘的布局由于键数缩小到60，键位改变了不少。为了熟悉一下键盘，整理一下近期的一些经验，以飨读者。</p>

<span id="more"></span><!-- more -->

<h2 id="toc_0">使用Django REST Framework 提示<code>Could not resolve URL for hyperlinked relationship using view name &quot;user-detail&quot;</code></h2>

<p>经过查询，由于我在app里的<code>urls.py</code>里设定了namespace名称，因此在Django设置序列化的文件<code>serializers.py</code>中，需要在<code>serializers.HyperlinkedIdentityField</code>中指定<code>view_name</code>的名称，例如：</p>

<pre><code class="language-python">class UserSerializer(serializers.HyperlinkedModelSerializer):
    url = serializers.HyperlinkedIdentityField(view_name=&quot;myapp:user-detail&quot;)

    class Meta:
    model = User
    fields = (&#39;url&#39;, &#39;username&#39;)
</code></pre>

<p>至此，该问题解决。</p>

<h2 id="toc_1">Django Models 处理Select形式字段经验总结</h2>

<p>其实还是挺简单的，由于我的需求只需要在Django后台页面显示，所以只需要在Models那里处理一下就好了。由于我需要的字段比较多，因此在app目录下新建了一个文件<code>cons.py</code>专门存储字段。</p>

<p>以下是模型文件<code>models.py</code>：</p>

<pre><code class="language-python"># -*- coding: UTF-8 -*-
from django.db import models
from .cons import SALCATEGORIES
from .cons import YEAR
from .cons import MONTH
from .cons import ORG

# Create your models here.
class Salary(models.Model):
    month = models.CharField(&#39;月&#39;,max_length=50, choices=MONTH)
    year = models.CharField(&#39;年&#39;,max_length=10, choices=YEAR)
    org = models.CharField(&#39;组织名&#39;,max_length=100, choices=ORG)
    create_timestamp = models.DateTimeField(&#39;创建时间&#39;,auto_now_add=True)
    change_timestamp = models.DateTimeField(&#39;修改时间&#39;,auto_now=True)
    def __str__(self):
        return dict(YEAR)[self.year]+dict(MONTH)[self.month]+&#39;_&#39;+dict(ORG)[self.org]

    class Meta:
        db_table = &#39;salary&#39;
</code></pre>

<p><code>cons.py</code>文件引入的内容如下：</p>

<pre><code class="language-python"># -*- coding: UTF-8 -*-
SALCATEGORIES = (
    (&#39;shubas&#39;,&#39;应发 - 基本工资&#39;),
    (&#39;shupos&#39;,&#39;应发 - 岗位工资&#39;),
    (&#39;shupls&#39;,&#39;应发 - 绩效工资&#39;),
    (&#39;shuotr&#39;,&#39;应发 - 其他&#39;),
    (&#39;wihuem&#39;,&#39;扣缴 - 失业保险&#39;),
    (&#39;wihmed&#39;,&#39;扣缴 - 医疗保险&#39;),
    (&#39;wihold&#39;,&#39;扣缴 - 养老保险&#39;),
    (&#39;wihgjj&#39;,&#39;扣缴 - 公积金&#39;),
    (&#39;wihbgj&#39;,&#39;扣缴 - 补充住房公积金&#39;),
    (&#39;wihotr&#39;,&#39;扣缴 - 其他&#39;),
    (&#39;relsal&#39;,&#39;实发 - 实发工资&#39;),
    (&#39;relotr&#39;,&#39;实发 - 其他&#39;),
)

YEAR = (
    (&#39;2020&#39;,&#39;2020年&#39;),
    (&#39;2021&#39;,&#39;2021年&#39;),
    (&#39;2022&#39;,&#39;2022年&#39;),
    (&#39;2023&#39;,&#39;2023年&#39;),
    (&#39;2024&#39;,&#39;2024年&#39;),
    (&#39;2025&#39;,&#39;2025年&#39;),
    (&#39;2026&#39;,&#39;2026年&#39;),
    (&#39;2027&#39;,&#39;2027年&#39;),
    (&#39;2028&#39;,&#39;2028年&#39;),
)

MONTH = (
    (&#39;01&#39;,&#39;01月&#39;),
    (&#39;02&#39;,&#39;02月&#39;),
    (&#39;03&#39;,&#39;03月&#39;),
    (&#39;04&#39;,&#39;04月&#39;),
    (&#39;05&#39;,&#39;05月&#39;),
    (&#39;06&#39;,&#39;06月&#39;),
    (&#39;07&#39;,&#39;07月&#39;),
    (&#39;08&#39;,&#39;08月&#39;),
    (&#39;09&#39;,&#39;09月&#39;),
    (&#39;10&#39;,&#39;10月&#39;),
    (&#39;11&#39;,&#39;11月&#39;),
    (&#39;12&#39;,&#39;12月&#39;),
)

ORG = (
    (&#39;abc&#39;,&#39;中国abc公司&#39;),
    (&#39;cde&#39;,&#39;上海cde公司&#39;),
)
</code></pre>

<p>这个键值对，存储的是前面的键，显示的是后面的值</p>

<p>上面的这个形式，对于该变量的类型也是一个挺有趣的东西。众所周知<code>( )</code>是个元组的初始化标志，正常来说元组是不能直接转换为字典的，但是上面这样的变量是可以正常转换的，请见如下测试代码和输出：</p>

<pre><code class="language-python">print(type(ORG))
for i in ORG:
    print(i[0]+&quot;_&quot;+i[1])
    print(type(i))

print(type(dict(ORG)))
print(dict(ORG))
for i in dict(ORG):
    print(i)
    print(type(i))
</code></pre>

<p>输出:</p>

<pre><code class="language-shell">&lt;class &#39;tuple&#39;&gt;
abc_中国abc公司
&lt;class &#39;tuple&#39;&gt;
cde_上海cde公司
&lt;class &#39;tuple&#39;&gt;
&lt;class &#39;dict&#39;&gt;
{&#39;abc&#39;: &#39;中国abc公司&#39;, &#39;cde&#39;: &#39;上海cde公司&#39;}
abc
&lt;class &#39;str&#39;&gt;
cde
&lt;class &#39;str&#39;&gt;
</code></pre>

<p>回到实现上来。最终我需要的后台显示情况如下：</p>

<p><img src="media/16229817791761/16229839637075.jpg" alt=""/></p>

<p><img src="media/16229817791761/16229839876474.jpg" alt=""/></p>

<p>数据库表存储情况如下：</p>

<p><img src="media/16229817791761/Pasted%20Screenshot%202021-06-06%2020-54-18.png" alt="Pasted Screenshot 2021-06-06 20-54-18"/></p>

<p>这里我们介绍一下Django里模型<code>Field</code>的<code>choices</code>字段：</p>

<blockquote>
<p>一个 <a href="https://docs.python.org/3/glossary.html#term-sequence">sequence</a> 本身由正好两个项目的迭代项组成（例如 <code>[(A，B)，(A，B)...]</code> ），作为该字段的选择。如果给定了选择，它们会被 <a href="https://docs.djangoproject.com/zh-hans/3.2/ref/models/instances/#validating-objects">模型验证</a> 强制执行，默认的表单部件将是一个带有这些选择的选择框，而不是标准的文本字段。</p>

<p>每个元组中的第一个元素是要在模型上设置的实际值，第二个元素是人可读的名称。例如：</p>

<pre><code class="language-python">YEAR_IN_SCHOOL_CHOICES = [
    (&#39;FR&#39;, &#39;Freshman&#39;),
    (&#39;SO&#39;, &#39;Sophomore&#39;),
    (&#39;JR&#39;, &#39;Junior&#39;),
    (&#39;SR&#39;, &#39;Senior&#39;),
    (&#39;GR&#39;, &#39;Graduate&#39;),
]
</code></pre>

<p>一般来说，最好在模型类内部定义选择，并为每个值定义一个合适的名称的常量：</p>

<pre><code class="language-python">from django.db import models

class Student(models.Model):
    FRESHMAN = &#39;FR&#39;
    SOPHOMORE = &#39;SO&#39;
    JUNIOR = &#39;JR&#39;
    SENIOR = &#39;SR&#39;
    GRADUATE = &#39;GR&#39;
    YEAR_IN_SCHOOL_CHOICES = [
        (FRESHMAN, &#39;Freshman&#39;),
        (SOPHOMORE, &#39;Sophomore&#39;),
        (JUNIOR, &#39;Junior&#39;),
        (SENIOR, &#39;Senior&#39;),
        (GRADUATE, &#39;Graduate&#39;),
    ]
    year_in_school = models.CharField(
        max_length=2,
        choices=YEAR_IN_SCHOOL_CHOICES,
        default=FRESHMAN,
    )

    def is_upperclass(self):
        return self.year_in_school in {self.JUNIOR, self.SENIOR}
</code></pre>

<p>虽然你可以在模型类之外定义一个选择列表，然后引用它，但在模型类内定义选择和每个选择的名称，可以将所有这些信息保留在使用它的类中，并帮助引用这些选择（例如，<code>Student.SOPHOMORE</code> 将在导入 <code>Student</code> 模型的任何地方工作）。</p>

<p>你还可以将你的可用选择收集到可用于组织目的的命名组中：</p>

<pre><code class="language-text">MEDIA_CHOICES = [
    (&#39;Audio&#39;, (
            (&#39;vinyl&#39;, &#39;Vinyl&#39;),
            (&#39;cd&#39;, &#39;CD&#39;),
        )
    ),
    (&#39;Video&#39;, (
            (&#39;vhs&#39;, &#39;VHS Tape&#39;),
            (&#39;dvd&#39;, &#39;DVD&#39;),
        )
    ),
    (&#39;unknown&#39;, &#39;Unknown&#39;),
]
</code></pre>

<p>每个元组中的第一个元素是应用于该组的名称。第二个元素是一个二元元组的迭代，每个二元元组包含一个值和一个可读的选项名称。分组后的选项可与未分组的选项结合在一个单一的列表中（如本例中的 <code>&#39;unknown&#39;</code> 选项）。</p>

<p>对于每一个设置了 <code>choice</code> 的模型字段，Django 会添加一个方法来检索字段当前值的可读名称。参见数据库 API 文档中的 <a href="https://docs.djangoproject.com/zh-hans/3.2/ref/models/instances/#django.db.models.Model.get_FOO_display"><code>get_FOO_display()</code></a>。</p>

<p>请注意，选择可以是任何序列对象——不一定是列表或元组。这让你可以动态地构造选择。但是如果你发现自己把 <code>chips</code> 魔改成动态的，你可能最好使用一个合适的的带有 <a href="https://docs.djangoproject.com/zh-hans/3.2/ref/models/fields/#django.db.models.ForeignKey"><code>ForeignKey</code></a> 的数据库表。 <code>chips</code> 是用于静态数据的，如果有的话，不应该有太大的变化。</p>

<p>每当 <code>choices</code> 的顺序变动时将会创建新的迁移。</p>

<p>除非 <a href="https://docs.djangoproject.com/zh-hans/3.2/ref/models/fields/#django.db.models.Field.blank"><code>blank=False</code></a> 与 <a href="https://docs.djangoproject.com/zh-hans/3.2/ref/models/fields/#django.db.models.Field.default"><code>default</code></a> 一起设置在字段上，否则包含 <code>&quot;---------&quot;</code> 的标签将与选择框一起呈现。要覆盖这种行为，可以在 <code>choices</code> 中添加一个包含 <code>None</code> 的元组，例如 <code>(None, &#39;Your String For Display&#39;)</code> 。另外，你也可以在有意义的地方使用一个空字符串来代替 <code>None</code> ——比如在 <a href="https://docs.djangoproject.com/zh-hans/3.2/ref/models/fields/#django.db.models.CharField"><code>CharField</code></a>。</p>
</blockquote>

<p>最后，新设备镇楼压轴。这可是近几年手头设备最贵的一年了。</p>

<p><img src="media/16229817791761/IMG_8731.jpg" alt="IMG_8731"/></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[告辞了，但没有完全告辞]]></title>
    <link href="https://zning.me/16223375496772.html"/>
    <updated>2021-05-30T09:19:09+08:00</updated>
    <id>https://zning.me/16223375496772.html</id>
    <content type="html"><![CDATA[
<p>今日手续的完成，标志着我已正式从农行离职了。</p>

<p>关于离职，这是一个思前想后的结果，这是一个艰难的决定。</p>

<p>衷心感谢农行近三年来的培养和鼓励。感谢数据中心各位领导、同事的支持，共同建设运维平台提高生产运维工作效率与质量。感谢2020年在人行数研所借调期间各行同事、各厂同学的支持、协助，在疫情这个特殊时段齐心保障首次数字人民币消费券试点工作顺利完成。更感谢一起码代码、查BUG、做变更的各位大佬们，此生有幸、共愿坦途。</p>

<p>最近半年多一直没有更新过网站了，一方面是近期各种资格考试和培训把业余时间占满了，没来得及总结近期技术积累；另一方面也在考虑行业发展规划方面的事情。</p>

<p>今天这篇，除了告辞，也想简单说一下个人对于目前IT运维，主要以应用与系统层的领域的愚见。</p>

<span id="more"></span><!-- more -->

<p>当下IT运维发展，在容器、微服务的大规模使用和传统裸机、虚拟化存量维护的现状下，逐步在技术形态上两极分化：一个方向上，以科技公司、互联网公司和开源团队为主要推动力的前沿技术，将容器与微服务进行规模化、网格化，继而提出了新的运维需求——服务自治、故障自愈、智能监控，因此逐步形成了云原生、Serverless、FaaS、IoC、无代码化等新业务开发和运维方式方法与模式；另一个方向上，传统运维——指纯手工操作占比60%以上、裸机偏多、虚化资源半自动或完全手工等特征——看似没有了领域市场，然而事实上该场景在中小企业、传统行业信息科技事业部等相关职能部门仍大量存在。</p>

<p>一个事物在同一个时刻出现两极分化时，必然会在某个节点选择方向。就应用系统层运维来说，目前节点就是前沿技术发展替代传统方式的爆发起点之时。只不过这个爆发点对于科技互联网企业，早在五年前就启动了。其他领域的传导没有那么快，也是十分正常的。毕竟对于科技互联网企业，技术的革新和快速迭代是他们的主要营收业务，但对于其他企业来说这只是业务支持，按照“互联网黑话”的说法就叫“IT赋能”。</p>

<p>当下前沿技术发展的路径，其实总体来看，是“小而美”的发展策略。要说“小而不美”那是前五年的话，近几年提出的自治、自愈，以及Serverless、无代码化技术的正式投产，是从“不美”到“美”的一种“转型宣言”。一个微服务集群就像是一个城池里的各种角色的人，即亚当·斯密在《国富论》总结提到的工业革命后的社会化分工，每个人在整个社会体系里面专业化的干一件事，将生产力以指数级提升，国民生产总值不断累积加，共同建设发展同一座城池。一个个容器或pods只干一两件事，他们之间相互通讯交流，共同推动了整个业务的正常运转。</p>

<p>在第一次、第二次软件工程危机前的面向过程编程方法兴盛之时，其实这也是“小而美”的时代，但由于业务的发展，单个文件的“小而美”被人为变”大“了。危机后，面向对象编程方法将“小而美”细化到代码内部实现，而对外一个单体程序则是庞然大物。而现阶段的”小而美“则是把代码内部实现和功能最小化，但是放眼到整个系统来看则是把复杂性上升到了系统层面。这么来看，历代软件工程的革新都想从简从美改善，但最终只不过将无序熵从一个领域转为另外一个领域罢了。从这个角度来看，前沿技术或许也不是什么前沿技术，更像是高智商人群设计的游戏规则，各种矛盾转移的”甩锅内卷大赛“。当然，这种看法有点戏谑了。</p>

<p>而这种发展路径，必然将无序熵从开发人员转移到了运维人员。原来的软件工程的开发模式——面向过程到面向对象——是一种开发人员的内部优化，现在的技术革新解放开发人员的同时改变运维人员的工作模式——运维人员也需要相当的代码功底才能做好运维工作，因为一个容器或pod所关联的，仅仅是一个功能，甚至是一个方法或函数的代码，这是一种操作系统与代码强绑定的关系。也由此，SRE发展出了DevOps，DevOps发展出了多个业态方向。</p>

<p>所以，现在的IT运维，尤其是应用与系统层领域，成了一个两面包夹芝士：开发人员由于前沿技术革新，不断提出运维新要求；用户、业务方或领导由于对前沿技术革新并不完全理解，对于运维人员工作内容上的忍耐度越来越低。</p>

<p>解决这样的窘境，可能只有开发运维干一件事才能解决，那就是共同声明：我们联合！</p>

<p>说是简单说点，结果写到这发现扯了这么多。以后公众号将持续更新，去年开的几个栏目：“源产控”系列、“慧响悦书”系列，以及正常的技术总结、日常评谈，都会持续跟进。至于更新频率嘛我尽量完成周更。</p>

<p>所以，没有完全告辞之意就在此了。欢迎关注慧响公众号，一起交流~不见不散~</p>

<p align="right">2021年5月28日 于上海陆家嘴</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[解决Elasticsearch SQL命令行启动报错 ./x-pack-env: No such file or directory]]></title>
    <link href="https://zning.me/15951488866193.html"/>
    <updated>2020-07-19T16:54:46+08:00</updated>
    <id>https://zning.me/15951488866193.html</id>
    <content type="html"><![CDATA[
<p>今天在云服务器上，通过下载源码包完成有认证配置的Elasticsearch部署后，准备执行如下命令进入Elasticsearch SQL进行一些SQL语句的操作：</p>

<span id="more"></span><!-- more -->

<pre><code class="language-shell">bin/elasticsearch-sql-cli uri=http://elastic:ESabc+2333@10.66.66.2:9200/
</code></pre>

<p>但执行后，发现使用<code>bin/elasticsearch-sql-cli</code>登录时，出错提示<code>./x-pack-env: No such file or directory</code>，莫名其妙，遂即查看下该脚本，内容如下：</p>

<pre><code class="language-shell">#!/bin/bash

# Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one
# or more contributor license agreements. Licensed under the Elastic License;
# you may not use this file except in compliance with the Elastic License.

source &quot;`dirname &quot;$0&quot;`&quot;/elasticsearch-env

source &quot;`dirname &quot;$0&quot;`&quot;/x-pack-env

CLI_JAR=$(ls $ES_HOME/bin/elasticsearch-sql-cli-*.jar)

exec \
  &quot;$JAVA&quot; \
  -jar &quot;$CLI_JAR&quot; \
  &quot;$@&quot;
</code></pre>

<p>其中有效行第二行的<code>source &quot;`dirname &quot;$0&quot;`&quot;/x-pack-env</code>看来执行起来有问题，估计是在依赖导入生效时找不到路径。经过一番折腾，解决了该问题，即将该行替换为如下命令：</p>

<pre><code class="language-shell"># source &quot;`dirname &quot;$0&quot;`&quot;/x-pack-env
source /usr/share/elasticsearch/bin/x-pack-env
</code></pre>

<p>即将引入依赖的路径写死即可，使用时请根据实际路径修改<code>x-pack-env</code>的指向路径。</p>

<p>修改完后，再用刚才的命令启动Elasticsearch SQL，正常进入，完美结局。</p>

<p>后来又仔细查找了下资料，发现该问题是Elasticsearch 7.4版本的一个小bug，在Elasticsearch 7.8的包内，<code>bin/elasticsearch-sql-cli</code>这个脚本内容已经修改为：</p>

<pre><code class="language-shell">#!/bin/bash

# Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one
# or more contributor license agreements. Licensed under the Elastic License;
# you may not use this file except in compliance with the Elastic License.

source &quot;`dirname &quot;$0&quot;`&quot;/elasticsearch-env

source &quot;$ES_HOME&quot;/bin/x-pack-env

CLI_JAR=$(ls &quot;$ES_HOME&quot;/bin/elasticsearch-sql-cli-*.jar)

exec \
  &quot;$JAVA&quot; \
  &quot;$XSHARE&quot; \
  -jar &quot;$CLI_JAR&quot; \
  &quot;$@&quot;
</code></pre>

<p>也就是说，如果Elasticsearch 7.8安装时，操作系统环境变量内<code>$ES_HOME</code>能够正确设置的话，就可以正常启动Elasticsearch SQL了。这样比直接修改产品本身脚本来说要规范的多了。</p>

<h2 id="toc_0">参考文献</h2>

<ol>
<li><a href="https://stackoverflow.com/questions/59420831/elasticsearch-7-sql-cli-x-pack-env-no-such-file-or-directory-error">Elasticsearch 7 SQL CLI : ./x-pack-env: No such file or directory error - Stack Overflow</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[“源产控”系列（三）在CentOS 8上使用Elastic Stack: Elasticsearch/Kibana 7.8的部署与认证配置]]></title>
    <link href="https://zning.me/15950655748460.html"/>
    <updated>2020-07-18T17:46:14+08:00</updated>
    <id>https://zning.me/15950655748460.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>本篇为慧响技术角“源产控”专题系列第3篇文章。</p>
</blockquote>

<!-- -->

<blockquote>
<p>慧响技术角“源产控”专题，将聚焦开源、国产化、自主可控三个方向的技术，以操作系统、中间件、数据库、程序应用等为粗分类，更新相关技术的发展趋势、探究技术核心的深度使用、系统总结技术整体架构，为对相关技术的学习者提供可观的资料，亦为个人同步学习总结的笔记，以飨读者。</p>
</blockquote>

<p>本篇对在CentOS 8上使用Elastic Stack套件中的Elasticsearch、Kibana进行简要总结，对Elasticsearch 7.8.0的部署、认证设置与Kibana 7.8.0的配套部署进行了详细总结。未来对在CentOS 8上使用Elastic Stack相关套件，将陆续更新其使用总结、性能调优等方面的系列文章，敬请期待。</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">Elastic Stack介绍</a>
</li>
<li>
<a href="#toc_1">Elasticsearch 7.8的部署</a>
<ul>
<li>
<a href="#toc_2">方式一：<code>YUM</code>方式安装</a>
</li>
<li>
<a href="#toc_3">方式二：下载<code>RPM</code>包手工安装</a>
</li>
<li>
<a href="#toc_4">方式三：源码安装</a>
</li>
</ul>
</li>
<li>
<a href="#toc_5">Elasticsearch 7.8的配置</a>
</li>
<li>
<a href="#toc_6">Elasticsearch 7.8的认证功能配置</a>
</li>
<li>
<a href="#toc_7">Kibana的部署与配置</a>
</li>
</ul>


<h2 id="toc_0">Elastic Stack介绍</h2>

<p>提起Elastic Stack，就不得不提到ELK。ELK是三个开源软件的缩写，分别为：Elasticsearch 、 Logstash以及Kibana , 它们都是开源软件。不过现在还新增了一个Beats，它是一个轻量级的日志收集处理工具(Agent)，Beats占用资源少，适合于在各个服务器上搜集日志后传输给Logstash，官方也推荐此工具，目前由于原本的ELK Stack成员中加入了Beats工具所以已改名为Elastic Stack。</p>

<p>Elastic Stack包含：</p>

<ul>
<li><p>Elasticsearch是个开源分布式搜索引擎，提供搜集、分析、存储数据三大功能。它的特点有：分布式，零配置，自动发现，索引自动分片，索引副本机制，RESTful风格接口，多数据源，自动搜索负载等。详细可参考Elasticsearch权威指南。</p></li>
<li><p>Logstash主要是用来日志的搜集、分析、过滤日志的工具，支持大量的数据获取方式。一般工作方式为C/S架构，client端安装在需要收集日志的主机上，Server端负责将收到的各节点日志进行过滤、修改等操作在一并发往Elasticsearch上去。</p></li>
<li><p>Kibana也是一个开源和免费的工具，Kibana可以为 Logstash 和 Elasticsearch 提供的日志分析友好的 Web 界面，可以帮助汇总、分析和搜索重要数据日志。</p></li>
<li><p>Beats在这里是一个轻量级日志采集器，其实Beats家族有6个成员，早期的ELK架构中使用Logstash收集、解析日志，但是Logstash对内存、CPU、I/O等资源消耗比较高。相比 Logstash，Beats所占系统的CPU和内存几乎可以忽略不计。</p></li>
</ul>

<h2 id="toc_1">Elasticsearch 7.8的部署</h2>

<h3 id="toc_2">方式一：<code>YUM</code>方式安装</h3>

<p>输入如下命令，下载安装公共签名证书：</p>

<pre><code class="language-shell">rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch
</code></pre>

<p>在目录<code>/etc/yum.repos.d/</code>下新建文件<code>elasticsearch.repo</code>，文件内容填写如下：</p>

<pre><code class="language-repo">[elasticsearch]
name=Elasticsearch repository for 7.x packages
baseurl=https://artifacts.elastic.co/packages/7.x/yum
gpgcheck=1
gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch
enabled=0
autorefresh=1
type=rpm-md
</code></pre>

<p>添加好后，直接执行<code>yum -y install --enablerepo=elasticsearch elasticsearch</code>安装即可。</p>

<h3 id="toc_3">方式二：下载<code>RPM</code>包手工安装</h3>

<p>执行如下命令，进行安装：</p>

<pre><code class="language-shell">wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.8.0-x86_64.rpm;
wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.8.0-x86_64.rpm.sha512;
shasum -a 512 -c elasticsearch-7.8.0-x86_64.rpm.sha512;
rpm --install elasticsearch-7.8.0-x86_64.rpm;
</code></pre>

<blockquote>
<p><strong>注意：</strong>若<code>shasum</code>提示找不到命令，请输入<code>yum -y install perl-Digest-SHA</code>进行依赖安装。</p>
</blockquote>

<!-- -->

<blockquote>
<p><strong>提示：</strong>鉴于Elastic网站为国外，下载速度极慢，可选择国内镜像地址，例如华为云的镜像，下载地址：<a href="https://mirrors.huaweicloud.com/elasticsearch/7.8.0/">https://mirrors.huaweicloud.com/elasticsearch/7.8.0/</a></p>
</blockquote>

<p>安装完成，系统提示如下：</p>

<pre><code class="language-shell">### NOT starting on installation, please execute the following statements to configure elasticsearch service to start automatically using systemd
 sudo systemctl daemon-reload
 sudo systemctl enable elasticsearch.service
### You can start elasticsearch service by executing
 sudo systemctl start elasticsearch.service
Created elasticsearch keystore in /etc/elasticsearch/elasticsearch.keystore
[/usr/lib/tmpfiles.d/elasticsearch.conf:1] Line references path below legacy directory /var/run/, updating /var/run/elasticsearch → /run/elasticsearch; please update the tmpfiles.d/ drop-in file accordingly.
</code></pre>

<h3 id="toc_4">方式三：源码安装</h3>

<p>如有更为灵活的需求，可通过官网下载Elasticsearch源码包进行部署。首先需建立用户属组：</p>

<pre><code class="language-shell">groupadd -g 888 elasticsearch;
useradd -g elasticsearch -m -u 888 elasticsearch;
</code></pre>

<p>后通过官网或镜像下载地址下载，解压到需要部署的文件夹：</p>

<pre><code class="language-shell">wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.8.0-linux-x86_64.tar.gz;
tar -zxvf elasticsearch-7.8.0-linux-x86_64.tar.gz -C /usr/share/;
mv /usr/share/elasticsearch-7.8.0/ /usr/share/elasticsearch;
chown -R elasticsearch:elasticsearch /usr/share/elasticsearch;
</code></pre>

<h2 id="toc_5">Elasticsearch 7.8的配置</h2>

<p>部署完成后就进入配置环节了，在配置之前我们需要知道的是进程文件和配置文件所在地。通过上述方式一、二安装，进程文件路径在<code>/usr/share/elasticsearch</code>，配置文件路径在<code>/etc/elasticsearch</code>。通过上述方式三安装，进程文件路径在解压文件路径，例如示例给出的<code>/usr/share/elasticsearch</code>，配置文件在进程文件路径下的<code>config</code>文件夹内。</p>

<p>在配置文件路径下有一个文件叫<code>jvm.options</code>，修改其中的<code>-Xms</code>、<code>-Xmx</code>两行参数来调整jvm的初始化堆大小以及最大堆大小。该值建议设置为操作系统内存的40%~50%。注：<code>-Xms</code>与<code>-Xmx</code>相同。例：若虚拟机内存为8G，可设置为如下参数：</p>

<pre><code class="language-options">-Xms4g
-Xmx4g
</code></pre>

<p>或者</p>

<pre><code class="language-options">-Xms4096m
-Xmx4096m
</code></pre>

<p>在配置文件路径下有一个文件叫<code>elasticsearch.yml</code>文件，对该文件进行配置以使集群启动：</p>

<pre><code class="language-yaml"># 集群名称，集群中所有节点统一
cluster.name: Elasticsearch-Cluster
# 当前节点名，自定义但集群内不允许重复
node.name: node-1
node.attr.rack: r1
# 下方数据和日志存放路径请按照实际情况修改路径
# 通过方法一、二安装，默认路径已存在，通过方法三安装，请指定存在的路径
path.data: /var/lib/elasticsearch
path.logs: /var/logs/elasticsearch
bootstrap.memory_lock: true
# 当前节点内网IP地址，虽也可以设置为0.0.0.0但建议还是按此设置
network.host: 10.66.66.1
http.port: 9200
# 集群中所有节点地址
discovery.seed_hosts: [&quot;10.66.66.1&quot;, &quot;10.66.66.2&quot;,&quot;10.66.66.3&quot;]
# 集群中所有节点名，应与集群所有节点的node.name一致
cluster.initial_master_nodes: [&quot;node-1&quot;, &quot;node-2&quot;,&quot;node-3&quot;]
</code></pre>

<p>另外，在启动Elasticsearch 7.8之前需先做些准备工作。首先编辑文件<code>/etc/security/limits.conf</code>，在尾部增加如下配置：</p>

<pre><code class="language-shell">elasticsearch      soft    nofile  100000
elasticsearch      hard    nofile  100000
elasticsearch      soft    memlock unlimited
elasticsearch      hard    memlock unlimited
elasticsearch      soft    nproc   4096
elasticsearch      hard    nproc   4096
</code></pre>

<p>编辑文件<code>/etc/sysctl.conf</code>，根据文件内已有内容，调整或新增如下配置：</p>

<pre><code class="language-shell">vm.swappiness=10
vm.max_map_count=262144
</code></pre>

<p>添加完成后，执行<code>sysctl -p</code>。</p>

<p>最后修改<code>/etc/hosts</code>文件，增加Elasticsearch集群主机名，例如：</p>

<pre><code class="language-shell">10.66.66.1  node-1
10.66.66.2  node-2
10.66.66.3  node-3
</code></pre>

<p>Elasticsearch内置Java，无需再在系统层再次部署Java。</p>

<p>完成配置后即可启动，方法一、二可通过<code>systemctl</code>命令进行启停：</p>

<pre><code class="language-shell">systemctl start elasticsearch;
systemctl stop elasticsearch;
systemctl restart elasticsearch;
</code></pre>

<p>如需开启开机自启，可通过如下命令开启：</p>

<pre><code class="language-shell">systemctl daemon-reload;
systemctl enable elasticsearch;
</code></pre>

<p>方法三启动可通过如下命令进行启停：</p>

<pre><code class="language-shell"># 启动
/usr/share/elasticsearch/bin/elasticsearch -d -p /usr/share/elasticsearch/pid;
# 停止
esid=$(cat /usr/share/elasticsearch/pid &amp;&amp; echo);
kill -SIGTERM $esid;
</code></pre>

<p>启动后可以执行如下命令，查看启动日志、集群节点发现、主节点选举是否正常：</p>

<pre><code class="language-shell"># 路径请注意：1. 按照配置文件设置的日志存放路径寻找；2. 按照实际集群名输入日志文件名
tail -f /var/logs/elasticsearch/Elasticsearch-Cluster.log;
# 按照方法一、二安装后也可以通过如下命令查看Elasticsearch进程情况
systemctl status elasticsearch;
</code></pre>

<p>也可以通过如下网址浏览器或<code>curl</code>命令访问直接获取Elasticsearch返回的集群健康检查信息：</p>

<pre><code class="language-shell">curl -XGET http://node-1:9200/_cluster/health?pretty
</code></pre>

<p>也可以在浏览器直接访问机器的<code>9200</code>端口，可展示如下页面：</p>

<p><img src="media/15950655748460/2020-7-18-003.png" alt="Elasticsearch启动成功后9200端口默认访问页"/></p>

<h2 id="toc_6">Elasticsearch 7.8的认证功能配置</h2>

<p>Elasticsearch默认启动后，<code>9200</code>端口通过设定的Host IP是可以随意访问的，这十分不安全。为了保证一定的安全性，我们可以修改<code>elasticsearch.yml</code>文件里<code>http.port</code>字段，修改默认端口，以及增加访问时必须帐密认证访问。</p>

<p>自Elasticsearch 6.8开始，Elastic将部分X-Pack付费功能免费开放使用，其中就有基础认证功能，因此在Elasticsearch 6.8以后，可直接使用Elasticsearch自带的认证功能。而之前的版本，则需要一个开源插件叫做elasticsearch-http-basic，作者仓库地址：<a href="https://github.com/Asquera/elasticsearch-http-basic">https://github.com/Asquera/elasticsearch-http-basic</a> 。然而目前此仓库已封版不再更新，因此建议如需使用Elasticsearch，不要使用Elasticsearch 6.8以前的版本。</p>

<p>接下来我们配置Elasticsearch 7.8的认证功能，首先在某一台集群节点中，进入进程文件路径，执行如下命令，创建一个证书颁发机构：</p>

<pre><code class="language-shell">bin/elasticsearch-certutil ca;
</code></pre>

<p>一路回车即可，中间有设置CA的密码，无需设置。完成后将在进程文件路径目录生成文件<code>elastic-stack-ca.p12</code>。后继续在该台已生成证书颁发机构的集群节点继续执行如下命令，创建一个证书与私钥：</p>

<pre><code class="language-shell">bin/elasticsearch-certutil cert --ca elastic-stack-ca.p12;
</code></pre>

<p>一路回车即可，中间有设置证书的密码，无需设置。完成后将在进程文件路径目录生成文件<code>elastic-certificates.p12</code>。完成生成后，将该文件拷贝到其他机器的相同路径，集群所有机器修改两个文件的属组：</p>

<pre><code class="language-shell">chown -R elasticsearch:elasticsearch *.p12;
</code></pre>

<p>集群所有机器建立软连接到配置文件路径下，示例如下，具体路径请修改为实际路径：</p>

<pre><code class="language-shell">ln -s /usr/share/elasticsearch/elastic-certificates.p12 /etc/elasticsearch/elastic-certificates.p12;
ln -s /usr/share/elasticsearch/elastic-stack-ca.p12 /etc/elasticsearch/elastic-stack-ca.p12;
</code></pre>

<p>配置<code>elasticsearch.yml</code>：</p>

<pre><code class="language-yaml"># 设置集群互信通信端口9300
transport.port: 9300
http.cors.enabled: true
http.cors.allow-origin: &quot;*&quot;
http.cors.allow-headers: Authorization
# 开启X-Pack的安全认证
xpack.security.enabled: true
# 开启X-Pack的集群内互信安全认证，与上面安全认证开关同步必开
xpack.security.transport.ssl.enabled: true
# 验证模式为证书模式
xpack.security.transport.ssl.verification_mode: certificate
# 配置证书路径
xpack.security.transport.ssl.keystore.path: elastic-certificates.p12
xpack.security.transport.ssl.truststore.path: elastic-certificates.p12
# 如果需要启用SSL/TLS通过HTTPS访问ES集群，再添加如下配置
xpack.security.http.ssl.enabled: true
xpack.security.http.ssl.keystore.path: elastic-certificates.p12
xpack.security.http.ssl.truststore.path: elastic-certificates.p12
xpack.security.http.ssl.client_authentication: none
</code></pre>

<p>修改<code>discovery.seed_hosts</code>字段配置，增加集群互信通信端口<code>9300</code>，例如：</p>

<pre><code class="language-shell">discovery.seed_hosts: [&quot;10.66.66.1:9300&quot;, &quot;10.66.66.2:9300&quot;,&quot;10.66.66.3:9300&quot;]
</code></pre>

<p>完成配置后，重启Elasticsearch进程即可。后进行内置用户认证密码的设置，在某一台集群节点中，进入进程文件路径，执行如下命令：</p>

<pre><code class="language-shell">bin/elasticsearch-setup-passwords interactive;
</code></pre>

<p>这里给Elasticsearch中内置用户创建密码，其内置用户有：</p>

<ul>
<li>elastic：拥有 superuser 角色，是内置的超级用户，它可以做任何事情。</li>
<li>kibana：拥有 kibana_system 角色，是Kibana用来连接Elasticsearch并与之通信的。Kibana服务器以该用户身份提交请求以访问集群监视API和.kibana索引不能访问 index。</li>
<li>logstash_system：拥有logstash_system角色。是Logstash在Elasticsearch中存储监控信息时使用。</li>
<li>beats_system：拥有 beats_system 角色。是Beats在Elasticsearch中存储监控信息时使用。</li>
<li>apm_system：APM服务器在Elasticsearch中存储监视信息时使用的用户。</li>
<li>remote_monitoring_user：Metricbeat用户在Elasticsearch中收集和存储监视信息时使用。 它具有remote_monitoring_agent和remote_monitoring_collector内置角色。</li>
</ul>

<p>完成此步设置，再使用HTTP/HTTPS协议通过<code>9200</code>端口访问时，就需要帐密了。输入帐密即可访问。<code>curl</code>也可，命令测试示例如下，例如用户名为<code>elastic</code>，密码为<code>ESabc+2333</code>：</p>

<pre><code class="language-shell">curl -uelastic:ESabc+2333 -XGET http://node-1:9200/_cluster/health?pretty
</code></pre>

<h2 id="toc_7">Kibana的部署与配置</h2>

<p>Kibana的部署配置比较简单，安装方式类似Elasticsearch，具体可自行选择，本节不再赘述，使用方法二，执行如下命令，进行安装：</p>

<pre><code class="language-shell">wget https://artifacts.elastic.co/downloads/kibana/kibana-7.8.0-x86_64.rpm;
shasum -a 512 kibana-7.8.0-x86_64.rpm;
rpm --install kibana-7.8.0-x86_64.rpm;
</code></pre>

<blockquote>
<p><strong>注意：</strong>若<code>shasum</code>提示找不到命令，请输入<code>yum -y install perl-Digest-SHA</code>进行依赖安装。</p>
</blockquote>

<!-- -->

<blockquote>
<p><strong>提示：</strong>鉴于Elastic网站为国外，下载速度极慢，可选择国内镜像地址，例如华为云的镜像，下载地址：<a href="https://mirrors.huaweicloud.com/kibana/7.8.0/">https://mirrors.huaweicloud.com/kibana/7.8.0/</a></p>
</blockquote>

<p>按上述方式安装后其进程文件路径在<code>/usr/share/kibana</code>，配置文件路径在<code>/etc/kibana</code>。</p>

<p>修改<code>/etc/kibana/kibana.yml</code>文件为如下配置：</p>

<pre><code class="language-yaml">server.port: 5601
server.host: &quot;0.0.0.0&quot;
# 设置Elasticsearch集群地址，方便Kibana做容灾管理
elasticsearch.hosts: [&quot;https://node-1:9200&quot;,&quot;https://node-2:9200&quot;,&quot;https://node-3:9200&quot;]
kibana.index: &quot;.kibana&quot;
# 国际化设置，设置为中文
i18n.locale: &quot;zh-CN&quot;
# 开启X-Pack的安全认证
xpack.security.enabled: true
# Elasticsearch内置账户密码
elasticsearch.username: &quot;kibana&quot;
elasticsearch.password: &quot;ESabc+2333&quot; # 设置内置账户密码时kibana账户的密码
# Kibana SSL/TLS访问开启，若无需配置SSL/TLS，可忽略
server.ssl.enabled: true
server.ssl.key: /etc/kibana/kibana-certificates.key
server.ssl.certificate: /etc/kibana/kibana-certificates.cer
server.ssl.certificateAuthorities: /etc/kibana/kibana-certificates-ca.cer
server.ssl.clientAuthentication: none
# Elasticsearch如开启SSL/TLS访问，则需要配置如下两条规则
elasticsearch.ssl.verificationMode: certificate
elasticsearch.ssl.certificateAuthorities: /etc/kibana/kibana-certificates-ca.cer
</code></pre>

<p>上述Kibana配置中SSL/TLS的认证有点小插曲需要说明，由于Kibana现在不支持<code>.p12</code>文件的加密认证方式，若目前没有SSL/TLS安全机构认可的签发证书，但仍需要启用SSL/TLS，此时需要我们将<code>.p12</code>文件转换后使用配置。具体操作如下：</p>

<pre><code class="language-shell">openssl pkcs12 -in elastic-certificates.p12 -nocerts -nodes &gt; kibana-certificates.key
openssl pkcs12 -in elastic-certificates.p12 -clcerts -nokeys &gt; kibana-certificates.cer
openssl pkcs12 -in elastic-certificates.p12 -cacerts -nokeys -chain &gt; kibana-certificates-ca.cer
</code></pre>

<p>将生成的几个文件放置在合适的路径，例如上述配置将这几个文件放置在了<code>/etc/kibana/</code>路径下，后调用即可。</p>

<blockquote>
<p><strong>注意：</strong>仍然建议使用域名向安全机构申请认可的签发证书后配置Kibana的SSL/TLS选项，因为自签发证书浏览器不认为是安全的，仍有安全风险，请注意。</p>
</blockquote>

<p>完成配置后即可启动，可通过<code>systemctl</code>命令进行启停：</p>

<pre><code class="language-shell">systemctl start kibana;
systemctl stop kibana;
systemctl restart kibana;
</code></pre>

<p>如需开启开机自启，可通过如下命令开启：</p>

<pre><code class="language-shell">systemctl daemon-reload;
systemctl enable kibana;
</code></pre>

<p>启动后通过浏览器访问即可，可通过输入内置用户访问Kibana，例如用户名为<code>elastic</code>，密码为<code>ESabc+2333</code>：</p>

<p><img src="media/15950655748460/2020-7-18-001.png" alt="Kibana登录界面"/></p>

<p><img src="media/15950655748460/2020-7-18-011.png" alt="Kibana登录后界面"/></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[解决CentOS在执行yum命令时报错"Error: rpmdb open failed"]]></title>
    <link href="https://zning.me/15946461550517.html"/>
    <updated>2020-07-13T21:15:55+08:00</updated>
    <id>https://zning.me/15946461550517.html</id>
    <content type="html"><![CDATA[
<p>今天在日常进行CentOS操作的时候，不知为何<code>yum -y install xxx</code>命令突然卡死不动，<code>Ctrl+C</code>取消执行也无解，遂关闭SSH重新建立连接，然而再次重新执行<code>yum -y install xxx</code>命令时却报如下错误：</p>

<span id="more"></span><!-- more -->

<pre><code class="language-shell">...
root@localhost:~&gt;yum -y install xxx
Loaded plugins: fastestmirror, langpacks
Existing lock /var/run/yum.pid: another copy is running as pid 27970.
Another app is currently holding the yum lock; waiting for it to exit...
  The other application is: yum
    Memory :  43 M RSS (362 MB VSZ)
    Started: Mon Jul 13 18:27:38 2020 - 03:46 ago
    State  : Uninterruptible, pid: 27970
Another app is currently holding the yum lock; waiting for it to exit...
  The other application is: yum
    Memory :  43 M RSS (362 MB VSZ)
    Started: Mon Jul 13 18:27:38 2020 - 03:48 ago
    State  : Uninterruptible, pid: 27970
...
</code></pre>

<p>初步判断应该是进程号为<code>27970</code>的那个假死了，即使刚才SSH关闭了也并没有杀掉他，遂干脆利落的执行了<code>kill -9 27970</code>直接杀死，后再执行<code>yum -y install xxx</code>命令，结果又有报错：</p>

<pre><code class="language-shell">...
root@localhost:~&gt;yum -y install xxx
error: rpmdb: BDB0113 Thread/process 27970/140274709284672 failed: BDB1507 Thread died in Berkeley DB library
error: db5 error(-30973) from dbenv-&gt;failchk: BDB0087 DB_RUNRECOVERY: Fatal error, run database recovery
error: cannot open Packages index using db5 -  (-30973)
error: cannot open Packages database in /var/lib/rpm
CRITICAL:yum.main:

Error: rpmdb open failed
...
</code></pre>

<p>哦豁，凉了，rpmdb本地数据存储文件炸了……不过好在有解决方法。</p>

<p>执行如下命令，清理YUM仓库本地数据存储文件：</p>

<pre><code class="language-shell">mv /var/lib/rpm/__db* /tmp;
</code></pre>

<p>执行如下命令，清理yum缓存：</p>

<pre><code class="language-shell">rpm --rebuilddb;
yum clean all
</code></pre>

<p>大功告成。</p>

<p>现在细想一下，yum当时有可能是因为网络问题假死，可能再等等就好了，不过毕竟当时有点着急，以后得注意下。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[《架构即未来》之可扩展性组织的人员配置（下一）]]></title>
    <link href="https://zning.me/15945523435332.html"/>
    <updated>2020-07-12T19:12:23+08:00</updated>
    <id>https://zning.me/15945523435332.html</id>
    <content type="html"><![CDATA[
<p>从去年夏末至今，我一直在阅读《架构即未来：现代企业可扩展的Web架构、流程和组织（原书第二版）》这本书。全书阐述了经过验证的信息技术扩展方法，对需要掌握的产品和服务的平滑扩展做了详尽的论述。具有一定的参考价值。从今天起，我将逐步总结之前所看内容，以飨读者，也以便自己回顾。文章既有摘转录，又有自我理解批注。</p>

<p>本文是该书的第一部分的下半部分，是书中的第五章，主要介绍了管理的相关内容，涉及了项目任务管理、团队建设、度量与目标树等部分。</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">什么是管理</a>
</li>
<li>
<a href="#toc_1">项目和任务管理</a>
</li>
<li>
<a href="#toc_2">团队建设：球队类比</a>
</li>
<li>
<a href="#toc_3">优化团队：花园类比</a>
</li>
<li>
<a href="#toc_4">度量、指标和目标评估</a>
</li>
<li>
<a href="#toc_5">目标树</a>
</li>
<li>
<a href="#toc_6">为成功铺路</a>
</li>
</ul>


<p>管理是有关执行和实现组织目标、愿景和使命必需的所有活动。它是“以明智和合乎道德的手段来完成任务”。要在管理上成功，。为一个奋发向上的领导，需要专注和致力于学习和成长。也需要安排好任务、人和度量以实现预期的目标。并有如下的特点：</p>

<ul>
<li>项目和任务管理是成功管理的关键。</li>
<li>人力资源管理与组织及其雇用、解雇、培养和发展相关。</li>
<li>度量是管理成功的关键。</li>
<li>管理者需要帮助团队完成任务。</li>
</ul>

<p>经验告诉我们，在每个成功的故事里，最为核心的是在关键的时刻，领导、经理和团队能否在挑战和机遇的面前崛起并使公司获益。“我们确实需要改变目前的方式，来适应新的世界秩序，软件和硬件是开发服务的原材料。我们需要重新思考机构、流程、激励和项目管理。”</p>

<h2 id="toc_0">什么是管理</h2>

<p>《韦氏词典》中对管理是这样定义的，“执行或者监督某些事情”或“以执法手段完成某件事情”。对这一定义，我们将略加修改，融入道德因素，更新后的定义是“以明智和合乎道德的手段来完成任务”。</p>

<p>一般来说，雇员有权知道他的表现如何，其不知道真实表现是对股东和员工的不道德行为。使命完成的方式与使命的实际完成一样至关重要。</p>

<p>管理和领导有许多区别，但两者都很重要。如果领导是承诺，那么管理就是行动。如果领导是目的地，那么管理就是方向。如果领导是激励，那么管理就是动机。如果领导是拉力，那么管理就是推力。对于成功的最大化股东财富，需要两者兼备。</p>

<p>管理包括度量活动、目标评估、指标制订。也包括人员配备中的人事责任、人事评估和团队建设（包括技能和其他的特性）。最后，管理包括项目管理中的所有活动，例如驱动团队完成任务，设定有挑战性的项目进度等等。</p>

<blockquote>
<p><strong>好的经理需要具备什么素质</strong></p>

<p>从领导的角度看，管理是一个有几个参数的函数，其中有些参数也适用于领导函数。</p>

<p>最好的经理在看到细节的同时，又能令人难以置信地面向目标和任务。一旦分配到一个任务或目标，他们就能把任务或目标分解。这个分解的活动不仅涉及行动，也包括沟通、组织结构、合理的薪酬后勤和资金等。</p>

<p>好的经理练就了一身管理人的技能，这有助于他们把组织内每个人的最大潜力都发掘出来。</p>

<p>最后，即使最好的经理也承认自己需要不断地提高，而能够意识到提高的唯一办法是度量。</p>
</blockquote>

<h2 id="toc_1">项目和任务管理</h2>

<p>好的经理在预算内按时完成项目，并且符合为股东创造价值的预期。优秀的经理，甚至能在面对逆境的时候亦能如此。但是，理解如何成功地完成那些项目所需要采取的行动也非常重要。</p>

<p>普鲁士名将赫尔穆特·冯·毛奇有句非常出名的话，“没有万全的作战计划”。这句话对管理复杂产品的研发尤为正确。</p>

<p>经验告诉我们，虽然项目计划很重要，但是创造价值的部分却不是最初的计划，而是在过程中对项目及其可能路径的深入思考。不幸的是，太多人把原来制订的计划看成是通往成功的唯一路径，而不是众多可能路径中的一个。毛奇的这句话警示我们要把关注点从最初计划的执行，转移到具有重要价值的应急上。与其像激光一样聚焦在计划的精密性上，我们更应该考虑选择哪条路径以取得项目的成功。因此，在AKF我们实践了5-95规则：即用5%的时间制订一个充足、保守和详细的计划，同时承认这个计划不是完备的，把其余95%的时间投入到应急演练，以应付突发事件。</p>

<p>项目计划的价值是在思考的过程中理解可能的执行选项，从而更好地为股东创造价值。</p>

<h2 id="toc_2">团队建设：球队类比</h2>

<p>在成本可控的前提下，为了确保最大的产出和最高的质量，我们必须不断地寻找负担得起的最好的人才。大多数人并不积极地管理技能、人员和团队的组成，其结果相当于欺骗公司和股东。</p>

<p>组织的产出取决于个人的产出和团队的规模。效率是高性价比可扩展性的一个组成部分，用来度量以同样的投入取得更多或更好的产出，或者以较少的投入取得更多的产出。人的扩展性是与个人、规模和组织相关联的。</p>

<p>记住，不做某事的决定意味着你决定了做某事。此外，忽略了一些应该做的事情其实等同于决定不去做。如果你没有花几周的时间与团队的成员们相处，那相当于你已经决定不花时间与他们相处。这是绝对不可原谅的，而且当与董事会讨论这些问题的时候，你的感觉可能也不会太好。</p>

<h2 id="toc_3">优化团队：花园类比</h2>

<p>组织的扩展在很大程度上依赖于人才，而这又取决于人均产出及在企业文化影响下行为的一致性。花园应该是经过精心设计的，团队也同样如此。设计团队意味着要找到符合组织愿景和使命所需要的合适人才。</p>

<p>在快速增长的公司里，管理者通常会花很多时间来面试和挑选候选人，但通常花在每个候选人身上的时间却很少。更糟的是，这些管理者通常不会花时间去分析和总结以往招聘决策的好坏。要为某个特定的岗位找到合适的人，需要重视和纠正过去招聘中的失败，复制以往招聘中的成功。然而，在面试中常常只关注技能，而忽略其他更重要的方面，如企业文化的适应性及团队的配合度。自问：为什么你要舍弃某人?为什么有人决定要离开?</p>

<p>要从生产力和质量的角度来考虑组织的人员需求。同时鼓励尽可能花更多的时间与候选人互动，力争第一次就找到合适的人。</p>

<p>照顾花园与优化团队异曲同工。所有与管理团队相关的事情，常常因为时间不足而被忽略。我们可能会花工夫来摘新花，但是却时常忘记，花园里那些已经盛开的花朵也同样需要滋养。</p>

<p>培养的目的是让团队成员成长，以满足股东的期望。这包括指导、赞扬、正确地掌握技术或方法、调整薪酬和股权以及任何使员工更强大和更优秀的办法。</p>

<p>好的团队喜欢积极的、可以实现的挑战。  </p>

<p>类比管理花园，营养就是你用在指导表现不佳的个人使其达到可以接受的表现水平的时间，是团队花在补偿表现差的个人所造成的不良结果的时间。对大多数执行人员和管理者来说，给花园除草是最痛苦的活动，因此，这往往是我们最后的手段。</p>

<p>虽然你必须遵守公司淘汰员工的有关要求，但是想办法尽快淘汰妨碍大家实现目标的人极为重要。越早淘汰这些表现差的人，就能越快找到合适的替代者，让团队向前发展。</p>

<p>当一个员工制造出充满敌意的工作环境时，这种情况会更加明显。长期的实践让我们领悟到，对表现差的人要尽早淘汰。</p>

<h2 id="toc_4">度量、指标和目标评估</h2>

<p>我们确信要营造一种企业文化，支持度量任何与创造股东价值相关联的活动。通常我们建议度量成本、可用性、响应时间、生产率以及质量。</p>

<p>成本直接影响平台的扩展性。无疑，公司的工程计划预算，如果不是有人发给你，就是你自己制订。理想的情况是，一个成长型公司有专门的部分预算用在平台或服务的扩展上。这个百分比就是一个有趣的、需要跟踪度量的指标。</p>

<p>我们建议以工程总支出的百分比和每笔交易的花费，作为度量扩展的成本。而对采用相对成本还是绝对成本来度量和报告可扩展性的成本存在着争议。因此，与其只报告扩展的绝对成本，在位股东创造价值的基础上，将这种价值规范化。</p>

<p>当要寻找度量的目标时，可用性是一个必不可少的选择。报告宕机时间中有多少与平台或系统的可扩展性问题相关联。目的是彻底消除因为用户无法完成交易而丧失业务机会的现象。</p>

<p>与可用性密切相关的是响应时间。即使没有服务水平协议（SLA），响应时间的度量应该和服务水平协议互相对照比较。在理想的情况下，响应时间所涉及的用户交易，应该是与实际的终端，而不是代理之间的互动。对关键的交易，除了绝对度量内部或外部服务水平以外，还应该包括跟踪月度环比的相对度量。假如收入和放弃率与某个关键交易的响应时间变长紧密相关，这些数据可以解释可扩展性项目的作用。</p>

<p>生产效率是另外一个重要的度量指标。在考虑如何度量生产效率的时候，真正的诀窍是把它分成至少两个部分。第一部分讨论工程团队是否用尽可能多的时间来解决工程相关的任务。第二部分的重点是度量每个工作日的产出。</p>

<p>质量不在可扩展性管理的度量范围之内。显然需要了解像错误数量这样的典型指标，生产系统和版本发布中的KLOC量，整个产品中错误的数量和产品质量工作的成本，我们建议围绕着影响可扩展性，把这些因素进一步分解。</p>

<h2 id="toc_5">目标树</h2>

<p>目标树的根是一个或者多个公司或组织的大目标，将其分解成次级的目标，通过达成这些次级目标进而实现大目标。</p>

<p>质量和可用性都会影响产品变现的机会。</p>

<h2 id="toc_6">为成功铺路</h2>

<p>到目前为止，我们描绘了一张经理的画像，他同时是工头、战术家、园丁和测量师，还不止这些。除了负责确保团队完成工作外，还要决定选择哪条路径，达成什么目标以及如何度量工作的进展，经理必须要确保已经用推土机把通往目标的路径推平铺好。</p>

<p>我们的意思只是管理者负责为组织扫清通往目标成功道路上的障碍。人们很容易把这种想法和“任何挡在路上的都是阻碍成功的障碍，必须移除”混淆起来。有时，路上的障碍起着确保你表现正常的作用。真正的障碍是出了问题，但没有处理。</p>

<p>当你遇到这些障碍时，记住这个团队不是为你工作，而是与你一起工作你可能是团队的负责人，但也是团队成功的关键部分。好的管理者实际上会亲自上手，帮助团队完成目标。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[“源产控”系列（二）在CentOS 8上使用Nginx 1.18: 编译安装与基本使用]]></title>
    <link href="https://zning.me/15941166309027.html"/>
    <updated>2020-07-07T18:10:30+08:00</updated>
    <id>https://zning.me/15941166309027.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>本篇为慧响技术角“源产控”专题系列第2篇文章。</p>
</blockquote>

<!-- -->

<blockquote>
<p>慧响技术角“源产控”专题，将聚焦开源、国产化、自主可控三个方向的技术，以操作系统、中间件、数据库、程序应用等为粗分类，更新相关技术的发展趋势、探究技术核心的深度使用、系统总结技术整体架构，为对相关技术的学习者提供可观的资料，亦为个人同步学习总结的笔记，以飨读者。</p>
</blockquote>

<p>本篇对在CentOS 8上使用Nginx 1.18的基本安装与基本使用进行介绍与总结，未来对在CentOS 8上使用Nginx 1.18的相关，将陆续更新其使用总结、性能调优等方面的系列文章，敬请期待。</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">Nginx是个啥</a>
</li>
<li>
<a href="#toc_1">在CentOS 8进行安装Nginx 1.18</a>
</li>
<li>
<a href="#toc_2">在CentOS 8初步使用Nginx 1.18</a>
</li>
<li>
<a href="#toc_3">参考资料</a>
</li>
</ul>


<h2 id="toc_0">Nginx是个啥</h2>

<p>Nginx（发音同“engine X”）是异步框架的网页服务器，也可以用作反向代理、负载平衡器和HTTP缓存。该软件由伊戈尔·赛索耶夫创建并于2004年首次公开发布。2011年成立同名公司以提供支持。2019年3月11日，Nginx公司被F5 Networks以6.7亿美元收购。Nginx是免费的开源软件，根据类BSD许可证的条款发布。一大部分Web服务器使用Nginx，通常作为负载均衡器。</p>

<p>其开源版本官网是nginx.org。商业版本官网是nginx.com。</p>

<p>不过说起来Nginx，就不得不提在Nginx被F5收购后Nginx作者被捕的事儿了。据外媒报道，2019年12月12日，俄罗斯警方搜查了商业服务器公司Nginx，并当场带走了两位联合创始人。。关于这个事儿，我在文章《战疫之下，哪些事可能会改变我们》也拿其为例对个人品牌与口碑塑造的一些风险进行了阐述。有兴趣的可以点进去看一看。</p>

<p>对于Nginx，由于其发展迅速、性能卓越、开源开放，收割了很多Web服务提供与负载均衡、反向代理场景的市场，也衍生了很优秀的二次开发版本例如国人开发的OpenResty、阿里开源的Tengine等。在“在CentOS 8上使用Nginx 1.18”系列后面将择机对两个优秀的二次开发进行介绍。</p>

<h2 id="toc_1">在CentOS 8进行安装Nginx 1.18</h2>

<p>虽然我们可以通过<code>yum -y install nginx</code>进行安装，但是在“源产库”系列第一篇文章《CentOS 8之初相识》中有过介绍，其镜像源预编译的版本为1.14，而截至文章发表之日Nginx的稳定版本已更新到了1.18，主线版本更新到了1.19，因此我们使用官网的稳定源码版本进行下载编译。</p>

<p><img src="media/15941166309027/15941181736377.jpg" alt="" style="width:1036px;"/></p>

<blockquote>
<p>Nginx版本下载页，截至发文（2020-07-07）稳定版本已更新到了1.18，主线版本更新到了1.19</p>
</blockquote>

<!-- -->

<blockquote>
<p>小知识：关于稳定版本和主线版本。前者是Nginx更新接收针对高严重性错误的修复，但不会使用最新的功能，其版本号的第二位用偶数表示。而后者是Nginx是更新活跃的开发分支，其添加了最新功能和错误修复，其版本号的第二位用奇数表示。不过在Nginx中，“稳定”指的是功能和更新频率，它与软件质量无关。稳定分支在其生命周期中从不接收新功能，并且通常仅接收一个或两个更新，用于修复严重的错误。 稳定分支的生命周期一般是一年，每年四月官方就会停止对当前稳定分支的维护，不再提供错误修复补丁。</p>
</blockquote>

<p>下载并解压<code>nginx-1.18.0.tar.gz</code>：</p>

<pre><code class="language-shell">wget http://nginx.org/download/nginx-1.18.0.tar.gz;
tar -zxvf nginx-1.18.0.tar.gz;
</code></pre>

<p>进入目录<code>cd nginx-1.18.0</code>，目录结构如下：</p>

<pre><code class="language-shell">总用量 1204
drwxr-xr-x 6 1001 1001    326 7月   7 18:50 auto
-rw-r--r-- 1 1001 1001 302863 4月  21 22:09 CHANGES
-rw-r--r-- 1 1001 1001 462213 4月  21 22:09 CHANGES.ru
drwxr-xr-x 2 1001 1001    168 7月   7 18:50 conf
-rwxr-xr-x 1 1001 1001   2502 4月  21 22:09 configure
drwxr-xr-x 4 1001 1001     72 7月   7 18:50 contrib
drwxr-xr-x 2 1001 1001     40 7月   7 18:50 html
-rw-r--r-- 1 1001 1001   1397 4月  21 22:09 LICENSE
drwxr-xr-x 2 1001 1001     21 7月   7 18:50 man
-rw-r--r-- 1 1001 1001     49 4月  21 22:09 README
drwxr-xr-x 9 1001 1001     91 7月   7 18:50 src
</code></pre>

<p><code>conf</code>文件夹内为Nginx的初始配置文件，<code>contrib</code>文件夹放置了三位开源贡献者贡献的三个插件内容，<code>html</code>文件夹为默认站点文件内容，<code>src</code>文件夹为Nginx源码。<code>configure</code>文件为编译的配置文件。</p>

<p>拷贝Nginx安装包内如下文件到路径，可以让vim对<code>nginx.conf</code>的语言语法进行高亮解析：</p>

<pre><code class="language-shell">cp -r contrib/vim/* ~/.vim/;
</code></pre>

<p><code>./configure --help</code>可以查看Nginx编译支持参数列表，<code>configure</code>文件进行设置后，会在<code>objs</code>文件内生成Nginx的安装所需文件，其中<code>ngx_modules.c</code>，该文件决定接下来编译有哪些模块需要编译进Nginx。</p>

<p>本文不涉及模块的编译使用，我们使用最简单的编译设置进行编译安装：</p>

<pre><code class="language-shell">./configure --prefix=/usr/local/nginx;
</code></pre>

<p>如果报如下错误，请执行命令<code>yum -y install pcre-devel openssl openssl-devel</code>进行安装：</p>

<pre><code class="language-shell">...
checking for PCRE library ... not found
checking for PCRE library in /usr/local/ ... not found
checking for PCRE library in /usr/include/pcre/ ... not found
checking for PCRE library in /usr/pkg/ ... not found
checking for PCRE library in /opt/local/ ... not found

./configure: error: the HTTP rewrite module requires the PCRE library.
You can either disable the module by using --without-http_rewrite_module
option, or install the PCRE library into the system, or build the PCRE library
statically from the source with nginx by using --with-pcre=&lt;path&gt; option.
</code></pre>

<p>出现以下内容，即为<code>configure</code>文件设置完成：</p>

<pre><code class="language-shell">Configuration summary
  + using system PCRE library
  + OpenSSL library is not used
  + using system zlib library

  nginx path prefix: &quot;/usr/local/nginx/&quot;
  nginx binary file: &quot;/usr/local/nginx/sbin/nginx&quot;
  nginx modules path: &quot;/usr/local/nginx/modules&quot;
  nginx configuration prefix: &quot;/usr/local/nginx/conf&quot;
  nginx configuration file: &quot;/usr/local/nginx/conf/nginx.conf&quot;
  nginx pid file: &quot;/usr/local/nginx/logs/nginx.pid&quot;
  nginx error log file: &quot;/usr/local/nginx/logs/error.log&quot;
  nginx http access log file: &quot;/usr/local/nginx/logs/access.log&quot;
  nginx http client request body temporary files: &quot;client_body_temp&quot;
  nginx http proxy temporary files: &quot;proxy_temp&quot;
  nginx http fastcgi temporary files: &quot;fastcgi_temp&quot;
  nginx http uwsgi temporary files: &quot;uwsgi_temp&quot;
  nginx http scgi temporary files: &quot;scgi_temp&quot;
</code></pre>

<p>紧接着执行<code>make</code>命令，执行后，在<code>objs</code>文件夹下可以看到编译后的<code>nginx</code>二进制文件和目标文件。无报错说明正常。如若首次安装Nginx，紧接着执行<code>make install</code>安装即可。如果是进行Nginx升级，<code>make install</code>是不应该用的，需要做的是将该目录下生成的<code>nginx</code>二进制文件拷贝到已有Nginx的二进制文件相应目录中。</p>

<h2 id="toc_2">在CentOS 8初步使用Nginx 1.18</h2>

<p>安装完成，输入如下命令进行启动Nginx：</p>

<pre><code class="language-shell">/usr/local/nginx/sbin/nginx;
</code></pre>

<p>Nginx基本指令格式为<code>nginx -s reload</code>，具体指令解释如下：</p>

<ul>
<li><code>nginx -?/-h</code>为显示帮助信息；</li>
<li><code>nginx  -c</code>为使用指定的配置文件，后跟配置文件路径；</li>
<li><code>nginx -g</code>为指定配置指令，后跟需要指定的配置指令；</li>
<li><code>nginx-p</code>为指定运行目录，后跟运行目录路径；</li>
<li><code>nginx -s</code>发送信号，其中后跟参数<code>stop</code>为立即停止服务、<code>quit</code>为有序停止服务；<code>reload</code>为重载配置文件、<code>reopen</code>为重新开始记录日志文件；</li>
<li><code>nginx -t/-T</code>测试配置文件是否有语法错误；</li>
<li><code>nginx -v、nginx -V</code>打印Nginx的版本信息或编译信息。</li>
</ul>

<p>在浏览器访问IP，如图，可以访问即正常。</p>

<p><img src="media/15941166309027/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202020-07-07%2019.25.48.png" alt="屏幕截图 2020-07-07 19.25.48"/></p>

<p><code>ps -ef | grep nginx</code>查看进程情况：</p>

<pre><code class="language-shell">root      8556     1  0 19:24 ?        00:00:00 nginx: master process /usr/local/nginx/sbin/nginx
nobody    8557  8556  0 19:24 ?        00:00:00 nginx: worker process
root      8561  8702  0 19:25 pts/0    00:00:00 grep --color=auto nginx
</code></pre>

<p>现在安装好的基本Nginx，完整的<code>nginx.conf</code>文件如下：</p>

<pre><code class="language-conf">#user  nobody;
worker_processes  auto;

error_log  logs/error.log;
#error_log  logs/error.log  notice;
#error_log  logs/error.log  info;

pid        logs/nginx.pid;


events {
    worker_connections  1024;
}


http {
    include       mime.types;
    default_type  application/octet-stream;

    log_format  main  &#39;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#39;
                      &#39;$status $body_bytes_sent &quot;$http_referer&quot; &#39;
                      &#39;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#39;;

    access_log  logs/access.log  main;

    server {
        listen       80;
        server_name  localhost;

        access_log  logs/host.access.log  main;

        location / {
            root   html;
            index  index.html index.htm;
        }

        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
    }
}
</code></pre>

<p>Nginx配置语法有如下的规则：</p>

<ol>
<li>日志文件由指令与指令块构成；</li>
<li>每条指令以<code>;</code>结尾，指令与参数间以空格符号分隔；</li>
<li>指令块以<code>{}</code>将多条指令组织在一起；</li>
<li><code>include</code>语句允许组合多个配置文件以提升可维护性；</li>
<li>使用<code>#</code>添加注释，提高可读性；</li>
<li>使用<code>$</code>可调用变量；</li>
<li>部分指令的参数支持正则表达式。</li>
</ol>

<p>Nginx有多种指令块，其中<code>http</code>块表示此为HTTP协议处理块、<code>upstream</code>表示有上游服务提供的配置信息、<code>location</code>对应URL的表达式、<code>server</code>对应一个或一组域/域名的访问。</p>

<p>以下是配置文件部分的参数解释:</p>

<ul>
<li><code>worker_processes</code>为设定工作进程数，若设置为<code>auto</code>则Nginx将根据机器的内核数进行设定分配，如无特殊需求一般建议设置为<code>auto</code>。</li>
<li><code>error_log</code>设定错误日志的路径与等级，其中等级有<code>debug</code>, <code>info</code>, <code>notice</code>, <code>warn</code>, <code>error</code>, <code>crit</code>, <code>alert</code>, <code>emerg</code>几种，以严重性从高到低的顺序列出。根据官方文档的解释，设定某个级别的意义是该级别及以上的日志会被打印在日志文件中，例如设置<code>info</code>则包括其本身在内以及<code>notice</code>, <code>warn</code>, <code>error</code>, <code>crit</code>, <code>alert</code>, <code>emerg</code>都会被列出。若第二个字段为空，默认为<code>error</code>。</li>
<li><code>log_format main &#39;XXX&#39;</code>设定日志格式，<code>main</code>标示为命名，在遇到对不同域名进行不同格式的命名记录时需要用到，<code>server</code>区块下<code>access_log logs/access.log main;</code>可以以main格式进行记录。</li>
</ul>

<p>关于其他参数例如<code>pid</code>、<code>worker_connections</code>等，以及<code>server</code>、<code>upstream</code>、<code>location</code>指令块，将在后续的“在CentOS 8上使用Nginx 1.18”系列文章中陆续介绍与展示使用方法。</p>

<h2 id="toc_3">参考资料</h2>

<ol>
<li><a href="oschina.net/news/106928/nginx-1-16-1-17-released">nginx 主线版 1.17.0 发布，1.14 稳定分支已停止维护</a></li>
<li><a href="nginx.org">Index - nginx.org</a></li>
<li><a href="http://nginx.org/en/docs/ngx_core_module.html#error_log">Core functionality - nginx.org</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[《架构即未来》之可扩展性组织的人员配置（中二）]]></title>
    <link href="https://zning.me/15939493033756.html"/>
    <updated>2020-07-05T19:41:43+08:00</updated>
    <id>https://zning.me/15939493033756.html</id>
    <content type="html"><![CDATA[
<p>从去年夏末至今，我一直在阅读《架构即未来：现代企业可扩展的Web架构、流程和组织（原书第二版）》这本书。全书阐述了经过验证的信息技术扩展方法，对需要掌握的产品和服务的平滑扩展做了详尽的论述。具有一定的参考价值。从今天起，我将逐步总结之前所看内容，以飨读者，也以便自己回顾。文章既有摘转录，又有自我理解批注。</p>

<p>本文是该书的第一部分的中间部分，是书中的第四章，主要介绍了领导力相关的概念、模型、理念、成功因果路线图等部分。</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">什么是领导力</a>
</li>
<li>
<a href="#toc_1">领导力概念模型</a>
<ul>
<li>
<a href="#toc_2">以身作则</a>
</li>
<li>
<a href="#toc_3">不刚愎自用</a>
</li>
<li>
<a href="#toc_4">以人为本，使命为先</a>
</li>
<li>
<a href="#toc_5">决策英明，以德服人</a>
</li>
<li>
<a href="#toc_6">用人不疑</a>
</li>
<li>
<a href="#toc_7">与股东价值保持一致</a>
</li>
<li>
<a href="#toc_8">变革型领导</a>
</li>
<li>
<a href="#toc_9">愿景</a>
</li>
<li>
<a href="#toc_10">使命</a>
</li>
<li>
<a href="#toc_11">目标</a>
</li>
</ul>
</li>
<li>
<a href="#toc_12">成功的因果路线图</a>
</li>
</ul>


<h2 id="toc_0">什么是领导力</h2>

<p>本书把领导力定义为“影响一个组织或者个人达成某个特定目标的行为的力量”，即对组织或个人完成具体目标的影响力。</p>

<p>领导不仅仅指个人或组织的直接报告人。你可以领导你的同伴、来自于其他机构的人，甚至管理层。一个组织内的榜样或楷模也是领导。你会看到，领导是关于你做什么和如何影响你周围其他人或好或坏的行为。</p>

<p>而领导的失败是无法扩展的最常见原因。经常有公司只聚焦在产品功能的研发上，没有意识到在客户的不断要求下，要达到合适的系统可用时间和响应速度需要做什么事情。</p>

<p>【领导：天生的还是人造的】一个人的领导能力是直接取决于其影响个人或组织行为的能力。影响行为的能力来自于几个方面，有些是天生的品质，有些是环境熏陶的结果，还有些是日久形成的容易修改的工具和方法。但是经常发生的是，好的外表被当成是最重要的，但是铁的事实是大多数人都愿意围着长相好看的人转。</p>

<p>我们相信，人的性格是天生的，这些因素可能不仅是由基因决定的，还由环境养成的。事实上，长相、感召力、仪表、魅力和人格只是许多领导因素中的一部分，尽管这些因素有所帮助，其他的因素在形成领导的影响力方面同样重要。</p>

<h2 id="toc_1">领导力概念模型</h2>

<p>我们相信领导的能力，或者说影响一个人或组织的相关目标的行为，是关于几个特性的函数。我们把这些特性叫做函数的参数。输入参数到领导力函数后，会返回一个结果，这个结果就可以反映出个人有效地改变和影响行为的能力。</p>

<p>领导力函数的有些参数，与你能否通过创新或者凭毅力做出什么事情有关系。在这里，创新指的是随口说出愿景的能力，而毅力是不断地花时间进行尝试，但产生的效果是一样的。</p>

<p>领导力函数的其他参数与别人如何看待你有关系。这里有两个重点：感觉和感觉的效果对你所领导团队的影响。如果你是位领导，任何时候都会有人在看着你，他们将在你最弱的时候看着你，并据此形成印象。</p>

<p>用函数来描述领导力的且的是要告诉你，尽管可能无法改变一些事情，但是你仍然可以努力来当一个好的领导。</p>

<p>在缺乏经验或者存在高度无知的情况下，对自己能力 的高估最严重。而要从好的领导力中受益，必须清楚地掌握现状。在《和谐领导》(Resonant Leadership)一书中，理查德·伯亚兹和安妮·麦基发现专注、希望和同情是改变个人的三个要素。专注是自知，包违的和能力，希望和回懂有助于产生愿景，从而驱动改变。</p>

<p>最好的选择就是有一个来自于老板、同级和下级的工作表现评议，常用的是360度评议(360-degree review process)。只有那些可以确切地告诉你如何帮助他们改善表现和绩效评议结果的人，才是你要影响的人。而这个过程必须是匿名的。</p>

<p>当然，如果绩效自评结果不能形成改进计划，那就是浪费你和组织的管理时间。我们认为计划应该既包括依赖和发扬优点，也包括克服自己的缺点。没有人因为优点而无法达到目标，也没有人因为缺点而取得成功。关于领导力，我们必须要通过最小化缺点来减少亏损面，通过最大化优点来增加积累面。</p>

<p>最好的领导人所共同拥有的几个特性，包括以身作则、不刚自用、努力完成使命，同时留意和同情组织的需要、及时决策、给团队授权、和股东的利益保持一致。</p>

<h3 id="toc_2">以身作则</h3>

<p>大多数的人都会赞同，在较好的工作环境或者文化环境里工作的员工，比在相对较差的工作环境或者文化环境里工作的员工的产出要高。</p>

<p>评估你对团队期望的文化标谁，并再次确定自己的行为举止是否与企业文化标准的要求一致。但这并不是说你的团队不能做你没做过的任何事情。从行为的角度来说，你应该表现给你的团队看，已所不欲，勿施于人。</p>

<p>他人对你滥用职权的看法会毁掉你的信用而且影响到领导力函数的结果。破坏领导力的函数会造成员工浪费时间来讨论滥用职收甚至让他们误以为类似的滥用职权是可以接受的行为，这些讨论浪费时间和金钱，减小组织的规模。</p>

<p>每个人都可以通过“以身作则”来提升领导力。严于律己，不徇私枉法，行为的方式应和你期望自己的组织的表现一样。</p>

<h3 id="toc_3">不刚愎自用</h3>

<p>任何一个公司的任何一个职位都不允许自我意识膨胀。过度自夸的言行与打造最好的团队背道而驰，随着时间的推移将会侵蚀股东的价值。最好的领导在为股东争取利益方面大公无私。你对他最忠心，愿意为他做任何事情，那个老板一定会把股东的利益放在第一位，并且总是强调团队。首先想着如何为股东创造更多的价值，而不是如何为个人带来价值。</p>

<h3 id="toc_4">以人为本，使命为先</h3>

<p>能干的领导和经理完成使命，伟大的领导和经理则是通过营造文化氛国，使员工感受到资识和尊重，诚实而且及时地处理绩效相关的反馈。</p>

<p>好的领导会确保那些产生最多价值的人得到最好的奖赏，确保那些表现远在一般人之上的人能够得到应有的休假。</p>

<p>关爱并不意味着要在组织内部实行福利或者终身雇用制度。关爱也并不意味着要去设置容易达成的目标。</p>

<p>检验“以人为本”是较为简单的，对于成功组织内经验丰富的领导，看看他手下有多少人曾经不断地跟随这位经理升迁就能知道。</p>

<p>“以已为本，使命为先”的领导把员工当成向上爬的梯子，踩着他们往上走。“以人为本，使命为先”的领导则为所有的一流员工建立好向上爬的梯子。</p>

<h3 id="toc_5">决策英明，以德服人</h3>

<p>一般而言，你要依靠合适的信息，不浪费时间，迅速行动并做出最佳决策。勇敢和决策是一个领导必然要关注的事情。</p>

<p>我们推崇一个信条，“你想让大家做什么，那么就教导什么，你教导什么，你的标准就是什么。”这里面的允许既指其他人也包括你自己。不论大事小情，这一条特别适用于那些违反道义的情况。</p>

<p>什么都不能摧毁你内在的可信性和你对组织的影响能力（甚至包括对不正当行为的理解）。绝不能把说谎、欺骗或偷盗与为股东创造财富相提并论。</p>

<h3 id="toc_6">用人不疑</h3>

<p>赋予团队权力，比任何领导力活动或行为对组织扩展能力的影响都大。授权是分配行为、责任和所有权，可能包括把部分或全部领导权和管理权交给个人或组织。以领导力而言，授权是提升个人、团队、领导和经理对其所负责事项的骄傲感。总体看来，相信自己被赋予权力的个人，比那些相信自己仅仅执行命令的个人，在做决策和管理过程中的效率更高。简而言之，真正的授权管理相当于加倍组织的产出，因为其本身不再成为所有活动的瓶颈。</p>

<p>然而，领导可能明确地把权力赋予其他人，但事实上却通过设定一个瓶颈限制着组织的产出。其结果是士气、产出和信任均遭到破坏。另外，赋予个人和团队权力并不是说领导对结果不再负责任。尽管领导可以授权，但是对达成的结果，他却要对股东负责。</p>

<h3 id="toc_7">与股东价值保持一致</h3>

<p>简单地说，在一个以营和为目的的公司里，你的工作就是为股东创造财富。更重要的是，你的工作就是要使股东的财富最大化。而如果是在非营利机构，这些是由慈善团体捐助或者别人付钱请你做好事的话，那么更常见的是创造一种情感财富。</p>

<h3 id="toc_8">变革型领导</h3>

<p>最有成效的领导往往通过理念来影响其组织，把团队的利益置于个人利益之上，为团队及其成员提供智能激发，为团队成员的福利和职业发展展现出诚实和个性化的关怀。</p>

<p>这些高尚的领导方式是通过理念的影响来实现，常被称为“变革型领导方式。”变革型领导方式的扩展性明显更好。不像交易型关注个人，变革型可以关注团队。从与个人讨论，变成与团队研究如何激励大家来完成任务。</p>

<h3 id="toc_9">愿景</h3>

<p>总的来说，领导在愿景和使命上面往往没有投入足够的精力。一般情况下，可以在年度计划会议中，安排一到两个小时有关愿景和使命的讨论。这个讨论团队可以选择，但是领导必须参与。</p>

<p>愿景应该是可以度量和验证的。但是在制定愿景的进程中，基于未来状态不是什么去定义最终状态耗时太多。因此，我们建议愿景要简洁地定义理想的终极状态。</p>

<p>展现愿景最简单的方法或许是从如何给一个人发出指令的角度整看。当指令发出时，你会同时说明如何确定目标是否达成。一个愿景应该符合下面这些标准：</p>

<ul>
<li>对理想未来的生动描述</li>
<li>为股东创造价值很重要</li>
<li>可度量</li>
<li>激动人心</li>
<li>结合信仰的因素</li>
<li>大致不变，但可根据需要修改</li>
<li>容易记忆</li>
</ul>

<h3 id="toc_10">使命</h3>

<p>如果愿景是对理想未来或者旅途终点的生动描绘，那么使命就是我们到达目的地的总路线或者行动计划。对使命的描述更亲于公司的现状，因为现状对到达理想状态或公司的愿景极为重要。使命应当包括一些目的，一些今天要做的事情和如何达成愿景的方向同愿景描述一样，使命的描述也应该是可以验证的。使命的描述应该符合下述条件：</p>

<ul>
<li>对当前状态和行动的描述</li>
<li>有目的感</li>
<li>可度量</li>
<li>一个大方向或一条通往愿景的道路</li>
</ul>

<h3 id="toc_11">目标</h3>

<p>目标就是旅行中确保我们走在正确道路上的路标或者里程碑。我们认为最好的目标是通过SMART来制订的。SMART是：</p>

<ul>
<li>具体(S）</li>
<li>可度量（M）</li>
<li>可达成(但是有挑战性）（A）</li>
<li>现实性(R）</li>
<li>时限性（或包含时间因素）（T）</li>
</ul>

<p>目标不会告诉人们如何做什么事情，但是会指明是否在沿着正确的道路前进。</p>

<h2 id="toc_12">成功的因果路线图</h2>

<p>作为一个领导，你可以做得最好和最容易的事情之一，是确保组织的成功，让大家理解他们所做的日常贡献怎么对实现组织的愿景起作用，进而为股东创造价值。我们把对这种关系的理解叫做成功的因果路线图。通常比较容易确定这种关系，否则我们就要质疑这一岗位存在的必要性。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[“源产控”系列（一）CentOS 8之初相识]]></title>
    <link href="https://zning.me/15932531416990.html"/>
    <updated>2020-06-27T18:19:01+08:00</updated>
    <id>https://zning.me/15932531416990.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p><strong>作者按：</strong>当下国内外软件开发领域，开源队伍的壮大趋势随着微软收购Github达到了新的高度，众多业界巨头例如Google、Facebook、IBM、Oracle、亚马逊、腾讯、阿里等亦在开源社区之中贡献着自己的智慧，并且带动了开源理念与风气。而随着开源的蓬勃发展，国产化软件也势头正猛，以阿里云OceanBase为代表的一系列国产化软件，凭借着它们强大的业务背景和一定体量的业务需求锤炼，成为了国产化软件的排头兵，带动了更多行业领域企业加入这股洪流之中。这两股强大的东风之流行，伴随着宏观经济贸易的不确定性，点燃了国民核心行业核心技术“自主可控”的星星之火。开源、国产化、自主可控，三个议题的碰撞会出现怎样的火花？“源产控”专题就在此应运而生。</p>

<p>慧响技术角“源产控”专题，将聚焦开源、国产化、自主可控三个方向的技术，以操作系统、中间件、数据库、程序应用等为粗分类，更新相关技术的发展趋势、探究技术核心的深度使用、系统总结技术整体架构，为对相关技术的学习者提供可观的资料，亦为个人同步学习总结的笔记，以飨读者。</p>

<p>本系列首文，即本文，以近期更新的开源操作系统CentOS 8的介绍为开篇，对CentOS 8进行一些简要的介绍，未来对开源操作系统CentOS 8，将陆续更新其基本使用总结、特性使用总结等方面的系列文章，敬请期待。</p>
</blockquote>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">CentOS是什么</a>
</li>
<li>
<a href="#toc_1">CentOS 8的新特性</a>
</li>
</ul>


<h2 id="toc_0">CentOS是什么</h2>

<p>下面简介摘自CentOS维基百科词条：</p>

<blockquote>
<p>CentOS（<strong>C</strong>ommunity <strong>Ent</strong>erprise <strong>O</strong>perating <strong>S</strong>ystem）是Linux发行版之一，它是来自于Red Hat Enterprise Linux（RHEL）依照开放源代码规定发布的源代码所编译而成。由于出自同样的源代码，因此有些要求高度稳定性的服务器以CentOS替代商业版的Red Hat Enterprise Linux使用。两者的不同，在于CentOS并不包含封闭源代码软件。CentOS 对上游代码的主要修改是为了移除不能自由使用的商标。2014年，CentOS宣布与Red Hat合作，但CentOS将会在新的委员会下继续运作，并不受RHEL的影响。</p>
</blockquote>

<p>比较通俗直白一点的说法，CentOS是Red Hat发行的商业版RHEL的开源替代版，也是作为Linux服务器使用的比较主流的开源操作系统之一。</p>

<p>CentOS第一次接触在2014年，也就是CentOS与Red Hat宣布合作的那一年。印象比较深的也是那一年发行的CentOS 7，由于其与Red Hat合作后，将RHEL 7的一些新特性例如<code>systemd</code>等引入，导致与其CentOS 6一些操作差距过大，再加上新系统教程较少，在系统上折腾了很久，也因此印象深刻。</p>

<p>截止本文撰稿，CentOS最新版本是<code>CentOS 8-2004</code>，其RHEL基础版为<code>RHEL 8.2</code>。</p>

<p><img src="media/15932531416990/15932603351520.jpg" alt=""/></p>

<p><img src="media/15932531416990/15932603152658.jpg" alt=""/></p>

<h2 id="toc_1">CentOS 8的新特性</h2>

<p>关于CentOS 8，主要的特性介绍如下：</p>

<p>CentOS项目是对<code>Red Hat Enterprise Linux</code>的100％兼容的重建，完全符合Red Hat的重新发布要求，并发布了一个新版本：<code>CentOS 8.0.1905</code>，适用于所有受支持的体系结构。</p>

<p>紧随<code>CentOS Linux 7.7</code>发行版之后，CentOS Linux 8现已正式发布，新版本基于<code>Red Hat Enterprise Linux 8.0</code>源，这意味着它具有混合云时代的所有强大的新特性和增强功能。</p>

<p><code>CentOS</code> 完全遵守 Red Hat 的再发行政策，并且致力与上游产品在功能上完全兼容。CentOS 对组件的修改主要是去除 Red Hat 的商标及美工图。</p>

<p>该版本还包含全新的 <code>CentOS Streams</code>，Centos Stream 是一个滚动发布的 Linux 发行版，它介于 Fedora Linux的上游开发和 RHEL 的下游开发之间而存在。你可以把 CentOS Streams 当成是用来体验最新红帽系 Linux 特性的一个版本，而无需等太久。</p>

<p>CentOS 8 主要改动和 <code>RedHat Enterprise Linux 8</code>是一致的，基于<code>Fedora 28</code>和内核版本 4.18, 为用户提供一个稳定的、安全的、一致的基础，跨越混合云部署，支持传统和新兴的工作负载所需的工具。此次发布的亮点包括：</p>

<p><strong>发行版</strong></p>

<p>通过 <code>BaseOS</code>和应用流 (<code>AppStream</code>) 仓库发布.</p>

<p><code>AppStream</code>是对传统 <code>rpm</code>格式的全新扩展，为一个组件同时提供多个主要版本</p>

<p><strong>软件管理</strong></p>

<p>YUM 包管理器基于 DNF 技术，提供模块化内容支持，增强了性能，并且提供了设计良好的 API 用于与其他工具集成</p>

<p><strong>Shell 和命令行工具</strong></p>

<p>RHEL 8 提供了版本控制工具: <code>Git 2.18</code>, <code>Mercurial 4.8</code>, 和 <code>Subversion 1.10.</code></p>

<p><strong>动态编程语言、Web 和数据库服务器</strong></p>

<p>Python 3.6 是默认的 Python 环境，有限支持 Python 2.7</p>

<p>Node.js 是在 RHEL 最新包含的，其他动态语言更新包括: PHP 7.2, Ruby 2.5, Perl 5.26, SWIG 3.0</p>

<p>RHEL 8 提供的数据库服务包括：MariaDB 10.3, MySQL 8.0, PostgreSQL 10, PostgreSQL 9.6, 和 Redis 5.</p>

<p>RHEL 8 提供Apache HTTP Server 2.4 以及首次引入的Nginx 1.14.</p>

<p>Squid 版本升级到 4.4 ，同时也首次提供Varnish Cache 6.0.</p>

<p><strong>桌面环境</strong></p>

<p>GNOME Shell 升级到 3.28.</p>

<p>GNOME 会话和显示管理使用 Wayland 作为默认的显示服务器，而 RHEL 7 默认的 X.Org server 依然提供</p>

<p><strong>安装程序以及镜像的创建</strong></p>

<p>Anaconda 安装程序可使用 LUKS2 磁盘加密，支持 NVDIMM 设备.</p>

<p>Image Builder 工具可以创建不同格式的自定义系统镜像，包括满足云平台的各种格式</p>

<p>支持使用硬件管理控制台 HMC 从 DVD 安装，同时也提供 IBM Z 主机的 Support Element (SE)</p>

<p><strong>内核</strong></p>

<p>扩展 Berkeley Packet Filtering (eBPF) 特性使得用户空间的各个点上附加自定义程序，包括 (sockets, trace points, packet reception) ，用于接收和处理数据。目前该特性还处于特性预览阶段</p>

<p>BPF Compiler Collection (BCC), 这是一个用来创建高效内核跟踪和操作的工具，目前处于技术预览阶段</p>

<p><strong>文件系统和存储</strong></p>

<p>LUKS version 2 (LUKS2) 格式替代旧的 LUKS (LUKS1) 格式. dm-crypt 子系统和 cryptsetup 工具现在使用 LUKS2 作为默认的加密卷格式</p>

<p><strong>安全</strong></p>

<p>默认的系统级的 加密策略, 用于配置核心加密子系统，覆盖 TLS, IPsec, SSH, DNSSEC, 和 Kerberos 协议。增加全新命令update-crypto-policies, 管理员可以轻松切换不同模式： <code>default</code>, <code>legacy</code>, <code>future</code>, 和 <code>fips</code>.</p>

<p>支持智能卡和硬件安全模块 (HSM) 的 PKCS #11</p>

<p><strong>网络</strong></p>

<p>nftables 框架替代 iptables 作为默认的网络包过滤工具</p>

<p>firewalld 守护进程使用 nftables 作为默认后端</p>

<p>支持 IPVLAN 虚拟网络驱动程序，用于连接多个容器</p>

<p>eXpress Data Path (XDP), XDP for Traffic Control (tc), 以及 Address Family eXpress Data Path (AF_XDP), 可作为部分 Berkeley Packet Filtering (eBPF) 扩展特性，目前还是技术预览阶段.</p>

<p><strong>虚拟化</strong></p>

<p>在RHEL8中创建的虚拟机中，现在支持并自动配置更现代的基于PCI Express的计算机类型（Q35）。这在虚拟设备的功能和兼容性方面提供了多种改进。</p>

<p>现在可以使用RHEL8Web控制台（也称为“驾驶舱”）创建和管理虚拟机。</p>

<p>qemu仿真器引入了沙箱功能，它为系统调用qemu可以执行的操作提供了可配置的限制，从而使虚拟机更加安全。</p>

<p><strong>编译器和开发工具</strong></p>

<p>GCC 编译器更新到 8.2 版本，支持更多 C++标准，更好的优化以及代码增强技术、提升警告和硬件特性支持</p>

<p>不同的代码生成、操作和调试工具现在可以处理 DWARF5 调试信息格式（体验阶段）</p>

<p>核心支持 eBPF 调试的工具包括BCC, PCP, 和 SystemTap.</p>

<p>glibc 库升级到 2.28 支持 Unicode 11, 更新的 Linux 系统调用，关键提升主要在 <code>DNS stub resolver</code> 、额外的安全加强和性能提升</p>

<p>RHEL 8 提供 <code>OpenJDK 11</code>, <code>OpenJDK 8</code>,<code>&lt;span&gt; &lt;/span&gt;IcedTea-Web</code>, 以及不同 Java 工具，如 <code>Ant</code>, <code>Maven</code>, 或 <code>Scala</code>.</p>

<p><strong>高可用和集群</strong></p>

<p>Pacemaker 集群资源管理器更新到最新版本 2.0.0, 修复了一系列 bug 以及功能做了提升</p>

<p>pcs 配置系统完全支持 Corosync 3, knet, 和节点名称</p>

<p><strong>相关链接</strong></p>

<p>CentOS 8 官方发行说明：</p>

<blockquote>
<p><a href="https://lists.centos.org/pipermail/centos-announce/2019-September/023449.html">https://lists.centos.org/pipermail/centos-announce/2019-September/023449.html</a></p>
</blockquote>

<p>完整的 RedHat 8 发行说明请看</p>

<blockquote>
<p><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/8.0_release_notes/overview">https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/8.0_release_notes/overview</a></p>
</blockquote>

<p>目前来看，CentOS 8的修改相比CentOS 6-&gt;7的修改来说，在系统管理层次上，完善相对变动要多不少，加上云管环境的逐步推广，也加上了云的特性。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[记一次OJ测试赛算法实现思路]]></title>
    <link href="https://zning.me/15918008140272.html"/>
    <updated>2020-06-10T22:53:34+08:00</updated>
    <id>https://zning.me/15918008140272.html</id>
    <content type="html"><![CDATA[
<p>今下午一个小时三道题的OJ测试赛告一段落。首先一首《凉凉》送给自己。整天工作CRUD以及近期代码几乎没碰，再加上OJ刷题的日子已经过去五年了，导致犯了好几个傻瓜的基础错误，浪费了原本就宝贵的时间。题二原本估计能拿一半分的（如果后台样例形式在我估计之中的话）。</p>

<p>现就测试赛三题中的前两题进行算法实现思路的整理。首先是本文中题目并不是原封不动的摘抄，这里只对关键要求进行回忆和描述，同时为了避免不必要的麻烦，因此题目描述进行了一定变形，但基本逻辑一致。另外我的思路不一定是正确和最优的，欢迎留言讨论。</p>

<span id="more"></span><!-- more -->

<p><strong>使用语言：</strong>Java。编译器可使用标准类以及<code>java.util.*</code>类。</p>

<h2 id="toc_0">题一：数组与遍历</h2>

<p><strong>题目描述：</strong>一家公司有n个营业点，编号从1到n，每次会对第i个营业点到第j个营业点供应m个货物。现有多条供货记录放置在一个二维数组里。假如有5个营业点，供货记录如下：<code>[[1,2,10],[2,3,30],[2,5,60]]</code>。程序需要计算出每个供货点接到的货物个数，本例返回结果应为<code>[10,100,90,60,60]</code>。二维数组大小、营业点个数有最高限制(不过我记得不深了，忘记是5000000还是50000000了)，0＜i＜j＜n。</p>

<p><strong>解题思路：</strong>本题形参表第一个为供货记录，二维数组；第二个为营业点个数，<code>int</code>型。本题很简单，简单的思路就是在二维数组遍历前建好以营业点为数量的一维数组。后遍历二维数组，再取第二层数组信息依次作为i,j,m，建循环由i到j，作为一维数组的下标，并加m进该一维数组元素中。</p>

<p>本题唯一注意的地方是自建一维数组下标。数组下标是0开头，n-1为最后一个。</p>

<h2 id="toc_1">题二：字符串</h2>

<p><strong>题目描述：</strong>根据所输入字符串进行还原。例如样例<code>WOSHINI[2BA]</code>，程序应该返回<code>WOSHINIBABA</code>。如样例<code>[3WO]SHINI[2BA]</code>，应返回<code>WOWOWOSHINIBABA</code>。样例有嵌套的情况，例如<code>WOSHI[3NI[2BA]]</code>，则应该是<code>WOSHINIBABANIBABANIBABA</code>。程序样例给出的嵌套不多于10层。</p>

<p><strong>解题思路：</strong>我这题当时做的时候没考虑嵌套情况。其实就是将字符串拆为字符，通过遍历判断中括号范围进行预先拼接，后与无括号部分进行最终拼接。</p>

<p>本来寻思这个题后面样例再怎么说有两三个非嵌套的吧……但是根据提交成绩来看好像就一个……另外这个题的弱智问题犯得太多了，不是丢<code>int</code>声明就是<code>char</code>转<code>int</code>的时候忘了根据ASCII码进行<code>-&#39;0&#39;</code>，尤其是最后这个错误，愣是查了半天代码才看出来这个错。改过来的时候就剩10分钟了……真的闹心啊……数据结构、算法和ACM学的东西全还给杨老师、沙老师和寰哥了……日了狗了……</p>

<p>目测此题也可以通过栈和递归实现。欢迎大佬给本蒟蒻分享方法。</p>

<h2 id="toc_2">题三：树</h2>

<p>由于时间有限，这个题没有时间看和做了。好像是个最小生成树问题。有心人可以在留言区评论。</p>

<p>由于这是测试赛，题目并不怎么难……这么看来正式赛专业组已经凉透了，只能大众组求虐了……做成这个熊样真的很暴露问题，以此为契机，计划每天做几个题，多刷刷LeetCode吧。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[关于更换Maven阿里云镜像的经验总结]]></title>
    <link href="https://zning.me/15905077159608.html"/>
    <updated>2020-05-26T23:41:55+08:00</updated>
    <id>https://zning.me/15905077159608.html</id>
    <content type="html"><![CDATA[
<p>今天需要编译<code>flink-1.9.0</code>和<code>flink-shaded</code>，由于网上所给信息是，在编译时需要在两个工程内设置非中央仓库的官方镜像源（<code>flink-1.9.0</code>已经自带）。</p>

<span id="more"></span><!-- more -->

<pre><code class="language-markup">&lt;profile&gt;
    &lt;id&gt;vendor-repos&lt;/id&gt;
    &lt;activation&gt;
        &lt;property&gt;
            &lt;name&gt;vendor-repos&lt;/name&gt;
        &lt;/property&gt;
    &lt;/activation&gt;
    &lt;!-- Add vendor maven repositories --&gt;
    &lt;repositories&gt;
        &lt;!-- Cloudera --&gt;
        &lt;repository&gt;
            &lt;id&gt;cloudera-releases&lt;/id&gt;
            &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos&lt;/url&gt;
            &lt;releases&gt;
                &lt;enabled&gt;true&lt;/enabled&gt;
            &lt;/releases&gt;
            &lt;snapshots&gt;
                &lt;enabled&gt;false&lt;/enabled&gt;
            &lt;/snapshots&gt;
        &lt;/repository&gt;
        &lt;!-- Hortonworks --&gt;
        &lt;repository&gt;
            &lt;id&gt;HDPReleases&lt;/id&gt;
            &lt;name&gt;HDP Releases&lt;/name&gt;
            &lt;url&gt;https://repo.hortonworks.com/content/repositories/releases/&lt;/url&gt;
            &lt;snapshots&gt;
                &lt;enabled&gt;false&lt;/enabled&gt;
            &lt;/snapshots&gt;
            &lt;releases&gt;
                &lt;enabled&gt;true&lt;/enabled&gt;
            &lt;/releases&gt;
        &lt;/repository&gt;
        &lt;repository&gt;
            &lt;id&gt;HortonworksJettyHadoop&lt;/id&gt;
            &lt;name&gt;HDP Jetty&lt;/name&gt;
            &lt;url&gt;https://repo.hortonworks.com/content/repositories/jetty-hadoop&lt;/url&gt;
            &lt;snapshots&gt;
                &lt;enabled&gt;false&lt;/enabled&gt;
            &lt;/snapshots&gt;
            &lt;releases&gt;
                &lt;enabled&gt;true&lt;/enabled&gt;
            &lt;/releases&gt;
        &lt;/repository&gt;
        &lt;!-- MapR --&gt;
        &lt;repository&gt;
            &lt;id&gt;mapr-releases&lt;/id&gt;
            &lt;url&gt;https://repository.mapr.com/maven/&lt;/url&gt;
            &lt;snapshots&gt;
                &lt;enabled&gt;false&lt;/enabled&gt;
            &lt;/snapshots&gt;
            &lt;releases&gt;
                &lt;enabled&gt;true&lt;/enabled&gt;
            &lt;/releases&gt;
        &lt;/repository&gt;
    &lt;/repositories&gt;
&lt;/profile&gt;
</code></pre>

<p>由于Maven本身是连接的apache.org的中央仓库镜像源，因此我在网上找到了阿里镜像源进行更换。其中教程一与二给的更换镜像源XML配置如下：</p>

<pre><code class="language-markup">&lt;mirrors&gt;
    &lt;mirror&gt;
        &lt;id&gt;aliyunmaven&lt;/id&gt;
        &lt;mirrorOf&gt;*&lt;/mirrorOf&gt;
        &lt;name&gt;阿里云公共仓库&lt;/name&gt;
        &lt;url&gt;https://maven.aliyun.com/repository/public&lt;/url&gt;
    &lt;/mirror&gt;
    &lt;mirror&gt;
        &lt;id&gt;aliyunmaven&lt;/id&gt;
        &lt;mirrorOf&gt;*&lt;/mirrorOf&gt;
        &lt;name&gt;阿里云谷歌仓库&lt;/name&gt;
        &lt;url&gt;https://maven.aliyun.com/repository/google&lt;/url&gt;
    &lt;/mirror&gt;
    &lt;mirror&gt;
        &lt;id&gt;aliyunmaven&lt;/id&gt;
        &lt;mirrorOf&gt;*&lt;/mirrorOf&gt;
        &lt;name&gt;阿里云阿帕奇仓库&lt;/name&gt;
        &lt;url&gt;https://maven.aliyun.com/repository/apache-snapshots&lt;/url&gt;
    &lt;/mirror&gt;
    &lt;mirror&gt;
        &lt;id&gt;aliyunmaven&lt;/id&gt;
        &lt;mirrorOf&gt;*&lt;/mirrorOf&gt;
        &lt;name&gt;阿里云spring仓库&lt;/name&gt;
        &lt;url&gt;https://maven.aliyun.com/repository/spring&lt;/url&gt;
    &lt;/mirror&gt;
    &lt;mirror&gt;
        &lt;id&gt;aliyunmaven&lt;/id&gt;
        &lt;mirrorOf&gt;*&lt;/mirrorOf&gt;
        &lt;name&gt;阿里云spring插件仓库&lt;/name&gt;
        &lt;url&gt;https://maven.aliyun.com/repository/spring-plugin&lt;/url&gt;
    &lt;/mirror&gt;
&lt;/mirrors&gt;
</code></pre>

<p>但是在更改后，我发现编译失败，说是在阿里云仓库找不到需要在我文章开头设置的仓库里找的包。但是我明明在工程的<code>pom.xml</code>里设置了四个非中央的官方仓库了呀？</p>

<p>后来又搜了两篇阿里云教程设置，我发现其XML配置为：</p>

<pre><code class="language-markup">&lt;mirrors&gt;
    &lt;mirror&gt;
        &lt;id&gt;aliyunmaven&lt;/id&gt;
        &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;
        &lt;name&gt;aliyun maven&lt;/name&gt;
        &lt;url&gt;https://maven.aliyun.com/repository/public &lt;/url&gt;
    &lt;/mirror&gt;
&lt;/mirrors&gt;
</code></pre>

<p>其中，<code>&lt;mirrorOf&gt;central&lt;/mirrorOf&gt;</code>同之前的仓库设置是不一样的，之前是<code>*</code>，现在是<code>central</code>。据此，我分析，由于通配符的问题，Maven全局的<code>Settings.xml</code>文件，将我工程内<code>pom.xml</code>设置的四个非中央官方仓库的设置给一并覆盖了，导致失效，从而无法让工程从应有的位置拉取依赖包，导致编译失败。</p>

<p>经过修改之后，<code>flink-shaded</code>工程目前编译已正常通过。确认是这个问题。因此我们可以知道的是：</p>

<ul>
<li>若设置了<code>&lt;mirrorOf&gt;*&lt;/mirrorOf&gt;</code>这个标签，则可能会导致工程内<code>pom.xml</code>相应设置的仓库失效。全局设置覆盖了工程设置。</li>
<li>应避免直接设置<code>&lt;mirrorOf&gt;*&lt;/mirrorOf&gt;</code>这个标签，而是设置指定仓库例如<code>&lt;mirrorOf&gt;central&lt;/mirrorOf&gt;</code>表示此为中央仓库的代理。</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[《动手学深度学习》PyTorch版学习记录（三）]]></title>
    <link href="https://zning.me/15820319987185.html"/>
    <updated>2020-02-18T21:19:58+08:00</updated>
    <id>https://zning.me/15820319987185.html</id>
    <content type="html"><![CDATA[
<p>近期参加了伯禹教育、Datawhale、和鲸科技组织的“《动手学深度学习》代码讲解PyTorch版”课程学习，本文为第一次学习记录。涉及知识点：过拟合、欠拟合的解决方案；梯度消失与梯度爆炸；循环神经网络。</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">过拟合、欠拟合的解决方案之权重衰减</a>
<ul>
<li>
<a href="#toc_1">方法</a>
</li>
<li>
<a href="#toc_2">高维线性回归实验</a>
</li>
<li>
<a href="#toc_3">从零开始实现</a>
<ul>
<li>
<a href="#toc_4">初始化模型参数</a>
</li>
<li>
<a href="#toc_5">定义$L_2$范数惩罚项</a>
</li>
<li>
<a href="#toc_6">定义训练和测试</a>
</li>
<li>
<a href="#toc_7">观察过拟合</a>
</li>
<li>
<a href="#toc_8">使用权重衰减</a>
</li>
</ul>
</li>
<li>
<a href="#toc_9">简洁实现</a>
</li>
<li>
<a href="#toc_10">小结</a>
</li>
<li>
<a href="#toc_11">练习</a>
</li>
</ul>
</li>
<li>
<a href="#toc_12">过拟合、欠拟合的解决方案之丢弃法</a>
<ul>
<li>
<a href="#toc_13">方法</a>
</li>
<li>
<a href="#toc_14">从零开始实现</a>
<ul>
<li>
<a href="#toc_15">定义模型参数</a>
</li>
<li>
<a href="#toc_16">定义模型</a>
</li>
<li>
<a href="#toc_17">训练和测试模型</a>
</li>
</ul>
</li>
<li>
<a href="#toc_18">简洁实现</a>
</li>
<li>
<a href="#toc_19">小结</a>
</li>
<li>
<a href="#toc_20">练习</a>
</li>
<li>
<a href="#toc_21">参考文献</a>
</li>
</ul>
</li>
<li>
<a href="#toc_22">数值稳定性和模型初始化</a>
<ul>
<li>
<a href="#toc_23">衰减和爆炸</a>
</li>
<li>
<a href="#toc_24">随机初始化模型参数</a>
<ul>
<li>
<a href="#toc_25">MXNet的默认随机初始化</a>
</li>
<li>
<a href="#toc_26">Xavier随机初始化</a>
</li>
</ul>
</li>
<li>
<a href="#toc_27">小结</a>
</li>
<li>
<a href="#toc_28">练习</a>
</li>
<li>
<a href="#toc_29">参考文献</a>
</li>
</ul>
</li>
<li>
<a href="#toc_30">循环神经网络</a>
<ul>
<li>
<a href="#toc_31">不含隐藏状态的神经网络</a>
</li>
<li>
<a href="#toc_32">含隐藏状态的循环神经网络</a>
</li>
<li>
<a href="#toc_33">应用：基于字符级循环神经网络的语言模型</a>
</li>
<li>
<a href="#toc_34">小结</a>
</li>
<li>
<a href="#toc_35">练习</a>
</li>
</ul>
</li>
<li>
<a href="#toc_36">循环神经网络的从零开始实现</a>
<ul>
<li>
<a href="#toc_37">one-hot向量</a>
</li>
<li>
<a href="#toc_38">初始化模型参数</a>
</li>
<li>
<a href="#toc_39">定义模型</a>
</li>
<li>
<a href="#toc_40">定义预测函数</a>
</li>
<li>
<a href="#toc_41">裁剪梯度</a>
</li>
<li>
<a href="#toc_42">困惑度</a>
</li>
<li>
<a href="#toc_43">定义模型训练函数</a>
</li>
<li>
<a href="#toc_44">训练模型并创作歌词</a>
</li>
<li>
<a href="#toc_45">小结</a>
</li>
<li>
<a href="#toc_46">练习</a>
</li>
</ul>
</li>
</ul>


<h2 id="toc_0">过拟合、欠拟合的解决方案之权重衰减</h2>

<p>上一节中我们观察了过拟合现象，即模型的训练误差远小于它在测试集上的误差。虽然增大训练数据集可能会减轻过拟合，但是获取额外的训练数据往往代价高昂。本节介绍应对过拟合问题的常用方法：权重衰减（weight decay）。</p>

<h3 id="toc_1">方法</h3>

<p>权重衰减等价于\(L_2\)范数正则化（regularization）。正则化通过为模型损失函数添加惩罚项使学出的模型参数值较小，是应对过拟合的常用手段。我们先描述\(L_2\)范数正则化，再解释它为何又称权重衰减。</p>

<p>\(L_2\)范数正则化在模型原损失函数基础上添加\(L_2\)范数惩罚项，从而得到训练所需要最小化的函数。\(L_2\)范数惩罚项指的是模型权重参数每个元素的平方和与一个正的常数的乘积。以<a href="linear-regression.ipynb">“线性回归”</a>一节中的线性回归损失函数</p>

<p>\[\ell(w_1, w_2, b) = \frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right)^2\]</p>

<p>为例，其中\(w_1, w_2\)是权重参数，\(b\)是偏差参数，样本\(i\)的输入为\(x_1^{(i)}, x_2^{(i)}\)，标签为\(y^{(i)}\)，样本数为\(n\)。将权重参数用向量\(\boldsymbol{w} = [w_1, w_2]\)表示，带有\(L_2\)范数惩罚项的新损失函数为</p>

<p>\[\ell(w_1, w_2, b) + \frac{\lambda}{2n} \|\boldsymbol{w}\|^2,\]</p>

<p>其中超参数\(\lambda &gt; 0\)。当权重参数均为0时，惩罚项最小。当\(\lambda\)较大时，惩罚项在损失函数中的比重较大，这通常会使学到的权重参数的元素较接近0。当\(\lambda\)设为0时，惩罚项完全不起作用。上式中\(L_2\)范数平方\(\|\boldsymbol{w}\|^2\)展开后得到\(w_1^2 + w_2^2\)。有了\(L_2\)范数惩罚项后，在小批量随机梯度下降中，我们将<a href="linear-regression.ipynb">“线性回归”</a>一节中权重\(w_1\)和\(w_2\)的迭代方式更改为</p>

<p>\[<br/>
\begin{aligned}<br/>
w_1 &amp;\leftarrow \left(1- \frac{\eta\lambda}{|\mathcal{B}|} \right)w_1 -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_1^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right),\\<br/>
w_2 &amp;\leftarrow \left(1- \frac{\eta\lambda}{|\mathcal{B}|} \right)w_2 -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_2^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right).<br/>
\end{aligned}<br/>
\]</p>

<p>可见，\(L_2\)范数正则化令权重\(w_1\)和\(w_2\)先自乘小于1的数，再减去不含惩罚项的梯度。因此，\(L_2\)范数正则化又叫权重衰减。权重衰减通过惩罚绝对值较大的模型参数为需要学习的模型增加了限制，这可能对过拟合有效。实际场景中，我们有时也在惩罚项中添加偏差元素的平方和。</p>

<h3 id="toc_2">高维线性回归实验</h3>

<p>下面，我们以高维线性回归为例来引入一个过拟合问题，并使用权重衰减来应对过拟合。设数据样本特征的维度为\(p\)。对于训练数据集和测试数据集中特征为\(x_1, x_2, \ldots, x_p\)的任一样本，我们使用如下的线性函数来生成该样本的标签：</p>

<p>\[y = 0.05 + \sum_{i = 1}^p 0.01x_i +  \epsilon,\]</p>

<p>其中噪声项\(\epsilon\)服从均值为0、标准差为0.01的正态分布。为了较容易地观察过拟合，我们考虑高维线性回归问题，如设维度\(p=200\)；同时，我们特意把训练数据集的样本数设低，如20。</p>

<pre><code class="language-python">%matplotlib inline
import d2lzh as d2l
from mxnet import autograd, gluon, init, nd
from mxnet.gluon import data as gdata, loss as gloss, nn

n_train, n_test, num_inputs = 20, 100, 200
true_w, true_b = nd.ones((num_inputs, 1)) * 0.01, 0.05

features = nd.random.normal(shape=(n_train + n_test, num_inputs))
labels = nd.dot(features, true_w) + true_b
labels += nd.random.normal(scale=0.01, shape=labels.shape)
train_features, test_features = features[:n_train, :], features[n_train:, :]
train_labels, test_labels = labels[:n_train], labels[n_train:]
</code></pre>

<h3 id="toc_3">从零开始实现</h3>

<p>下面先介绍从零开始实现权重衰减的方法。我们通过在目标函数后添加\(L_2\)范数惩罚项来实现权重衰减。</p>

<h4 id="toc_4">初始化模型参数</h4>

<p>首先，定义随机初始化模型参数的函数。该函数为每个参数都附上梯度。</p>

<pre><code class="language-python">def init_params():
    w = nd.random.normal(scale=1, shape=(num_inputs, 1))
    b = nd.zeros(shape=(1,))
    w.attach_grad()
    b.attach_grad()
    return [w, b]
</code></pre>

<h4 id="toc_5">定义\(L_2\)范数惩罚项</h4>

<p>下面定义\(L_2\)范数惩罚项。这里只惩罚模型的权重参数。</p>

<pre><code class="language-python">def l2_penalty(w):
    return (w**2).sum() / 2
</code></pre>

<h4 id="toc_6">定义训练和测试</h4>

<p>下面定义如何在训练数据集和测试数据集上分别训练和测试模型。与前面几节中不同的是，这里在计算最终的损失函数时添加了\(L_2\)范数惩罚项。</p>

<pre><code class="language-python">batch_size, num_epochs, lr = 1, 100, 0.003
net, loss = d2l.linreg, d2l.squared_loss
train_iter = gdata.DataLoader(gdata.ArrayDataset(
    train_features, train_labels), batch_size, shuffle=True)

def fit_and_plot(lambd):
    w, b = init_params()
    train_ls, test_ls = [], []
    for _ in range(num_epochs):
        for X, y in train_iter:
            with autograd.record():
                # 添加了L2范数惩罚项
                l = loss(net(X, w, b), y) + lambd * l2_penalty(w)
            l.backward()
            d2l.sgd([w, b], lr, batch_size)
        train_ls.append(loss(net(train_features, w, b),
                             train_labels).mean().asscalar())
        test_ls.append(loss(net(test_features, w, b),
                            test_labels).mean().asscalar())
    d2l.semilogy(range(1, num_epochs + 1), train_ls, &#39;epochs&#39;, &#39;loss&#39;,
                 range(1, num_epochs + 1), test_ls, [&#39;train&#39;, &#39;test&#39;])
    print(&#39;L2 norm of w:&#39;, w.norm().asscalar())
</code></pre>

<h4 id="toc_7">观察过拟合</h4>

<p>接下来，让我们训练并测试高维线性回归模型。当<code>lambd</code>设为0时，我们没有使用权重衰减。结果训练误差远小于测试集上的误差。这是典型的过拟合现象。</p>

<pre><code class="language-python">fit_and_plot(lambd=0)
</code></pre>

<p><img src="media/15820319987185/output_9_0.svg" alt="output_9_0"/></p>

<pre><code class="language-text">L2 norm of w: 11.611941
</code></pre>

<h4 id="toc_8">使用权重衰减</h4>

<p>下面我们使用权重衰减。可以看出，训练误差虽然有所提高，但测试集上的误差有所下降。过拟合现象得到一定程度的缓解。另外，权重参数的\(L_2\)范数比不使用权重衰减时的更小，此时的权重参数更接近0。</p>

<pre><code class="language-python">fit_and_plot(lambd=3)
</code></pre>

<p><img src="media/15820319987185/output_11_0.svg" alt="output_11_0"/></p>

<pre><code class="language-text">L2 norm of w: 0.040959217
</code></pre>

<h3 id="toc_9">简洁实现</h3>

<p>这里我们直接在构造<code>Trainer</code>实例时通过<code>wd</code>参数来指定权重衰减超参数。默认下，Gluon会对权重和偏差同时衰减。我们可以分别对权重和偏差构造<code>Trainer</code>实例，从而只对权重衰减。</p>

<pre><code class="language-python">def fit_and_plot_gluon(wd):
    net = nn.Sequential()
    net.add(nn.Dense(1))
    net.initialize(init.Normal(sigma=1))
    # 对权重参数衰减。权重名称一般是以weight结尾
    trainer_w = gluon.Trainer(net.collect_params(&#39;.*weight&#39;), &#39;sgd&#39;,
                              {&#39;learning_rate&#39;: lr, &#39;wd&#39;: wd})
    # 不对偏差参数衰减。偏差名称一般是以bias结尾
    trainer_b = gluon.Trainer(net.collect_params(&#39;.*bias&#39;), &#39;sgd&#39;,
                              {&#39;learning_rate&#39;: lr})
    train_ls, test_ls = [], []
    for _ in range(num_epochs):
        for X, y in train_iter:
            with autograd.record():
                l = loss(net(X), y)
            l.backward()
            # 对两个Trainer实例分别调用step函数，从而分别更新权重和偏差
            trainer_w.step(batch_size)
            trainer_b.step(batch_size)
        train_ls.append(loss(net(train_features),
                             train_labels).mean().asscalar())
        test_ls.append(loss(net(test_features),
                            test_labels).mean().asscalar())
    d2l.semilogy(range(1, num_epochs + 1), train_ls, &#39;epochs&#39;, &#39;loss&#39;,
                 range(1, num_epochs + 1), test_ls, [&#39;train&#39;, &#39;test&#39;])
    print(&#39;L2 norm of w:&#39;, net[0].weight.data().norm().asscalar())
</code></pre>

<p>与从零开始实现权重衰减的实验现象类似，使用权重衰减可以在一定程度上缓解过拟合问题。</p>

<pre><code class="language-python">fit_and_plot_gluon(0)
</code></pre>

<p><img src="media/15820319987185/output_15_0.svg" alt="output_15_0"/></p>

<pre><code class="language-text">L2 norm of w: 13.311795
</code></pre>

<pre><code class="language-python">fit_and_plot_gluon(3)
</code></pre>

<p><img src="media/15820319987185/output_16_0.svg" alt="output_16_0"/></p>

<pre><code class="language-text">L2 norm of w: 0.032668564
</code></pre>

<h3 id="toc_10">小结</h3>

<ul>
<li>正则化通过为模型损失函数添加惩罚项使学出的模型参数值较小，是应对过拟合的常用手段。</li>
<li>权重衰减等价于\(L_2\)范数正则化，通常会使学到的权重参数的元素较接近0。</li>
<li>权重衰减可以通过Gluon的<code>wd</code>超参数来指定。</li>
<li>可以定义多个<code>Trainer</code>实例对不同的模型参数使用不同的迭代方法。</li>
</ul>

<h3 id="toc_11">练习</h3>

<ul>
<li>回顾一下训练误差和泛化误差的关系。除了权重衰减、增大训练量以及使用复杂度合适的模型，你还能想到哪些办法来应对过拟合？</li>
<li>如果你了解贝叶斯统计，你觉得权重衰减对应贝叶斯统计里的哪个重要概念？</li>
<li>调节实验中的权重衰减超参数，观察并分析实验结果。</li>
</ul>

<h2 id="toc_12">过拟合、欠拟合的解决方案之丢弃法</h2>

<p>除了前一节介绍的权重衰减以外，深度学习模型常常使用丢弃法（dropout）[1] 来应对过拟合问题。丢弃法有一些不同的变体。本节中提到的丢弃法特指倒置丢弃法（inverted dropout）。</p>

<h3 id="toc_13">方法</h3>

<p>回忆一下，<a href="mlp.ipynb">“多层感知机”</a>一节的图3.3描述了一个单隐藏层的多层感知机。其中输入个数为4，隐藏单元个数为5，且隐藏单元\(h_i\)（\(i=1, \ldots, 5\)）的计算表达式为</p>

<p>\[h_i = \phi\left(x_1 w_{1i} + x_2 w_{2i} + x_3 w_{3i} + x_4 w_{4i} + b_i\right),\]</p>

<p>这里\(\phi\)是激活函数，\(x_1, \ldots, x_4\)是输入，隐藏单元\(i\)的权重参数为\(w_{1i}, \ldots, w_{4i}\)，偏差参数为\(b_i\)。当对该隐藏层使用丢弃法时，该层的隐藏单元将有一定概率被丢弃掉。设丢弃概率为\(p\)，<br/>
那么有\(p\)的概率\(h_i\)会被清零，有\(1-p\)的概率\(h_i\)会除以\(1-p\)做拉伸。丢弃概率是丢弃法的超参数。具体来说，设随机变量\(\xi_i\)为0和1的概率分别为\(p\)和\(1-p\)。使用丢弃法时我们计算新的隐藏单元\(h_i&#39;\)</p>

<p>\[h_i&#39; = \frac{\xi_i}{1-p} h_i.\]</p>

<p>由于\(E(\xi_i) = 1-p\)，因此</p>

<p>\[E(h_i&#39;) = \frac{E(\xi_i)}{1-p}h_i = h_i.\]</p>

<p>即丢弃法不改变其输入的期望值。让我们对图3.3中的隐藏层使用丢弃法，一种可能的结果如图3.5所示，其中\(h_2\)和\(h_5\)被清零。这时输出值的计算不再依赖\(h_2\)和\(h_5\)，在反向传播时，与这两个隐藏单元相关的权重的梯度均为0。由于在训练中隐藏层神经元的丢弃是随机的，即\(h_1, \ldots, h_5\)都有可能被清零，输出层的计算无法过度依赖\(h_1, \ldots, h_5\)中的任一个，从而在训练模型时起到正则化的作用，并可以用来应对过拟合。在测试模型时，我们为了拿到更加确定性的结果，一般不使用丢弃法。</p>

<p><img src="media/15820319987185/dropout.svg" alt="隐藏层使用了丢弃法的多层感知机"/></p>

<h3 id="toc_14">从零开始实现</h3>

<p>根据丢弃法的定义，我们可以很容易地实现它。下面的<code>dropout</code>函数将以<code>drop_prob</code>的概率丢弃<code>NDArray</code>输入<code>X</code>中的元素。</p>

<pre><code class="language-python">import d2lzh as d2l
from mxnet import autograd, gluon, init, nd
from mxnet.gluon import loss as gloss, nn

def dropout(X, drop_prob):
    assert 0 &lt;= drop_prob &lt;= 1
    keep_prob = 1 - drop_prob
    # 这种情况下把全部元素都丢弃
    if keep_prob == 0:
        return X.zeros_like()
    mask = nd.random.uniform(0, 1, X.shape) &lt; keep_prob
    return mask * X / keep_prob
</code></pre>

<p>我们运行几个例子来测试一下<code>dropout</code>函数。其中丢弃概率分别为0、0.5和1。</p>

<pre><code class="language-python">X = nd.arange(16).reshape((2, 8))
dropout(X, 0)
</code></pre>

<pre><code class="language-text">[[ 0.  1.  2.  3.  4.  5.  6.  7.]
 [ 8.  9. 10. 11. 12. 13. 14. 15.]]
&lt;NDArray 2x8 @cpu(0)&gt;
</code></pre>

<pre><code class="language-python">dropout(X, 0.5)
</code></pre>

<pre><code class="language-text">[[ 0.  2.  4.  6.  0.  0.  0. 14.]
 [ 0. 18.  0.  0. 24. 26. 28.  0.]]
&lt;NDArray 2x8 @cpu(0)&gt;
</code></pre>

<pre><code class="language-python">dropout(X, 1)
</code></pre>

<pre><code class="language-text">[[0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0.]]
&lt;NDArray 2x8 @cpu(0)&gt;
</code></pre>

<h4 id="toc_15">定义模型参数</h4>

<p>实验中，我们依然使用<a href="softmax-regression-scratch.ipynb">“softmax回归的从零开始实现”</a>一节中介绍的Fashion-MNIST数据集。我们将定义一个包含两个隐藏层的多层感知机，其中两个隐藏层的输出个数都是256。</p>

<pre><code class="language-python">num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256

W1 = nd.random.normal(scale=0.01, shape=(num_inputs, num_hiddens1))
b1 = nd.zeros(num_hiddens1)
W2 = nd.random.normal(scale=0.01, shape=(num_hiddens1, num_hiddens2))
b2 = nd.zeros(num_hiddens2)
W3 = nd.random.normal(scale=0.01, shape=(num_hiddens2, num_outputs))
b3 = nd.zeros(num_outputs)

params = [W1, b1, W2, b2, W3, b3]
for param in params:
    param.attach_grad()
</code></pre>

<h4 id="toc_16">定义模型</h4>

<p>下面定义的模型将全连接层和激活函数ReLU串起来，并对每个激活函数的输出使用丢弃法。我们可以分别设置各个层的丢弃概率。通常的建议是把靠近输入层的丢弃概率设得小一点。在这个实验中，我们把第一个隐藏层的丢弃概率设为0.2，把第二个隐藏层的丢弃概率设为0.5。我们可以通过<a href="../chapter_prerequisite/autograd.ipynb">“自动求梯度”</a>一节中介绍的<code>is_training</code>函数来判断运行模式为训练还是测试，并只需在训练模式下使用丢弃法。</p>

<pre><code class="language-python">drop_prob1, drop_prob2 = 0.2, 0.5

def net(X):
    X = X.reshape((-1, num_inputs))
    H1 = (nd.dot(X, W1) + b1).relu()
    if autograd.is_training():  # 只在训练模型时使用丢弃法
        H1 = dropout(H1, drop_prob1)  # 在第一层全连接后添加丢弃层
    H2 = (nd.dot(H1, W2) + b2).relu()
    if autograd.is_training():
        H2 = dropout(H2, drop_prob2)  # 在第二层全连接后添加丢弃层
    return nd.dot(H2, W3) + b3
</code></pre>

<h4 id="toc_17">训练和测试模型</h4>

<p>这部分与之前多层感知机的训练和测试类似。</p>

<pre><code class="language-python">num_epochs, lr, batch_size = 5, 0.5, 256
loss = gloss.SoftmaxCrossEntropyLoss()
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,
              params, lr)
</code></pre>

<pre><code class="language-text">epoch 1, loss 1.1250, train acc 0.564, test acc 0.773
epoch 2, loss 0.5834, train acc 0.784, test acc 0.784
epoch 3, loss 0.4944, train acc 0.820, test acc 0.847
epoch 4, loss 0.4462, train acc 0.837, test acc 0.858
epoch 5, loss 0.4169, train acc 0.848, test acc 0.867
</code></pre>

<h3 id="toc_18">简洁实现</h3>

<p>在Gluon中，我们只需要在全连接层后添加<code>Dropout</code>层并指定丢弃概率。在训练模型时，<code>Dropout</code>层将以指定的丢弃概率随机丢弃上一层的输出元素；在测试模型时，<code>Dropout</code>层并不发挥作用。</p>

<pre><code class="language-python">net = nn.Sequential()
net.add(nn.Dense(256, activation=&quot;relu&quot;),
        nn.Dropout(drop_prob1),  # 在第一个全连接层后添加丢弃层
        nn.Dense(256, activation=&quot;relu&quot;),
        nn.Dropout(drop_prob2),  # 在第二个全连接层后添加丢弃层
        nn.Dense(10))
net.initialize(init.Normal(sigma=0.01))
</code></pre>

<p>下面训练并测试模型。</p>

<pre><code class="language-python">trainer = gluon.Trainer(net.collect_params(), &#39;sgd&#39;, {&#39;learning_rate&#39;: lr})
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None,
              None, trainer)
</code></pre>

<pre><code class="language-text">epoch 1, loss 1.1132, train acc 0.565, test acc 0.782
epoch 2, loss 0.5729, train acc 0.788, test acc 0.840
epoch 3, loss 0.4903, train acc 0.821, test acc 0.847
epoch 4, loss 0.4407, train acc 0.838, test acc 0.860
epoch 5, loss 0.4163, train acc 0.847, test acc 0.839
</code></pre>

<h3 id="toc_19">小结</h3>

<ul>
<li>我们可以通过使用丢弃法应对过拟合。</li>
<li>丢弃法只在训练模型时使用。</li>
</ul>

<h3 id="toc_20">练习</h3>

<ul>
<li>如果把本节中的两个丢弃概率超参数对调，会有什么结果？</li>
<li>增大迭代周期数，比较使用丢弃法与不使用丢弃法的结果。</li>
<li>如果将模型改得更加复杂，如增加隐藏层单元，使用丢弃法应对过拟合的效果是否更加明显？</li>
<li>以本节中的模型为例，比较使用丢弃法与权重衰减的效果。如果同时使用丢弃法和权重衰减，效果会如何？</li>
</ul>

<h3 id="toc_21">参考文献</h3>

<p>[1] Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., &amp; Salakhutdinov, R. (2014). Dropout: a simple way to prevent neural networks from overfitting. JMLR</p>

<h2 id="toc_22">数值稳定性和模型初始化</h2>

<p>理解了正向传播与反向传播以后，我们来讨论一下深度学习模型的数值稳定性问题以及模型参数的初始化方法。深度模型有关数值稳定性的典型问题是衰减（vanishing）和爆炸（explosion）。</p>

<h3 id="toc_23">衰减和爆炸</h3>

<p>当神经网络的层数较多时，模型的数值稳定性容易变差。假设一个层数为\(L\)的多层感知机的第\(l\)层\(\boldsymbol{H}^{(l)}\)的权重参数为\(\boldsymbol{W}^{(l)}\)，输出层\(\boldsymbol{H}^{(L)}\)的权重参数为\(\boldsymbol{W}^{(L)}\)。为了便于讨论，不考虑偏差参数，且设所有隐藏层的激活函数为恒等映射（identity mapping）\(\phi(x) = x\)。给定输入\(\boldsymbol{X}\)，多层感知机的第\(l\)层的输出\(\boldsymbol{H}^{(l)} = \boldsymbol{X} \boldsymbol{W}^{(1)} \boldsymbol{W}^{(2)} \ldots \boldsymbol{W}^{(l)}\)。此时，如果层数\(l\)较大，\(\boldsymbol{H}^{(l)}\)的计算可能会出现衰减或爆炸。举个例子，假设输入和所有层的权重参数都是标量，如权重参数为0.2和5，多层感知机的第30层输出为输入\(\boldsymbol{X}\)分别与\(0.2^{30} \approx 1 \times 10^{-21}\)（衰减）和\(5^{30} \approx 9 \times 10^{20}\)（爆炸）的乘积。类似地，当层数较多时，梯度的计算也更容易出现衰减或爆炸。</p>

<p>随着内容的不断深入，我们会在后面的章节进一步介绍深度学习的数值稳定性问题以及解决方法。</p>

<h3 id="toc_24">随机初始化模型参数</h3>

<p>在神经网络中，通常需要随机初始化模型参数。下面我们来解释这样做的原因。</p>

<p>回顾<a href="mlp.ipynb">“多层感知机”</a>一节图3.3描述的多层感知机。为了方便解释，假设输出层只保留一个输出单元\(o_1\)（删去\(o_2\)和\(o_3\)以及指向它们的箭头），且隐藏层使用相同的激活函数。如果将每个隐藏单元的参数都初始化为相等的值，那么在正向传播时每个隐藏单元将根据相同的输入计算出相同的值，并传递至输出层。在反向传播中，每个隐藏单元的参数梯度值相等。因此，这些参数在使用基于梯度的优化算法迭代后值依然相等。之后的迭代也是如此。在这种情况下，无论隐藏单元有多少，隐藏层本质上只有1个隐藏单元在发挥作用。因此，正如在前面的实验中所做的那样，我们通常将神经网络的模型参数，特别是权重参数，进行随机初始化。</p>

<h4 id="toc_25">MXNet的默认随机初始化</h4>

<p>随机初始化模型参数的方法有很多。在<a href="linear-regression-gluon.ipynb">“线性回归的简洁实现”</a>一节中，我们使用<code>net.initialize(init.Normal(sigma=0.01))</code>使模型<code>net</code>的权重参数采用正态分布的随机初始化方式。如果不指定初始化方法，如<code>net.initialize()</code>，MXNet将使用默认的随机初始化方法：权重参数每个元素随机采样于-0.07到0.07之间的均匀分布，偏差参数全部清零。</p>

<h4 id="toc_26">Xavier随机初始化</h4>

<p>还有一种比较常用的随机初始化方法叫作Xavier随机初始化 [1]。<br/>
假设某全连接层的输入个数为\(a\)，输出个数为\(b\)，Xavier随机初始化将使该层中权重参数的每个元素都随机采样于均匀分布</p>

<p>\[U\left(-\sqrt{\frac{6}{a+b}}, \sqrt{\frac{6}{a+b}}\right).\]</p>

<p>它的设计主要考虑到，模型参数初始化后，每层输出的方差不该受该层输入个数影响，且每层梯度的方差也不该受该层输出个数影响。</p>

<h3 id="toc_27">小结</h3>

<ul>
<li>深度模型有关数值稳定性的典型问题是衰减和爆炸。当神经网络的层数较多时，模型的数值稳定性容易变差。</li>
<li>我们通常需要随机初始化神经网络的模型参数，如权重参数。</li>
</ul>

<h3 id="toc_28">练习</h3>

<ul>
<li>有人说随机初始化模型参数是为了“打破对称性”。这里的“对称”应如何理解？</li>
<li>是否可以将线性回归或softmax回归中所有的权重参数都初始化为相同值？</li>
</ul>

<h3 id="toc_29">参考文献</h3>

<p>[1] Glorot, X., &amp; Bengio, Y. (2010, March). Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics (pp. 249-256).</p>

<h2 id="toc_30">循环神经网络</h2>

<p>上一节介绍的\(n\)元语法中，时间步\(t\)的词\(w_t\)基于前面所有词的条件概率只考虑了最近时间步的\(n-1\)个词。如果要考虑比\(t-(n-1)\)更早时间步的词对\(w_t\)的可能影响，我们需要增大\(n\)。但这样模型参数的数量将随之呈指数级增长（可参考上一节的练习）。</p>

<p>本节将介绍循环神经网络。它并非刚性地记忆所有固定长度的序列，而是通过隐藏状态来存储之前时间步的信息。首先我们回忆一下前面介绍过的多层感知机，然后描述如何添加隐藏状态来将它变成循环神经网络。</p>

<h3 id="toc_31">不含隐藏状态的神经网络</h3>

<p>让我们考虑一个含单隐藏层的多层感知机。给定样本数为\(n\)、输入个数（特征数或特征向量维度）为\(d\)的小批量数据样本\(\boldsymbol{X} \in \mathbb{R}^{n \times d}\)。设隐藏层的激活函数为\(\phi\)，那么隐藏层的输出\(\boldsymbol{H} \in \mathbb{R}^{n \times h}\)计算为</p>

<p>\[\boldsymbol{H} = \phi(\boldsymbol{X} \boldsymbol{W}_{xh} + \boldsymbol{b}_h),\]</p>

<p>其中隐藏层权重参数\(\boldsymbol{W}_{xh} \in \mathbb{R}^{d \times h}\)，隐藏层偏差参数 \(\boldsymbol{b}_h \in \mathbb{R}^{1 \times h}\)，\(h\)为隐藏单元个数。上式相加的两项形状不同，因此将按照广播机制相加（参见<a href="../chapter_prerequisite/ndarray.ipynb">“数据操作”</a>一节）。把隐藏变量\(\boldsymbol{H}\)作为输出层的输入，且设输出个数为\(q\)（如分类问题中的类别数），输出层的输出为</p>

<p>\[\boldsymbol{O} = \boldsymbol{H} \boldsymbol{W}_{hq} + \boldsymbol{b}_q,\]</p>

<p>其中输出变量\(\boldsymbol{O} \in \mathbb{R}^{n \times q}\), 输出层权重参数\(\boldsymbol{W}_{hq} \in \mathbb{R}^{h \times q}\), 输出层偏差参数\(\boldsymbol{b}_q \in \mathbb{R}^{1 \times q}\)。如果是分类问题，我们可以使用\(\text{softmax}(\boldsymbol{O})\)来计算输出类别的概率分布。</p>

<h3 id="toc_32">含隐藏状态的循环神经网络</h3>

<p>现在我们考虑输入数据存在时间相关性的情况。假设\(\boldsymbol{X}_t \in \mathbb{R}^{n \times d}\)是序列中时间步\(t\)的小批量输入，\(\boldsymbol{H}_t  \in \mathbb{R}^{n \times h}\)是该时间步的隐藏变量。与多层感知机不同的是，这里我们保存上一时间步的隐藏变量\(\boldsymbol{H}_{t-1}\)，并引入一个新的权重参数\(\boldsymbol{W}_{hh} \in \mathbb{R}^{h \times h}\)，该参数用来描述在当前时间步如何使用上一时间步的隐藏变量。具体来说，时间步\(t\)的隐藏变量的计算由当前时间步的输入和上一时间步的隐藏变量共同决定：</p>

<p>\[\boldsymbol{H}_t = \phi(\boldsymbol{X}_t \boldsymbol{W}_{xh} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{hh}  + \boldsymbol{b}_h).\]</p>

<p>与多层感知机相比，我们在这里添加了\(\boldsymbol{H}_{t-1} \boldsymbol{W}_{hh}\)一项。由上式中相邻时间步的隐藏变量\(\boldsymbol{H}_t\)和\(\boldsymbol{H}_{t-1}\)之间的关系可知，这里的隐藏变量能够捕捉截至当前时间步的序列的历史信息，就像是神经网络当前时间步的状态或记忆一样。因此，该隐藏变量也称为隐藏状态。由于隐藏状态在当前时间步的定义使用了上一时间步的隐藏状态，上式的计算是循环的。使用循环计算的网络即循环神经网络（recurrent neural network）。</p>

<p>循环神经网络有很多种不同的构造方法。含上式所定义的隐藏状态的循环神经网络是极为常见的一种。若无特别说明，本章中的循环神经网络均基于上式中隐藏状态的循环计算。在时间步\(t\)，输出层的输出和多层感知机中的计算类似：</p>

<p>\[\boldsymbol{O}_t = \boldsymbol{H}_t \boldsymbol{W}_{hq} + \boldsymbol{b}_q.\]</p>

<p>循环神经网络的参数包括隐藏层的权重\(\boldsymbol{W}_{xh} \in \mathbb{R}^{d \times h}\)、\(\boldsymbol{W}_{hh} \in \mathbb{R}^{h \times h}\)和偏差 \(\boldsymbol{b}_h \in \mathbb{R}^{1 \times h}\)，以及输出层的权重\(\boldsymbol{W}_{hq} \in \mathbb{R}^{h \times q}\)和偏差\(\boldsymbol{b}_q \in \mathbb{R}^{1 \times q}\)。值得一提的是，即便在不同时间步，循环神经网络也始终使用这些模型参数。因此，循环神经网络模型参数的数量不随时间步的增加而增长。</p>

<p>图6.1展示了循环神经网络在3个相邻时间步的计算逻辑。在时间步\(t\)，隐藏状态的计算可以看成是将输入\(\boldsymbol{X}_t\)和前一时间步隐藏状态\(\boldsymbol{H}_{t-1}\)连结后输入一个激活函数为\(\phi\)的全连接层。该全连接层的输出就是当前时间步的隐藏状态\(\boldsymbol{H}_t\)，且模型参数为\(\boldsymbol{W}_{xh}\)与\(\boldsymbol{W}_{hh}\)的连结，偏差为\(\boldsymbol{b}_h\)。当前时间步\(t\)的隐藏状态\(\boldsymbol{H}_t\)将参与下一个时间步\(t+1\)的隐藏状态\(\boldsymbol{H}_{t+1}\)的计算，并输入到当前时间步的全连接输出层。</p>

<p><img src="media/15820319987185/rnn.svg" alt="含隐藏状态的循环神经网络"/></p>

<p>我们刚刚提到，隐藏状态中\(\boldsymbol{X}_t \boldsymbol{W}_{xh} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{hh}\)的计算等价于\(\boldsymbol{X}_t\)与\(\boldsymbol{H}_{t-1}\)连结后的矩阵乘以\(\boldsymbol{W}_{xh}\)与\(\boldsymbol{W}_{hh}\)连结后的矩阵。接下来，我们用一个具体的例子来验证这一点。首先，我们构造矩阵<code>X</code>、<code>W_xh</code>、<code>H</code>和<code>W_hh</code>，它们的形状分别为(3, 1)、(1, 4)、(3, 4)和(4, 4)。将<code>X</code>与<code>W_xh</code>、<code>H</code>与<code>W_hh</code>分别相乘，再把两个乘法运算的结果相加，得到形状为(3, 4)的矩阵。</p>

<pre><code class="language-python">from mxnet import nd

X, W_xh = nd.random.normal(shape=(3, 1)), nd.random.normal(shape=(1, 4))
H, W_hh = nd.random.normal(shape=(3, 4)), nd.random.normal(shape=(4, 4))
nd.dot(X, W_xh) + nd.dot(H, W_hh)
</code></pre>

<pre><code class="language-text">[[ 5.0373516   2.6754622  -1.6607479  -0.40628886]
 [ 0.948454    0.46941757 -1.1866101  -1.180677  ]
 [-1.1514019   0.8373027  -2.197437   -5.2480164 ]]
&lt;NDArray 3x4 @cpu(0)&gt;
</code></pre>

<p>将矩阵<code>X</code>和<code>H</code>按列（维度1）连结，连结后的矩阵形状为(3, 5)。可见，连结后矩阵在维度1的长度为矩阵<code>X</code>和<code>H</code>在维度1的长度之和（\(1+4\)）。然后，将矩阵<code>W_xh</code>和<code>W_hh</code>按行（维度0）连结，连结后的矩阵形状为(5, 4)。最后将两个连结后的矩阵相乘，得到与上面代码输出相同的形状为(3, 4)的矩阵。</p>

<pre><code class="language-python">nd.dot(nd.concat(X, H, dim=1), nd.concat(W_xh, W_hh, dim=0))
</code></pre>

<pre><code class="language-text">[[ 5.0373516   2.6754622  -1.6607479  -0.40628862]
 [ 0.94845396  0.46941754 -1.1866102  -1.1806769 ]
 [-1.1514019   0.83730274 -2.1974368  -5.2480164 ]]
&lt;NDArray 3x4 @cpu(0)&gt;
</code></pre>

<h3 id="toc_33">应用：基于字符级循环神经网络的语言模型</h3>

<p>最后我们介绍如何应用循环神经网络来构建一个语言模型。设小批量中样本数为1，文本序列为“想”“要”“有”“直”“升”“机”。图6.2演示了如何使用循环神经网络基于当前和过去的字符来预测下一个字符。在训练时，我们对每个时间步的输出层输出使用softmax运算，然后使用交叉熵损失函数来计算它与标签的误差。在图6.2中，由于隐藏层中隐藏状态的循环计算，时间步3的输出\(\boldsymbol{O}_3\)取决于文本序列“想”“要”“有”。 由于训练数据中该序列的下一个词为“直”，时间步3的损失将取决于该时间步基于序列“想”“要”“有”生成下一个词的概率分布与该时间步的标签“直”。</p>

<p><img src="media/15820319987185/rnn-train.svg" alt="基于字符级循环神经网络的语言模型。输入序列和标签序列分别为“想”“要”“有”“直”“升”和“要”“有”“直”“升”“机”"/></p>

<p>因为每个输入词是一个字符，因此这个模型被称为字符级循环神经网络（character-level recurrent neural network）。因为不同字符的个数远小于不同词的个数（对于英文尤其如此），所以字符级循环神经网络的计算通常更加简单。在接下来的几节里，我们将介绍它的具体实现。</p>

<h3 id="toc_34">小结</h3>

<ul>
<li>使用循环计算的网络即循环神经网络。</li>
<li>循环神经网络的隐藏状态可以捕捉截至当前时间步的序列的历史信息。</li>
<li>循环神经网络模型参数的数量不随时间步的增加而增长。</li>
<li>可以基于字符级循环神经网络来创建语言模型。</li>
</ul>

<h3 id="toc_35">练习</h3>

<ul>
<li>如果使用循环神经网络来预测一段文本序列的下一个词，输出个数应该设为多少？</li>
<li>为什么循环神经网络可以表达某时间步的词基于文本序列中所有过去的词的条件概率？</li>
</ul>

<h2 id="toc_36">循环神经网络的从零开始实现</h2>

<p>在本节中，我们将从零开始实现一个基于字符级循环神经网络的语言模型，并在周杰伦专辑歌词数据集上训练一个模型来进行歌词创作。首先，我们读取周杰伦专辑歌词数据集：</p>

<pre><code class="language-python">import d2lzh as d2l
import math
from mxnet import autograd, nd
from mxnet.gluon import loss as gloss
import time

(corpus_indices, char_to_idx, idx_to_char,
 vocab_size) = d2l.load_data_jay_lyrics()
</code></pre>

<h3 id="toc_37">one-hot向量</h3>

<p>为了将词表示成向量输入到神经网络，一个简单的办法是使用one-hot向量。假设词典中不同字符的数量为\(N\)（即词典大小<code>vocab_size</code>），每个字符已经同一个从0到\(N-1\)的连续整数值索引一一对应。如果一个字符的索引是整数\(i\), 那么我们创建一个全0的长为\(N\)的向量，并将其位置为\(i\)的元素设成1。该向量就是对原字符的one-hot向量。下面分别展示了索引为0和2的one-hot向量，向量长度等于词典大小。</p>

<pre><code class="language-python">nd.one_hot(nd.array([0, 2]), vocab_size)
</code></pre>

<pre><code class="language-text">[[1. 0. 0. ... 0. 0. 0.]
 [0. 0. 1. ... 0. 0. 0.]]
&lt;NDArray 2x1027 @cpu(0)&gt;
</code></pre>

<p>我们每次采样的小批量的形状是(批量大小, 时间步数)。下面的函数将这样的小批量变换成数个可以输入进网络的形状为(批量大小, 词典大小)的矩阵，矩阵个数等于时间步数。也就是说，时间步\(t\)的输入为\(\boldsymbol{X}_t \in \mathbb{R}^{n \times d}\)，其中\(n\)为批量大小，\(d\)为输入个数，即one-hot向量长度（词典大小）。</p>

<pre><code class="language-python">def to_onehot(X, size):  # 本函数已保存在d2lzh包中方便以后使用
    return [nd.one_hot(x, size) for x in X.T]

X = nd.arange(10).reshape((2, 5))
inputs = to_onehot(X, vocab_size)
len(inputs), inputs[0].shape
</code></pre>

<pre><code class="language-text">(5, (2, 1027))
</code></pre>

<h3 id="toc_38">初始化模型参数</h3>

<p>接下来，我们初始化模型参数。隐藏单元个数 <code>num_hiddens</code>是一个超参数。</p>

<pre><code class="language-python">num_inputs, num_hiddens, num_outputs = vocab_size, 256, vocab_size
ctx = d2l.try_gpu()
print(&#39;will use&#39;, ctx)

def get_params():
    def _one(shape):
        return nd.random.normal(scale=0.01, shape=shape, ctx=ctx)

    # 隐藏层参数
    W_xh = _one((num_inputs, num_hiddens))
    W_hh = _one((num_hiddens, num_hiddens))
    b_h = nd.zeros(num_hiddens, ctx=ctx)
    # 输出层参数
    W_hq = _one((num_hiddens, num_outputs))
    b_q = nd.zeros(num_outputs, ctx=ctx)
    # 附上梯度
    params = [W_xh, W_hh, b_h, W_hq, b_q]
    for param in params:
        param.attach_grad()
    return params
</code></pre>

<pre><code class="language-text">will use gpu(0)
</code></pre>

<h3 id="toc_39">定义模型</h3>

<p>我们根据循环神经网络的计算表达式实现该模型。首先定义<code>init_rnn_state</code>函数来返回初始化的隐藏状态。它返回由一个形状为(批量大小, 隐藏单元个数)的值为0的<code>NDArray</code>组成的元组。使用元组是为了更便于处理隐藏状态含有多个<code>NDArray</code>的情况。</p>

<pre><code class="language-python">def init_rnn_state(batch_size, num_hiddens, ctx):
    return (nd.zeros(shape=(batch_size, num_hiddens), ctx=ctx), )
</code></pre>

<p>下面的<code>rnn</code>函数定义了在一个时间步里如何计算隐藏状态和输出。这里的激活函数使用了tanh函数。<a href="../chapter_deep-learning-basics/mlp.ipynb">“多层感知机”</a>一节中介绍过，当元素在实数域上均匀分布时，tanh函数值的均值为0。</p>

<pre><code class="language-python">def rnn(inputs, state, params):
    # inputs和outputs皆为num_steps个形状为(batch_size, vocab_size)的矩阵
    W_xh, W_hh, b_h, W_hq, b_q = params
    H, = state
    outputs = []
    for X in inputs:
        H = nd.tanh(nd.dot(X, W_xh) + nd.dot(H, W_hh) + b_h)
        Y = nd.dot(H, W_hq) + b_q
        outputs.append(Y)
    return outputs, (H,)
</code></pre>

<p>做个简单的测试来观察输出结果的个数（时间步数），以及第一个时间步的输出层输出的形状和隐藏状态的形状。</p>

<pre><code class="language-python">state = init_rnn_state(X.shape[0], num_hiddens, ctx)
inputs = to_onehot(X.as_in_context(ctx), vocab_size)
params = get_params()
outputs, state_new = rnn(inputs, state, params)
len(outputs), outputs[0].shape, state_new[0].shape
</code></pre>

<pre><code class="language-text">(5, (2, 1027), (2, 256))
</code></pre>

<h3 id="toc_40">定义预测函数</h3>

<p>以下函数基于前缀<code>prefix</code>（含有数个字符的字符串）来预测接下来的<code>num_chars</code>个字符。这个函数稍显复杂，其中我们将循环神经单元<code>rnn</code>设置成了函数参数，这样在后面小节介绍其他循环神经网络时能重复使用这个函数。</p>

<pre><code class="language-python"># 本函数已保存在d2lzh包中方便以后使用
def predict_rnn(prefix, num_chars, rnn, params, init_rnn_state,
                num_hiddens, vocab_size, ctx, idx_to_char, char_to_idx):
    state = init_rnn_state(1, num_hiddens, ctx)
    output = [char_to_idx[prefix[0]]]
    for t in range(num_chars + len(prefix) - 1):
        # 将上一时间步的输出作为当前时间步的输入
        X = to_onehot(nd.array([output[-1]], ctx=ctx), vocab_size)
        # 计算输出和更新隐藏状态
        (Y, state) = rnn(X, state, params)
        # 下一个时间步的输入是prefix里的字符或者当前的最佳预测字符
        if t &lt; len(prefix) - 1:
            output.append(char_to_idx[prefix[t + 1]])
        else:
            output.append(int(Y[0].argmax(axis=1).asscalar()))
    return &#39;&#39;.join([idx_to_char[i] for i in output])
</code></pre>

<p>我们先测试一下<code>predict_rnn</code>函数。我们将根据前缀“分开”创作长度为10个字符（不考虑前缀长度）的一段歌词。因为模型参数为随机值，所以预测结果也是随机的。</p>

<pre><code class="language-python">predict_rnn(&#39;分开&#39;, 10, rnn, params, init_rnn_state, num_hiddens, vocab_size,
            ctx, idx_to_char, char_to_idx)
</code></pre>

<pre><code class="language-text">&#39;分开鸣捏性目泣游试找抬夕&#39;
</code></pre>

<h3 id="toc_41">裁剪梯度</h3>

<p>循环神经网络中较容易出现梯度衰减或梯度爆炸。我们会在<a href="bptt.ipynb">“通过时间反向传播”</a>一节中解释原因。为了应对梯度爆炸，我们可以裁剪梯度（clip gradient）。假设我们把所有模型参数梯度的元素拼接成一个向量 \(\boldsymbol{g}\)，并设裁剪的阈值是\(\theta\)。裁剪后的梯度</p>

<p>\[ \min\left(\frac{\theta}{\|\boldsymbol{g}\|}, 1\right)\boldsymbol{g}\]</p>

<p>的\(L_2\)范数不超过\(\theta\)。</p>

<pre><code class="language-python"># 本函数已保存在d2lzh包中方便以后使用
def grad_clipping(params, theta, ctx):
    norm = nd.array([0], ctx)
    for param in params:
        norm += (param.grad ** 2).sum()
    norm = norm.sqrt().asscalar()
    if norm &gt; theta:
        for param in params:
            param.grad[:] *= theta / norm
</code></pre>

<h3 id="toc_42">困惑度</h3>

<p>我们通常使用困惑度（perplexity）来评价语言模型的好坏。回忆一下<a href="../chapter_deep-learning-basics/softmax-regression.ipynb">“softmax回归”</a>一节中交叉熵损失函数的定义。困惑度是对交叉熵损失函数做指数运算后得到的值。特别地，</p>

<ul>
<li>最佳情况下，模型总是把标签类别的概率预测为1，此时困惑度为1；</li>
<li>最坏情况下，模型总是把标签类别的概率预测为0，此时困惑度为正无穷；</li>
<li>基线情况下，模型总是预测所有类别的概率都相同，此时困惑度为类别个数。</li>
</ul>

<p>显然，任何一个有效模型的困惑度必须小于类别个数。在本例中，困惑度必须小于词典大小<code>vocab_size</code>。</p>

<h3 id="toc_43">定义模型训练函数</h3>

<p>跟之前章节的模型训练函数相比，这里的模型训练函数有以下几点不同：</p>

<ol>
<li>使用困惑度评价模型。</li>
<li>在迭代模型参数前裁剪梯度。</li>
<li>对时序数据采用不同采样方法将导致隐藏状态初始化的不同。相关讨论可参考<a href="lang-model-dataset.ipynb">“语言模型数据集（周杰伦专辑歌词）”</a>一节。</li>
</ol>

<p>另外，考虑到后面将介绍的其他循环神经网络，为了更通用，这里的函数实现更长一些。</p>

<pre><code class="language-python"># 本函数已保存在d2lzh包中方便以后使用
def train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,
                          vocab_size, ctx, corpus_indices, idx_to_char,
                          char_to_idx, is_random_iter, num_epochs, num_steps,
                          lr, clipping_theta, batch_size, pred_period,
                          pred_len, prefixes):
    if is_random_iter:
        data_iter_fn = d2l.data_iter_random
    else:
        data_iter_fn = d2l.data_iter_consecutive
    params = get_params()
    loss = gloss.SoftmaxCrossEntropyLoss()

    for epoch in range(num_epochs):
        if not is_random_iter:  # 如使用相邻采样，在epoch开始时初始化隐藏状态
            state = init_rnn_state(batch_size, num_hiddens, ctx)
        l_sum, n, start = 0.0, 0, time.time()
        data_iter = data_iter_fn(corpus_indices, batch_size, num_steps, ctx)
        for X, Y in data_iter:
            if is_random_iter:  # 如使用随机采样，在每个小批量更新前初始化隐藏状态
                state = init_rnn_state(batch_size, num_hiddens, ctx)
            else:  # 否则需要使用detach函数从计算图分离隐藏状态
                for s in state:
                    s.detach()
            with autograd.record():
                inputs = to_onehot(X, vocab_size)
                # outputs有num_steps个形状为(batch_size, vocab_size)的矩阵
                (outputs, state) = rnn(inputs, state, params)
                # 拼接之后形状为(num_steps * batch_size, vocab_size)
                outputs = nd.concat(*outputs, dim=0)
                # Y的形状是(batch_size, num_steps)，转置后再变成长度为
                # batch * num_steps 的向量，这样跟输出的行一一对应
                y = Y.T.reshape((-1,))
                # 使用交叉熵损失计算平均分类误差
                l = loss(outputs, y).mean()
            l.backward()
            grad_clipping(params, clipping_theta, ctx)  # 裁剪梯度
            d2l.sgd(params, lr, 1)  # 因为误差已经取过均值，梯度不用再做平均
            l_sum += l.asscalar() * y.size
            n += y.size

        if (epoch + 1) % pred_period == 0:
            print(&#39;epoch %d, perplexity %f, time %.2f sec&#39; % (
                epoch + 1, math.exp(l_sum / n), time.time() - start))
            for prefix in prefixes:
                print(&#39; -&#39;, predict_rnn(
                    prefix, pred_len, rnn, params, init_rnn_state,
                    num_hiddens, vocab_size, ctx, idx_to_char, char_to_idx))
</code></pre>

<h3 id="toc_44">训练模型并创作歌词</h3>

<p>现在我们可以训练模型了。首先，设置模型超参数。我们将根据前缀“分开”和“不分开”分别创作长度为50个字符（不考虑前缀长度）的一段歌词。我们每过50个迭代周期便根据当前训练的模型创作一段歌词。</p>

<pre><code class="language-python">num_epochs, num_steps, batch_size, lr, clipping_theta = 250, 35, 32, 1e2, 1e-2
pred_period, pred_len, prefixes = 50, 50, [&#39;分开&#39;, &#39;不分开&#39;]
</code></pre>

<p>下面采用随机采样训练模型并创作歌词。</p>

<pre><code class="language-python">train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,
                      vocab_size, ctx, corpus_indices, idx_to_char,
                      char_to_idx, True, num_epochs, num_steps, lr,
                      clipping_theta, batch_size, pred_period, pred_len,
                      prefixes)
</code></pre>

<pre><code class="language-text">epoch 50, perplexity 69.080416, time 0.22 sec
 - 分开 我不要再的你  哼哈兮 快果我 别子我 我不要 干什么 我不要 我爱我的可爱女人 坏坏的让我疯狂的
 - 不分开 我不要再的你  哼哈兮 快果我 别子我 我不要 干什么 我不要 我爱我的可爱女人 坏坏的让我疯狂的
epoch 100, perplexity 10.197520, time 0.22 sec
 - 分开  我爱到这 我想要好我 不场 娘子 征不 弓箭的老 在小村外  让有什么 有一场热演 你色蜡烛 是
 - 不分开柳 我爱你这你 我不 我不 我不要再想 我不 我不 我不要再想 我不 我不 我不要再想 我不 我不 
epoch 150, perplexity 2.933384, time 0.22 sec
 - 分开 有直了不多 有话就满说 别人在怕羞 有蝪横著走 这里什么奇怪的事都有 包括像猫的狗 印地安老斑鸠 
 - 不分开吗 我叫你爸 你打我妈 这样种容 染不了痛 我不要再我 我不能这想 我不 我不 我不要再想你 不知不
epoch 200, perplexity 1.633695, time 0.22 sec
 - 分开 一直心上心仪的母斑鸠 包仔安蕃 在小镇 背对背决斗 一只灰狼 问候完空屋 白色蜡烛 温暖了空屋 白
 - 不分开简 我叫你爸 你打我妈 这样对吗后嘛这样 何么让酒牵点子B 瞎 说着心口斯 双截棍红的泥老 我都耍的
epoch 250, perplexity 1.334753, time 0.22 sec
 - 分开 一直走钩拳仪的母斑鸠 牛仔红蕃 在小镇 背对背决斗 一只灰狼 问候完空屋 白色蜡烛 温暖了空屋 白
 - 不分开期 然后将过去 慢慢温习 让我爱上你 那场悲剧 是你完美演出的一场戏 宁愿心碎哭泣 再狠狠忘记 你爱
</code></pre>

<p>接下来采用相邻采样训练模型并创作歌词。</p>

<pre><code class="language-python">train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,
                      vocab_size, ctx, corpus_indices, idx_to_char,
                      char_to_idx, False, num_epochs, num_steps, lr,
                      clipping_theta, batch_size, pred_period, pred_len,
                      prefixes)
</code></pre>

<pre><code class="language-text">epoch 50, perplexity 60.953674, time 0.22 sec
 - 分开 我想要这 我想了这 我有了空 我有了空 我有了空 我有了空 我有了空 我有了空 我有了空 我有了空
 - 不分开 我想要这 我想了这 我有了空 我有了空 我有了空 我有了空 我有了空 我有了空 我有了空 我有了空
epoch 100, perplexity 7.151810, time 0.22 sec
 - 分开 我想要这样 我不要再想 我不能再想 我不 我不 我不要再想你 爱情我的见快就像龙卷风 不能开暴我的
 - 不分开只 你去那这不我 不知 你想很久了吧? 我说你的爱写在默 也想我不多你想一场悲剧 我想我这辈的注样 
epoch 150, perplexity 2.091185, time 0.22 sec
 - 分开 娘子我 印怪的枪我 泪 却九走听单默一步还 说录星到风的语瓣 古我一口吴侬软 的话是是一直了老 就
 - 不分开觉 你已经离开我 不知不觉 我跟了这节奏 后知后觉 又过了一个秋 后知后觉 我该好好生活 我该好好生
epoch 200, perplexity 1.281988, time 0.22 sec
 - 分开 我满到 爱怎么 三过了里 这人忆 的诉段 时一些风霜 老唱盘 旧皮箱 装满了明信片的铁盒里藏著一片
 - 不分开觉 你已经离开我 不知不觉 我跟了这节奏 后知后觉 又过了一个秋 后知后觉 我该好好生活 我该好好生
epoch 250, perplexity 1.183621, time 0.22 sec
 - 分开 问候到 它在空 装满了明信片的铁盒里藏著一片玫瑰花瓣 黄金葛爬满了雕花的门窗 夕阳斜斜映在斑驳的砖
 - 不分开觉 你已经离开我 不知不觉 我跟了这节奏 后知后觉 又过了一个秋 后知后觉 我该好好生活 我该好好生
</code></pre>

<h3 id="toc_45">小结</h3>

<ul>
<li>可以用基于字符级循环神经网络的语言模型来生成文本序列，例如创作歌词。</li>
<li>当训练循环神经网络时，为了应对梯度爆炸，可以裁剪梯度。</li>
<li>困惑度是对交叉熵损失函数做指数运算后得到的值。</li>
</ul>

<h3 id="toc_46">练习</h3>

<ul>
<li>调调超参数，观察并分析对运行时间、困惑度以及创作歌词的结果造成的影响。</li>
<li>不裁剪梯度，运行本节中的代码，结果会怎样？</li>
<li>将<code>pred_period</code>变量设为1，观察未充分训练的模型（困惑度高）是如何创作歌词的。你获得了什么启发？</li>
<li>将相邻采样改为不从计算图分离隐藏状态，运行时间有没有变化？</li>
<li>将本节中使用的激活函数替换成ReLU，重复本节的实验。</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[《动手学深度学习》PyTorch版学习记录（一）]]></title>
    <link href="https://zning.me/15816738049513.html"/>
    <updated>2020-02-14T17:50:04+08:00</updated>
    <id>https://zning.me/15816738049513.html</id>
    <content type="html"><![CDATA[
<p>近期参加了伯禹教育、Datawhale、和鲸科技组织的“《动手学深度学习》代码讲解PyTorch版”课程学习，本文为第一次学习记录。涉及知识点：线性回归、softmax回归和过拟合与欠拟合。</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">线性回归</a>
<ul>
<li>
<a href="#toc_1">线性回归的基本要素</a>
<ul>
<li>
<a href="#toc_2">模型</a>
</li>
<li>
<a href="#toc_3">模型训练</a>
<ul>
<li>
<a href="#toc_4">训练数据</a>
</li>
<li>
<a href="#toc_5">损失函数</a>
</li>
<li>
<a href="#toc_6">优化算法</a>
</li>
</ul>
</li>
<li>
<a href="#toc_7">模型预测</a>
</li>
</ul>
</li>
<li>
<a href="#toc_8">线性回归的表示方法</a>
<ul>
<li>
<a href="#toc_9">神经网络图</a>
</li>
<li>
<a href="#toc_10">矢量计算表达式</a>
</li>
</ul>
</li>
<li>
<a href="#toc_11">小结</a>
</li>
<li>
<a href="#toc_12">练习</a>
</li>
</ul>
</li>
<li>
<a href="#toc_13">线性回归的从零开始实现</a>
<ul>
<li>
<a href="#toc_14">生成数据集</a>
</li>
<li>
<a href="#toc_15">读取数据</a>
</li>
<li>
<a href="#toc_16">初始化模型参数</a>
</li>
<li>
<a href="#toc_17">定义模型</a>
</li>
<li>
<a href="#toc_18">定义损失函数</a>
</li>
<li>
<a href="#toc_19">定义优化算法</a>
</li>
<li>
<a href="#toc_20">训练模型</a>
</li>
<li>
<a href="#toc_21">小结</a>
</li>
<li>
<a href="#toc_22">练习</a>
</li>
</ul>
</li>
<li>
<a href="#toc_23">softmax回归</a>
<ul>
<li>
<a href="#toc_24">分类问题</a>
<ul>
<li>
<a href="#toc_25">softmax回归模型</a>
</li>
<li>
<a href="#toc_26">softmax运算</a>
</li>
</ul>
</li>
<li>
<a href="#toc_27">单样本分类的矢量计算表达式</a>
</li>
<li>
<a href="#toc_28">小批量样本分类的矢量计算表达式</a>
</li>
<li>
<a href="#toc_29">交叉熵损失函数</a>
</li>
<li>
<a href="#toc_30">模型预测及评价</a>
</li>
<li>
<a href="#toc_31">小结</a>
</li>
<li>
<a href="#toc_32">练习</a>
</li>
</ul>
</li>
<li>
<a href="#toc_33">softmax回归的从零开始实现</a>
<ul>
<li>
<a href="#toc_34">获取和读取数据</a>
</li>
<li>
<a href="#toc_35">初始化模型参数</a>
</li>
<li>
<a href="#toc_36">实现softmax运算</a>
</li>
<li>
<a href="#toc_37">定义模型</a>
</li>
<li>
<a href="#toc_38">定义损失函数</a>
</li>
<li>
<a href="#toc_39">计算分类准确率</a>
</li>
<li>
<a href="#toc_40">训练模型</a>
</li>
<li>
<a href="#toc_41">预测</a>
</li>
<li>
<a href="#toc_42">小结</a>
</li>
<li>
<a href="#toc_43">练习</a>
</li>
</ul>
</li>
<li>
<a href="#toc_44">模型选择、欠拟合和过拟合</a>
<ul>
<li>
<a href="#toc_45">训练误差和泛化误差</a>
</li>
<li>
<a href="#toc_46">模型选择</a>
<ul>
<li>
<a href="#toc_47">验证数据集</a>
</li>
<li>
<a href="#toc_48">$K$ 折交叉验证</a>
</li>
</ul>
</li>
<li>
<a href="#toc_49">欠拟合和过拟合</a>
<ul>
<li>
<a href="#toc_50">模型复杂度</a>
</li>
<li>
<a href="#toc_51">训练数据集大小</a>
</li>
</ul>
</li>
<li>
<a href="#toc_52">多项式函数拟合实验</a>
<ul>
<li>
<a href="#toc_53">生成数据集</a>
</li>
<li>
<a href="#toc_54">定义、训练和测试模型</a>
</li>
<li>
<a href="#toc_55">三阶多项式函数拟合（正常）</a>
</li>
<li>
<a href="#toc_56">线性函数拟合（欠拟合）</a>
</li>
<li>
<a href="#toc_57">训练样本不足（过拟合）</a>
</li>
</ul>
</li>
<li>
<a href="#toc_58">小结</a>
</li>
<li>
<a href="#toc_59">练习</a>
</li>
</ul>
</li>
</ul>


<h2 id="toc_0">线性回归</h2>

<p>线性回归输出是一个连续值，因此适用于回归问题。回归问题在实际中很常见，如预测房屋价格、气温、销售额等连续值的问题。与回归问题不同，分类问题中模型的最终输出是一个离散值。我们所说的图像分类、垃圾邮件识别、疾病检测等输出为离散值的问题都属于分类问题的范畴。softmax回归则适用于分类问题。</p>

<p>由于线性回归和softmax回归都是单层神经网络，它们涉及的概念和技术同样适用于大多数的深度学习模型。我们首先以线性回归为例，介绍大多数深度学习模型的基本要素和表示方法。</p>

<h3 id="toc_1">线性回归的基本要素</h3>

<p>我们以一个简单的房屋价格预测作为例子来解释线性回归的基本要素。这个应用的目标是预测一栋房子的售出价格（元）。我们知道这个价格取决于很多因素，如房屋状况、地段、市场行情等。为了简单起见，这里我们假设价格只取决于房屋状况的两个因素，即面积（平方米）和房龄（年）。接下来我们希望探索价格与这两个因素的具体关系。</p>

<h4 id="toc_2">模型</h4>

<p>设房屋的面积为\(x_1\)，房龄为\(x_2\)，售出价格为\(y\)。我们需要建立基于输入\(x_1\)和\(x_2\)来计算输出\(y\)的表达式，也就是模型（model）。顾名思义，线性回归假设输出与各个输入之间是线性关系：</p>

<p>\[\hat{y} = x_1 w_1 + x_2 w_2 + b,\]</p>

<p>其中\(w_1\)和\(w_2\)是权重（weight），\(b\)是偏差（bias），且均为标量。它们是线性回归模型的参数（parameter）。模型输出\(\hat{y}\)是线性回归对真实价格\(y\)的预测或估计。我们通常允许它们之间有一定误差。</p>

<h4 id="toc_3">模型训练</h4>

<p>接下来我们需要通过数据来寻找特定的模型参数值，使模型在数据上的误差尽可能小。这个过程叫作模型训练（model training）。下面我们介绍模型训练所涉及的3个要素。</p>

<h5 id="toc_4">训练数据</h5>

<p>我们通常收集一系列的真实数据，例如多栋房屋的真实售出价格和它们对应的面积和房龄。我们希望在这个数据上面寻找模型参数来使模型的预测价格与真实价格的误差最小。在机器学习术语里，该数据集被称为训练数据集（training data set）或训练集（training set），一栋房屋被称为一个样本（sample），其真实售出价格叫作标签（label），用来预测标签的两个因素叫作特征（feature）。特征用来表征样本的特点。</p>

<p>假设我们采集的样本数为\(n\)，索引为\(i\)的样本的特征为\(x_1^{(i)}\)和\(x_2^{(i)}\)，标签为\(y^{(i)}\)。对于索引为\(i\)的房屋，线性回归模型的房屋价格预测表达式为</p>

<p>\[\hat{y}^{(i)} = x_1^{(i)} w_1 + x_2^{(i)} w_2 + b.\]</p>

<h5 id="toc_5">损失函数</h5>

<p>在模型训练中，我们需要衡量价格预测值与真实值之间的误差。通常我们会选取一个非负数作为误差，且数值越小表示误差越小。一个常用的选择是平方函数。它在评估索引为\(i\)的样本误差的表达式为</p>

<p>\[\ell^{(i)}(w_1, w_2, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2,\]</p>

<p>其中常数\(1/2\)使对平方项求导后的常数系数为1，这样在形式上稍微简单一些。显然，误差越小表示预测价格与真实价格越相近，且当二者相等时误差为0。给定训练数据集，这个误差只与模型参数相关，因此我们将它记为以模型参数为参数的函数。在机器学习里，将衡量误差的函数称为损失函数（loss function）。这里使用的平方误差函数也称为平方损失（square loss）。</p>

<p>通常，我们用训练数据集中所有样本误差的平均来衡量模型预测的质量，即</p>

<p>\[\ell(w_1, w_2, b) =\frac{1}{n} \sum_{i=1}^n \ell^{(i)}(w_1, w_2, b) =\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right)^2.\]</p>

<p>在模型训练中，我们希望找出一组模型参数，记为\(w_1^*, w_2^*, b^*\)，来使训练样本平均损失最小：</p>

<p>\[w_1^*, w_2^*, b^* = \operatorname*{argmin}_{w_1, w_2, b}\  \ell(w_1, w_2, b).\]</p>

<h5 id="toc_6">优化算法</h5>

<p>当模型和损失函数形式较为简单时，上面的误差最小化问题的解可以直接用公式表达出来。这类解叫作解析解（analytical solution）。本节使用的线性回归和平方误差刚好属于这个范畴。然而，大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。这类解叫作数值解（numerical solution）。</p>

<p>在求数值解的优化算法中，小批量随机梯度下降（mini-batch stochastic gradient descent）在深度学习中被广泛使用。它的算法很简单：先选取一组模型参数的初始值，如随机选取；接下来对参数进行多次迭代，使每次迭代都可能降低损失函数的值。在每次迭代中，先随机均匀采样一个由固定数目训练数据样本所组成的小批量（mini-batch）\(\mathcal{B}\)，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后用此结果与预先设定的一个正数的乘积作为模型参数在本次迭代的减小量。</p>

<p>在训练本节讨论的线性回归模型的过程中，模型的每个参数将作如下迭代：</p>

<p>\[<br/>
\begin{aligned}<br/>
w_1 &amp;\leftarrow w_1 -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \frac{ \partial \ell^{(i)}(w_1, w_2, b)  }{\partial w_1} = w_1 -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_1^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right),\\<br/>
w_2 &amp;\leftarrow w_2 -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \frac{ \partial \ell^{(i)}(w_1, w_2, b)  }{\partial w_2} = w_2 -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_2^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right),\\<br/>
b &amp;\leftarrow b -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \frac{ \partial \ell^{(i)}(w_1, w_2, b)  }{\partial b} = b -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right).<br/>
\end{aligned}<br/>
\]</p>

<p>在上式中，\(|\mathcal{B}|\)代表每个小批量中的样本个数（批量大小，batch size），\(\eta\)称作学习率（learning rate）并取正数。需要强调的是，这里的批量大小和学习率的值是人为设定的，并不是通过模型训练学出的，因此叫作超参数（hyperparameter）。我们通常所说的“调参”指的正是调节超参数，例如通过反复试错来找到超参数合适的值。在少数情况下，超参数也可以通过模型训练学出。本书对此类情况不做讨论。</p>

<h4 id="toc_7">模型预测</h4>

<p>模型训练完成后，我们将模型参数\(w_1, w_2, b\)在优化算法停止时的值分别记作\(\hat{w}_1, \hat{w}_2, \hat{b}\)。注意，这里我们得到的并不一定是最小化损失函数的最优解\(w_1^*, w_2^*, b^*\)，而是对最优解的一个近似。然后，我们就可以使用学出的线性回归模型\(x_1 \hat{w}_1 + x_2 \hat{w}_2 + \hat{b}\)来估算训练数据集以外任意一栋面积（平方米）为\(x_1\)、房龄（年）为\(x_2\)的房屋的价格了。这里的估算也叫作模型预测、模型推断或模型测试。</p>

<h3 id="toc_8">线性回归的表示方法</h3>

<p>我们已经阐述了线性回归的模型表达式、训练和预测。下面我们解释线性回归与神经网络的联系，以及线性回归的矢量计算表达式。</p>

<h4 id="toc_9">神经网络图</h4>

<p>在深度学习中，我们可以使用神经网络图直观地表现模型结构。为了更清晰地展示线性回归作为神经网络的结构，图3.1使用神经网络图表示本节中介绍的线性回归模型。神经网络图隐去了模型参数权重和偏差。</p>

<p><img src="media/15816738049513/linreg.svg" alt="线性回归是一个单层神经网络"/></p>

<p>在图3.1所示的神经网络中，输入分别为\(x_1\)和\(x_2\)，因此输入层的输入个数为2。输入个数也叫特征数或特征向量维度。图3.1中网络的输出为\(o\)，输出层的输出个数为1。需要注意的是，我们直接将图3.1中神经网络的输出\(o\)作为线性回归的输出，即\(\hat{y} = o\)。由于输入层并不涉及计算，按照惯例，图3.1所示的神经网络的层数为1。所以，线性回归是一个单层神经网络。输出层中负责计算\(o\)的单元又叫神经元。在线性回归中，\(o\)的计算依赖于\(x_1\)和\(x_2\)。也就是说，输出层中的神经元和输入层中各个输入完全连接。因此，这里的输出层又叫全连接层（fully-connected layer）或稠密层（dense layer）。</p>

<h4 id="toc_10">矢量计算表达式</h4>

<p>在模型训练或预测时，我们常常会同时处理多个数据样本并用到矢量计算。在介绍线性回归的矢量计算表达式之前，让我们先考虑对两个向量相加的两种方法。</p>

<p>下面先定义两个1000维的向量。</p>

<pre><code class="language-python">from mxnet import nd
from time import time

a = nd.ones(shape=1000)
b = nd.ones(shape=1000)
</code></pre>

<p>向量相加的一种方法是，将这两个向量按元素逐一做标量加法。</p>

<pre><code class="language-python">start = time()
c = nd.zeros(shape=1000)
for i in range(1000):
    c[i] = a[i] + b[i]
time() - start
</code></pre>

<pre><code class="language-text">0.147660493850708
</code></pre>

<p>向量相加的另一种方法是，将这两个向量直接做矢量加法。</p>

<pre><code class="language-python">start = time()
d = a + b
time() - start
</code></pre>

<pre><code class="language-text">0.0002694129943847656
</code></pre>

<p>结果很明显，后者比前者更省时。因此，我们应该尽可能采用矢量计算，以提升计算效率。</p>

<p>让我们再次回到本节的房价预测问题。如果我们对训练数据集里的3个房屋样本（索引分别为1、2和3）逐一预测价格，将得到</p>

<p>\[<br/>
\begin{aligned}<br/>
\hat{y}^{(1)} &amp;= x_1^{(1)} w_1 + x_2^{(1)} w_2 + b,\\<br/>
\hat{y}^{(2)} &amp;= x_1^{(2)} w_1 + x_2^{(2)} w_2 + b,\\<br/>
\hat{y}^{(3)} &amp;= x_1^{(3)} w_1 + x_2^{(3)} w_2 + b.<br/>
\end{aligned}<br/>
\]</p>

<p>现在，我们将上面3个等式转化成矢量计算。设</p>

<p>\[<br/>
\boldsymbol{\hat{y}} =<br/>
\begin{bmatrix}<br/>
    \hat{y}^{(1)} \\<br/>
    \hat{y}^{(2)} \\<br/>
    \hat{y}^{(3)}<br/>
\end{bmatrix},\quad<br/>
\boldsymbol{X} =<br/>
\begin{bmatrix}<br/>
    x_1^{(1)} &amp; x_2^{(1)} \\<br/>
    x_1^{(2)} &amp; x_2^{(2)} \\<br/>
    x_1^{(3)} &amp; x_2^{(3)}<br/>
\end{bmatrix},\quad<br/>
\boldsymbol{w} =<br/>
\begin{bmatrix}<br/>
    w_1 \\<br/>
    w_2<br/>
\end{bmatrix}.<br/>
\]</p>

<p>对3个房屋样本预测价格的矢量计算表达式为\(\boldsymbol{\hat{y}} = \boldsymbol{X} \boldsymbol{w} + b,\) 其中的加法运算使用了广播机制（参见<a href="../chapter_prerequisite/ndarray.ipynb">“数据操作”</a>一节）。例如：</p>

<pre><code class="language-python">a = nd.ones(shape=3)
b = 10
a + b
</code></pre>

<pre><code class="language-text">[11. 11. 11.]
&lt;NDArray 3 @cpu(0)&gt;
</code></pre>

<p>广义上讲，当数据样本数为\(n\)，特征数为\(d\)时，线性回归的矢量计算表达式为</p>

<p>\[\boldsymbol{\hat{y}} = \boldsymbol{X} \boldsymbol{w} + b,\]</p>

<p>其中模型输出\(\boldsymbol{\hat{y}} \in \mathbb{R}^{n \times 1}\)， 批量数据样本特征\(\boldsymbol{X} \in \mathbb{R}^{n \times d}\)，权重\(\boldsymbol{w} \in \mathbb{R}^{d \times 1}\)， 偏差\(b \in \mathbb{R}\)。相应地，批量数据样本标签\(\boldsymbol{y} \in \mathbb{R}^{n \times 1}\)。设模型参数\(\boldsymbol{\theta} = [w_1, w_2, b]^\top\)，我们可以重写损失函数为</p>

<p>\[\ell(\boldsymbol{\theta})=\frac{1}{2n}(\boldsymbol{\hat{y}}-\boldsymbol{y})^\top(\boldsymbol{\hat{y}}-\boldsymbol{y}).\]</p>

<p>小批量随机梯度下降的迭代步骤将相应地改写为</p>

<p>\[\boldsymbol{\theta} \leftarrow \boldsymbol{\theta} -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}   \nabla_{\boldsymbol{\theta}} \ell^{(i)}(\boldsymbol{\theta}),\]</p>

<p>其中梯度是损失有关3个为标量的模型参数的偏导数组成的向量：</p>

<p><img src="media/15816738049513/Pasted%20Screenshot%202020-02-14%2018-05-56.png" alt="Pasted Screenshot 2020-02-14 18-05-56"/></p>

<h3 id="toc_11">小结</h3>

<ul>
<li>和大多数深度学习模型一样，对于线性回归这样一种单层神经网络，它的基本要素包括模型、训练数据、损失函数和优化算法。</li>
<li>既可以用神经网络图表示线性回归，又可以用矢量计算表示该模型。</li>
<li>应该尽可能采用矢量计算，以提升计算效率。</li>
</ul>

<h3 id="toc_12">练习</h3>

<ul>
<li>使用其他包（如NumPy）或其他编程语言（如MATLAB），比较相加两个向量的两种方法的运行时间。</li>
</ul>

<h2 id="toc_13">线性回归的从零开始实现</h2>

<p>在了解了线性回归的背景知识之后，现在我们可以动手实现它了。尽管强大的深度学习框架可以减少大量重复性工作，但若过于依赖它提供的便利，会导致我们很难深入理解深度学习是如何工作的。因此，本节将介绍如何只利用<code>NDArray</code>和<code>autograd</code>来实现一个线性回归的训练。</p>

<p>首先，导入本节中实验所需的包或模块，其中的matplotlib包可用于作图，且设置成嵌入显示。</p>

<pre><code class="language-python">%matplotlib inline
from IPython import display
from matplotlib import pyplot as plt
from mxnet import autograd, nd
import random
</code></pre>

<h3 id="toc_14">生成数据集</h3>

<p>我们构造一个简单的人工训练数据集，它可以使我们能够直观比较学到的参数和真实的模型参数的区别。设训练数据集样本数为1000，输入个数（特征数）为2。给定随机生成的批量样本特征\(\boldsymbol{X} \in \mathbb{R}^{1000 \times 2}\)，我们使用线性回归模型真实权重\(\boldsymbol{w} = [2, -3.4]^\top\)和偏差\(b = 4.2\)，以及一个随机噪声项\(\epsilon\)来生成标签</p>

<p>\[\boldsymbol{y} = \boldsymbol{X}\boldsymbol{w} + b + \epsilon,\]</p>

<p>其中噪声项\(\epsilon\)服从均值为0、标准差为0.01的正态分布。噪声代表了数据集中无意义的干扰。下面，让我们生成数据集。</p>

<pre><code class="language-python">num_inputs = 2
num_examples = 1000
true_w = [2, -3.4]
true_b = 4.2
features = nd.random.normal(scale=1, shape=(num_examples, num_inputs))
labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b
labels += nd.random.normal(scale=0.01, shape=labels.shape)
</code></pre>

<p>注意，<code>features</code>的每一行是一个长度为2的向量，而<code>labels</code>的每一行是一个长度为1的向量（标量）。</p>

<pre><code class="language-python">features[0], labels[0]
</code></pre>

<pre><code class="language-text">(
 [2.2122064 0.7740038]
 &lt;NDArray 2 @cpu(0)&gt;, 
 [6.000587]
 &lt;NDArray 1 @cpu(0)&gt;)
</code></pre>

<p>通过生成第二个特征<code>features[:, 1]</code>和标签 <code>labels</code> 的散点图，可以更直观地观察两者间的线性关系。</p>

<pre><code class="language-python">def use_svg_display():
    # 用矢量图显示
    display.set_matplotlib_formats(&#39;svg&#39;)

def set_figsize(figsize=(3.5, 2.5)):
    use_svg_display()
    # 设置图的尺寸
    plt.rcParams[&#39;figure.figsize&#39;] = figsize

set_figsize()
plt.scatter(features[:, 1].asnumpy(), labels.asnumpy(), 1);  # 加分号只显示图
</code></pre>

<p><img src="media/15816738049513/output_7_0.svg" alt="output_7_0"/></p>

<p>我们将上面的<code>plt</code>作图函数以及<code>use_svg_display</code>函数和<code>set_figsize</code>函数定义在<code>d2lzh</code>包里。以后在作图时，我们将直接调用<code>d2lzh.plt</code>。由于<code>plt</code>在<code>d2lzh</code>包中是一个全局变量，我们在作图前只需要调用<code>d2lzh.set_figsize()</code>即可打印矢量图并设置图的尺寸。</p>

<h3 id="toc_15">读取数据</h3>

<p>在训练模型的时候，我们需要遍历数据集并不断读取小批量数据样本。这里我们定义一个函数：它每次返回<code>batch_size</code>（批量大小）个随机样本的特征和标签。</p>

<pre><code class="language-python"># 本函数已保存在d2lzh包中方便以后使用
def data_iter(batch_size, features, labels):
    num_examples = len(features)
    indices = list(range(num_examples))
    random.shuffle(indices)  # 样本的读取顺序是随机的
    for i in range(0, num_examples, batch_size):
        j = nd.array(indices[i: min(i + batch_size, num_examples)])
        yield features.take(j), labels.take(j)  # take函数根据索引返回对应元素
</code></pre>

<p>让我们读取第一个小批量数据样本并打印。每个批量的特征形状为(10, 2)，分别对应批量大小和输入个数；标签形状为批量大小。</p>

<pre><code class="language-python">batch_size = 10

for X, y in data_iter(batch_size, features, labels):
    print(X, y)
    break
</code></pre>

<pre><code class="language-text">[[ 1.8523372  -1.1978964 ]
 [-1.464119    0.50711036]
 [-0.02477526 -0.3930928 ]
 [ 0.5264945  -0.4439822 ]
 [-0.65918195 -0.7501681 ]
 [ 2.0027666   0.04691195]
 [ 0.23753463 -0.16727363]
 [-1.2387311   1.6483529 ]
 [ 1.3671521   0.4300703 ]
 [-0.13764037  3.4004579 ]]
&lt;NDArray 10x2 @cpu(0)&gt; 
[11.983968   -0.46103162  5.501128    6.778484    5.426218    8.048336
  5.2323737  -3.8805866   5.4759536  -7.6405106 ]
&lt;NDArray 10 @cpu(0)&gt;
</code></pre>

<h3 id="toc_16">初始化模型参数</h3>

<p>我们将权重初始化成均值为0、标准差为0.01的正态随机数，偏差则初始化成0。</p>

<pre><code class="language-python">w = nd.random.normal(scale=0.01, shape=(num_inputs, 1))
b = nd.zeros(shape=(1,))
</code></pre>

<p>之后的模型训练中，需要对这些参数求梯度来迭代参数的值，因此我们需要创建它们的梯度。</p>

<pre><code class="language-python">w.attach_grad()
b.attach_grad()
</code></pre>

<h3 id="toc_17">定义模型</h3>

<p>下面是线性回归的矢量计算表达式的实现。我们使用<code>dot</code>函数做矩阵乘法。</p>

<pre><code class="language-python">def linreg(X, w, b):  # 本函数已保存在d2lzh包中方便以后使用
    return nd.dot(X, w) + b
</code></pre>

<h3 id="toc_18">定义损失函数</h3>

<p>我们使用上一节描述的平方损失来定义线性回归的损失函数。在实现中，我们需要把真实值<code>y</code>变形成预测值<code>y_hat</code>的形状。以下函数返回的结果也将和<code>y_hat</code>的形状相同。</p>

<pre><code class="language-python">def squared_loss(y_hat, y):  # 本函数已保存在d2lzh包中方便以后使用
    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2
</code></pre>

<h3 id="toc_19">定义优化算法</h3>

<p>以下的<code>sgd</code>函数实现了上一节中介绍的小批量随机梯度下降算法。它通过不断迭代模型参数来优化损失函数。这里自动求梯度模块计算得来的梯度是一个批量样本的梯度和。我们将它除以批量大小来得到平均值。</p>

<pre><code class="language-python">def sgd(params, lr, batch_size):  # 本函数已保存在d2lzh包中方便以后使用
    for param in params:
        param[:] = param - lr * param.grad / batch_size
</code></pre>

<h3 id="toc_20">训练模型</h3>

<p>在训练中，我们将多次迭代模型参数。在每次迭代中，我们根据当前读取的小批量数据样本（特征<code>X</code>和标签<code>y</code>），通过调用反向函数<code>backward</code>计算小批量随机梯度，并调用优化算法<code>sgd</code>迭代模型参数。由于我们之前设批量大小<code>batch_size</code>为10，每个小批量的损失<code>l</code>的形状为(10, 1)。回忆一下<a href="../chapter_prerequisite/autograd.ipynb">“自动求梯度”</a>一节。由于变量<code>l</code>并不是一个标量，运行<code>l.backward()</code>将对<code>l</code>中元素求和得到新的变量，再求该变量有关模型参数的梯度。</p>

<p>在一个迭代周期（epoch）中，我们将完整遍历一遍<code>data_iter</code>函数，并对训练数据集中所有样本都使用一次（假设样本数能够被批量大小整除）。这里的迭代周期个数<code>num_epochs</code>和学习率<code>lr</code>都是超参数，分别设3和0.03。在实践中，大多超参数都需要通过反复试错来不断调节。虽然迭代周期数设得越大模型可能越有效，但是训练时间可能过长。而有关学习率对模型的影响，我们会在后面“优化算法”一章中详细介绍。</p>

<pre><code class="language-python">lr = 0.03
num_epochs = 3
net = linreg
loss = squared_loss

for epoch in range(num_epochs):  # 训练模型一共需要num_epochs个迭代周期
    # 在每一个迭代周期中，会使用训练数据集中所有样本一次（假设样本数能够被批量大小整除）。X
    # 和y分别是小批量样本的特征和标签
    for X, y in data_iter(batch_size, features, labels):
        with autograd.record():
            l = loss(net(X, w, b), y)  # l是有关小批量X和y的损失
        l.backward()  # 小批量的损失对模型参数求梯度
        sgd([w, b], lr, batch_size)  # 使用小批量随机梯度下降迭代模型参数
    train_l = loss(net(features, w, b), labels)
    print(&#39;epoch %d, loss %f&#39; % (epoch + 1, train_l.mean().asnumpy()))
</code></pre>

<pre><code class="language-text">epoch 1, loss 0.040909
epoch 2, loss 0.000152
epoch 3, loss 0.000050
</code></pre>

<p>训练完成后，我们可以比较学到的参数和用来生成训练集的真实参数。它们应该很接近。</p>

<pre><code class="language-python">true_w, w
</code></pre>

<pre><code class="language-text">([2, -3.4], 
 [[ 1.9996984]
  [-3.3998005]]
 &lt;NDArray 2x1 @cpu(0)&gt;)
</code></pre>

<pre><code class="language-python">true_b, b
</code></pre>

<pre><code class="language-text">(4.2, 
 [4.20011]
 &lt;NDArray 1 @cpu(0)&gt;)
</code></pre>

<h3 id="toc_21">小结</h3>

<ul>
<li>可以看出，仅使用<code>NDArray</code>和<code>autograd</code>模块就可以很容易地实现一个模型。接下来，本书会在此基础上描述更多深度学习模型，并介绍怎样使用更简洁的代码（见下一节）来实现它们。</li>
</ul>

<h3 id="toc_22">练习</h3>

<ul>
<li>为什么<code>squared_loss</code>函数中需要使用<code>reshape</code>函数？</li>
<li>尝试使用不同的学习率，观察损失函数值的下降快慢。</li>
<li>如果样本个数不能被批量大小整除，<code>data_iter</code>函数的行为会有什么变化？</li>
</ul>

<h2 id="toc_23">softmax回归</h2>

<p>前几节介绍的线性回归模型适用于输出为连续值的情景。在另一类情景中，模型输出可以是一个像图像类别这样的离散值。对于这样的离散值预测问题，我们可以使用诸如softmax回归在内的分类模型。和线性回归不同，softmax回归的输出单元从一个变成了多个，且引入了softmax运算使输出更适合离散值的预测和训练。本节以softmax回归模型为例，介绍神经网络中的分类模型。</p>

<h3 id="toc_24">分类问题</h3>

<p>让我们考虑一个简单的图像分类问题，其输入图像的高和宽均为2像素，且色彩为灰度。这样每个像素值都可以用一个标量表示。我们将图像中的4像素分别记为\(x_1, x_2, x_3, x_4\)。假设训练数据集中图像的真实标签为狗、猫或鸡（假设可以用4像素表示出这3种动物），这些标签分别对应离散值\(y_1, y_2, y_3\)。</p>

<p>我们通常使用离散的数值来表示类别，例如\(y_1=1, y_2=2, y_3=3\)。如此，一张图像的标签为1、2和3这3个数值中的一个。虽然我们仍然可以使用回归模型来进行建模，并将预测值就近定点化到1、2和3这3个离散值之一，但这种连续值到离散值的转化通常会影响到分类质量。因此我们一般使用更加适合离散值输出的模型来解决分类问题。</p>

<h4 id="toc_25">softmax回归模型</h4>

<p>softmax回归跟线性回归一样将输入特征与权重做线性叠加。与线性回归的一个主要不同在于，softmax回归的输出值个数等于标签里的类别数。因为一共有4种特征和3种输出动物类别，所以权重包含12个标量（带下标的\(w\)）、偏差包含3个标量（带下标的\(b\)），且对每个输入计算\(o_1, o_2, o_3\)这3个输出：</p>

<p>\[<br/>
\begin{aligned}<br/>
o_1 &amp;= x_1 w_{11} + x_2 w_{21} + x_3 w_{31} + x_4 w_{41} + b_1,\\<br/>
o_2 &amp;= x_1 w_{12} + x_2 w_{22} + x_3 w_{32} + x_4 w_{42} + b_2,\\<br/>
o_3 &amp;= x_1 w_{13} + x_2 w_{23} + x_3 w_{33} + x_4 w_{43} + b_3.<br/>
\end{aligned}<br/>
\]</p>

<p>图3.2用神经网络图描绘了上面的计算。softmax回归同线性回归一样，也是一个单层神经网络。由于每个输出\(o_1, o_2, o_3\)的计算都要依赖于所有的输入\(x_1, x_2, x_3, x_4\)，softmax回归的输出层也是一个全连接层。</p>

<p><img src="media/15816738049513/softmaxreg.svg" alt="softmax回归是一个单层神经网络"/></p>

<h4 id="toc_26">softmax运算</h4>

<p>既然分类问题需要得到离散的预测输出，一个简单的办法是将输出值\(o_i\)当作预测类别是\(i\)的置信度，并将值最大的输出所对应的类作为预测输出，即输出\(\operatorname*{argmax}_i o_i\)。例如，如果\(o_1,o_2,o_3\)分别为\(0.1,10,0.1\)，由于\(o_2\)最大，那么预测类别为2，其代表猫。</p>

<p>然而，直接使用输出层的输出有两个问题。一方面，由于输出层的输出值的范围不确定，我们难以直观上判断这些值的意义。例如，刚才举的例子中的输出值10表示“很置信”图像类别为猫，因为该输出值是其他两类的输出值的100倍。但如果\(o_1=o_3=10^3\)，那么输出值10却又表示图像类别为猫的概率很低。另一方面，由于真实标签是离散值，这些离散值与不确定范围的输出值之间的误差难以衡量。</p>

<p>softmax运算符（softmax operator）解决了以上两个问题。它通过下式将输出值变换成值为正且和为1的概率分布：</p>

<p>\[\hat{y}_1, \hat{y}_2, \hat{y}_3 = \text{softmax}(o_1, o_2, o_3),\]</p>

<p>其中</p>

<p>\[<br/>
\hat{y}_1 = \frac{ \exp(o_1)}{\sum_{i=1}^3 \exp(o_i)},\quad<br/>
\hat{y}_2 = \frac{ \exp(o_2)}{\sum_{i=1}^3 \exp(o_i)},\quad<br/>
\hat{y}_3 = \frac{ \exp(o_3)}{\sum_{i=1}^3 \exp(o_i)}.<br/>
\]</p>

<p>容易看出\(\hat{y}_1 + \hat{y}_2 + \hat{y}_3 = 1\)且\(0 \leq \hat{y}_1, \hat{y}_2, \hat{y}_3 \leq 1\)，因此\(\hat{y}_1, \hat{y}_2, \hat{y}_3\)是一个合法的概率分布。这时候，如果\(\hat{y}_2=0.8\)，不管\(\hat{y}_1\)和\(\hat{y}_3\)的值是多少，我们都知道图像类别为猫的概率是80%。此外，我们注意到</p>

<p>\[\operatorname*{argmax}_i o_i = \operatorname*{argmax}_i \hat y_i,\]</p>

<p>因此softmax运算不改变预测类别输出。</p>

<h3 id="toc_27">单样本分类的矢量计算表达式</h3>

<p>为了提高计算效率，我们可以将单样本分类通过矢量计算来表达。在上面的图像分类问题中，假设softmax回归的权重和偏差参数分别为</p>

<p>\[<br/>
\boldsymbol{W} = <br/>
\begin{bmatrix}<br/>
    w_{11} &amp; w_{12} &amp; w_{13} \\<br/>
    w_{21} &amp; w_{22} &amp; w_{23} \\<br/>
    w_{31} &amp; w_{32} &amp; w_{33} \\<br/>
    w_{41} &amp; w_{42} &amp; w_{43}<br/>
\end{bmatrix},\quad<br/>
\boldsymbol{b} = <br/>
\begin{bmatrix}<br/>
    b_1 &amp; b_2 &amp; b_3<br/>
\end{bmatrix},<br/>
\]</p>

<p>设高和宽分别为2个像素的图像样本\(i\)的特征为</p>

<p>\[\boldsymbol{x}^{(i)} = \begin{bmatrix}x_1^{(i)} &amp; x_2^{(i)} &amp; x_3^{(i)} &amp; x_4^{(i)}\end{bmatrix},\]</p>

<p>输出层的输出为</p>

<p>\[\boldsymbol{o}^{(i)} = \begin{bmatrix}o_1^{(i)} &amp; o_2^{(i)} &amp; o_3^{(i)}\end{bmatrix},\]</p>

<p>预测为狗、猫或鸡的概率分布为</p>

<p>\[\boldsymbol{\hat{y}}^{(i)} = \begin{bmatrix}\hat{y}_1^{(i)} &amp; \hat{y}_2^{(i)} &amp; \hat{y}_3^{(i)}\end{bmatrix}.\]</p>

<p>softmax回归对样本\(i\)分类的矢量计算表达式为</p>

<p>\[<br/>
\begin{aligned}<br/>
\boldsymbol{o}^{(i)} &amp;= \boldsymbol{x}^{(i)} \boldsymbol{W} + \boldsymbol{b},\\<br/>
\boldsymbol{\hat{y}}^{(i)} &amp;= \text{softmax}(\boldsymbol{o}^{(i)}).<br/>
\end{aligned}<br/>
\]</p>

<h3 id="toc_28">小批量样本分类的矢量计算表达式</h3>

<p>为了进一步提升计算效率，我们通常对小批量数据做矢量计算。广义上讲，给定一个小批量样本，其批量大小为\(n\)，输入个数（特征数）为\(d\)，输出个数（类别数）为\(q\)。设批量特征为\(\boldsymbol{X} \in \mathbb{R}^{n \times d}\)。假设softmax回归的权重和偏差参数分别为\(\boldsymbol{W} \in \mathbb{R}^{d \times q}\)和\(\boldsymbol{b} \in \mathbb{R}^{1 \times q}\)。softmax回归的矢量计算表达式为</p>

<p>\[<br/>
\begin{aligned}<br/>
\boldsymbol{O} &amp;= \boldsymbol{X} \boldsymbol{W} + \boldsymbol{b},\\<br/>
\boldsymbol{\hat{Y}} &amp;= \text{softmax}(\boldsymbol{O}),<br/>
\end{aligned}<br/>
\]</p>

<p>其中的加法运算使用了广播机制，\(\boldsymbol{O}, \boldsymbol{\hat{Y}} \in \mathbb{R}^{n \times q}\)且这两个矩阵的第\(i\)行分别为样本\(i\)的输出\(\boldsymbol{o}^{(i)}\)和概率分布\(\boldsymbol{\hat{y}}^{(i)}\)。</p>

<h3 id="toc_29">交叉熵损失函数</h3>

<p>前面提到，使用softmax运算后可以更方便地与离散标签计算误差。我们已经知道，softmax运算将输出变换成一个合法的类别预测分布。实际上，真实标签也可以用类别分布表达：对于样本\(i\)，我们构造向量\(\boldsymbol{y}^{(i)}\in \mathbb{R}^{q}\) ，使其第\(y^{(i)}\)（样本\(i\)类别的离散数值）个元素为1，其余为0。这样我们的训练目标可以设为使预测概率分布\(\boldsymbol{\hat y}^{(i)}\)尽可能接近真实的标签概率分布\(\boldsymbol{y}^{(i)}\)。</p>

<p>我们可以像线性回归那样使用平方损失函数\(\|\boldsymbol{\hat y}^{(i)}-\boldsymbol{y}^{(i)}\|^2/2\)。然而，想要预测分类结果正确，我们其实并不需要预测概率完全等于标签概率。例如，在图像分类的例子里，如果\(y^{(i)}=3\)，那么我们只需要\(\hat{y}^{(i)}_3\)比其他两个预测值\(\hat{y}^{(i)}_1\)和\(\hat{y}^{(i)}_2\)大就行了。即使\(\hat{y}^{(i)}_3\)值为0.6，不管其他两个预测值为多少，类别预测均正确。而平方损失则过于严格，例如\(\hat y^{(i)}_1=\hat y^{(i)}_2=0.2\)比\(\hat y^{(i)}_1=0, \hat y^{(i)}_2=0.4\)的损失要小很多，虽然两者都有同样正确的分类预测结果。</p>

<p>改善上述问题的一个方法是使用更适合衡量两个概率分布差异的测量函数。其中，交叉熵（cross entropy）是一个常用的衡量方法：</p>

<p>\[H\left(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}\right ) = -\sum_{j=1}^q y_j^{(i)} \log \hat y_j^{(i)},\]</p>

<p>其中带下标的\(y_j^{(i)}\)是向量\(\boldsymbol y^{(i)}\)中非0即1的元素，需要注意将它与样本\(i\)类别的离散数值，即不带下标的\(y^{(i)}\)区分。在上式中，我们知道向量\(\boldsymbol y^{(i)}\)中只有第\(y^{(i)}\)个元素\(y^{(i)}_{y^{(i)}}\)为1，其余全为0，于是\(H(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}) = -\log \hat y_{y^{(i)}}^{(i)}\)。也就是说，交叉熵只关心对正确类别的预测概率，因为只要其值足够大，就可以确保分类结果正确。当然，遇到一个样本有多个标签时，例如图像里含有不止一个物体时，我们并不能做这一步简化。但即便对于这种情况，交叉熵同样只关心对图像中出现的物体类别的预测概率。</p>

<p>假设训练数据集的样本数为\(n\)，交叉熵损失函数定义为<br/>
\[\ell(\boldsymbol{\Theta}) = \frac{1}{n} \sum_{i=1}^n H\left(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}\right ),\]</p>

<p>其中\(\boldsymbol{\Theta}\)代表模型参数。同样地，如果每个样本只有一个标签，那么交叉熵损失可以简写成\(\ell(\boldsymbol{\Theta}) = -(1/n)  \sum_{i=1}^n \log \hat y_{y^{(i)}}^{(i)}\)。从另一个角度来看，我们知道最小化\(\ell(\boldsymbol{\Theta})\)等价于最大化\(\exp(-n\ell(\boldsymbol{\Theta}))=\prod_{i=1}^n \hat y_{y^{(i)}}^{(i)}\)，即最小化交叉熵损失函数等价于最大化训练数据集所有标签类别的联合预测概率。</p>

<h3 id="toc_30">模型预测及评价</h3>

<p>在训练好softmax回归模型后，给定任一样本特征，就可以预测每个输出类别的概率。通常，我们把预测概率最大的类别作为输出类别。如果它与真实类别（标签）一致，说明这次预测是正确的。在之后<a href="softmax-regression-scratch.ipynb">“softmax回归的从零开始实现”</a>一节的实验中，我们将使用准确率（accuracy）来评价模型的表现。它等于正确预测数量与总预测数量之比。</p>

<h3 id="toc_31">小结</h3>

<ul>
<li>softmax回归适用于分类问题。它使用softmax运算输出类别的概率分布。</li>
<li>softmax回归是一个单层神经网络，输出个数等于分类问题中的类别个数。</li>
<li>交叉熵适合衡量两个概率分布的差异。</li>
</ul>

<h3 id="toc_32">练习</h3>

<ul>
<li>查阅资料，了解最大似然估计。它与最小化交叉熵损失函数有哪些异曲同工之妙？</li>
</ul>

<h2 id="toc_33">softmax回归的从零开始实现</h2>

<p>这一节我们来动手实现softmax回归。首先导入本节实现所需的包或模块。</p>

<pre><code class="language-python">%matplotlib inline
import d2lzh as d2l
from mxnet import autograd, nd
</code></pre>

<h3 id="toc_34">获取和读取数据</h3>

<p>我们将使用Fashion-MNIST数据集，并设置批量大小为256。</p>

<pre><code class="language-python">batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
</code></pre>

<h3 id="toc_35">初始化模型参数</h3>

<p>跟线性回归中的例子一样，我们将使用向量表示每个样本。已知每个样本输入是高和宽均为28像素的图像。模型的输入向量的长度是\(28 \times 28 = 784\)：该向量的每个元素对应图像中每个像素。由于图像有10个类别，单层神经网络输出层的输出个数为10，因此softmax回归的权重和偏差参数分别为\(784 \times 10\)和\(1 \times 10\)的矩阵。</p>

<pre><code class="language-python">num_inputs = 784
num_outputs = 10

W = nd.random.normal(scale=0.01, shape=(num_inputs, num_outputs))
b = nd.zeros(num_outputs)
</code></pre>

<p>同之前一样，我们要为模型参数附上梯度。</p>

<pre><code class="language-python">W.attach_grad()
b.attach_grad()
</code></pre>

<h3 id="toc_36">实现softmax运算</h3>

<p>在介绍如何定义softmax回归之前，我们先描述一下对如何对多维<code>NDArray</code>按维度操作。在下面的例子中，给定一个<code>NDArray</code>矩阵<code>X</code>。我们可以只对其中同一列（<code>axis=0</code>）或同一行（<code>axis=1</code>）的元素求和，并在结果中保留行和列这两个维度（<code>keepdims=True</code>）。</p>

<pre><code class="language-python">X = nd.array([[1, 2, 3], [4, 5, 6]])
X.sum(axis=0, keepdims=True), X.sum(axis=1, keepdims=True)
</code></pre>

<pre><code class="language-text">(
 [[5. 7. 9.]]
 &lt;NDArray 1x3 @cpu(0)&gt;, 
 [[ 6.]
  [15.]]
 &lt;NDArray 2x1 @cpu(0)&gt;)
</code></pre>

<p>下面我们就可以定义前面小节里介绍的softmax运算了。在下面的函数中，矩阵<code>X</code>的行数是样本数，列数是输出个数。为了表达样本预测各个输出的概率，softmax运算会先通过<code>exp</code>函数对每个元素做指数运算，再对<code>exp</code>矩阵同行元素求和，最后令矩阵每行各元素与该行元素之和相除。这样一来，最终得到的矩阵每行元素和为1且非负。因此，该矩阵每行都是合法的概率分布。softmax运算的输出矩阵中的任意一行元素代表了一个样本在各个输出类别上的预测概率。</p>

<pre><code class="language-python">def softmax(X):
    X_exp = X.exp()
    partition = X_exp.sum(axis=1, keepdims=True)
    return X_exp / partition  # 这里应用了广播机制
</code></pre>

<p>可以看到，对于随机输入，我们将每个元素变成了非负数，且每一行和为1。</p>

<pre><code class="language-python">X = nd.random.normal(shape=(2, 5))
X_prob = softmax(X)
X_prob, X_prob.sum(axis=1)
</code></pre>

<pre><code class="language-text">(
 [[0.21324193 0.33961776 0.1239742  0.27106097 0.05210521]
  [0.11462264 0.3461234  0.19401033 0.29583326 0.04941036]]
 &lt;NDArray 2x5 @cpu(0)&gt;, 
 [1.0000001 1.       ]
 &lt;NDArray 2 @cpu(0)&gt;)
</code></pre>

<h3 id="toc_37">定义模型</h3>

<p>有了softmax运算，我们可以定义上节描述的softmax回归模型了。这里通过<code>reshape</code>函数将每张原始图像改成长度为<code>num_inputs</code>的向量。</p>

<pre><code class="language-python">def net(X):
    return softmax(nd.dot(X.reshape((-1, num_inputs)), W) + b)
</code></pre>

<h3 id="toc_38">定义损失函数</h3>

<p>上一节中，我们介绍了softmax回归使用的交叉熵损失函数。为了得到标签的预测概率，我们可以使用<code>pick</code>函数。在下面的例子中，变量<code>y_hat</code>是2个样本在3个类别的预测概率，变量<code>y</code>是这2个样本的标签类别。通过使用<code>pick</code>函数，我们得到了2个样本的标签的预测概率。与<a href="softmax-regression.ipynb">“softmax回归”</a>一节数学表述中标签类别离散值从1开始逐一递增不同，在代码中，标签类别的离散值是从0开始逐一递增的。</p>

<pre><code class="language-python">y_hat = nd.array([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])
y = nd.array([0, 2], dtype=&#39;int32&#39;)
nd.pick(y_hat, y)
</code></pre>

<pre><code class="language-text">[0.1 0.5]
&lt;NDArray 2 @cpu(0)&gt;
</code></pre>

<p>下面实现了<a href="softmax-regression.ipynb">“softmax回归”</a>一节中介绍的交叉熵损失函数。</p>

<pre><code class="language-python">def cross_entropy(y_hat, y):
    return -nd.pick(y_hat, y).log()
</code></pre>

<h3 id="toc_39">计算分类准确率</h3>

<p>给定一个类别的预测概率分布<code>y_hat</code>，我们把预测概率最大的类别作为输出类别。如果它与真实类别<code>y</code>一致，说明这次预测是正确的。分类准确率即正确预测数量与总预测数量之比。</p>

<p>为了演示准确率的计算，下面定义准确率<code>accuracy</code>函数。其中<code>y_hat.argmax(axis=1)</code>返回矩阵<code>y_hat</code>每行中最大元素的索引，且返回结果与变量<code>y</code>形状相同。我们在<a href="../chapter_prerequisite/ndarray.ipynb">“数据操作”</a>一节介绍过，相等条件判断式<code>(y_hat.argmax(axis=1) == y)</code>是一个值为0（相等为假）或1（相等为真）的<code>NDArray</code>。由于标签类型为整数，我们先将变量<code>y</code>变换为浮点数再进行相等条件判断。</p>

<pre><code class="language-python">def accuracy(y_hat, y):
    return (y_hat.argmax(axis=1) == y.astype(&#39;float32&#39;)).mean().asscalar()
</code></pre>

<p>让我们继续使用在演示<code>pick</code>函数时定义的变量<code>y_hat</code>和<code>y</code>，并将它们分别作为预测概率分布和标签。可以看到，第一个样本预测类别为2（该行最大元素0.6在本行的索引为2），与真实标签0不一致；第二个样本预测类别为2（该行最大元素0.5在本行的索引为2），与真实标签2一致。因此，这两个样本上的分类准确率为0.5。</p>

<pre><code class="language-python">accuracy(y_hat, y)
</code></pre>

<pre><code class="language-text">0.5
</code></pre>

<p>类似地，我们可以评价模型<code>net</code>在数据集<code>data_iter</code>上的准确率。</p>

<pre><code class="language-python"># 本函数已保存在d2lzh包中方便以后使用。该函数将被逐步改进：它的完整实现将在“图像增广”一节中
# 描述
def evaluate_accuracy(data_iter, net):
    acc_sum, n = 0.0, 0
    for X, y in data_iter:
        y = y.astype(&#39;float32&#39;)
        acc_sum += (net(X).argmax(axis=1) == y).sum().asscalar()
        n += y.size
    return acc_sum / n
</code></pre>

<p>因为我们随机初始化了模型<code>net</code>，所以这个随机模型的准确率应该接近于类别个数10的倒数0.1。</p>

<pre><code class="language-python">evaluate_accuracy(test_iter, net)
</code></pre>

<pre><code class="language-text">0.0925
</code></pre>

<h3 id="toc_40">训练模型</h3>

<p>训练softmax回归的实现跟<a href="linear-regression-scratch.ipynb">“线性回归的从零开始实现”</a>一节介绍的线性回归中的实现非常相似。我们同样使用小批量随机梯度下降来优化模型的损失函数。在训练模型时，迭代周期数<code>num_epochs</code>和学习率<code>lr</code>都是可以调的超参数。改变它们的值可能会得到分类更准确的模型。</p>

<pre><code class="language-python">num_epochs, lr = 5, 0.1

# 本函数已保存在d2lzh包中方便以后使用
def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,
              params=None, lr=None, trainer=None):
    for epoch in range(num_epochs):
        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0
        for X, y in train_iter:
            with autograd.record():
                y_hat = net(X)
                l = loss(y_hat, y).sum()
            l.backward()
            if trainer is None:
                d2l.sgd(params, lr, batch_size)
            else:
                trainer.step(batch_size)  # “softmax回归的简洁实现”一节将用到
            y = y.astype(&#39;float32&#39;)
            train_l_sum += l.asscalar()
            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()
            n += y.size
        test_acc = evaluate_accuracy(test_iter, net)
        print(&#39;epoch %d, loss %.4f, train acc %.3f, test acc %.3f&#39;
              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))

train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, batch_size,
          [W, b], lr)
</code></pre>

<pre><code class="language-text">epoch 1, loss 0.7880, train acc 0.746, test acc 0.794
epoch 2, loss 0.5739, train acc 0.810, test acc 0.827
epoch 3, loss 0.5297, train acc 0.823, test acc 0.832
epoch 4, loss 0.5046, train acc 0.830, test acc 0.838
epoch 5, loss 0.4903, train acc 0.835, test acc 0.838
</code></pre>

<h3 id="toc_41">预测</h3>

<p>训练完成后，现在就可以演示如何对图像进行分类了。给定一系列图像（第三行图像输出），我们比较一下它们的真实标签（第一行文本输出）和模型预测结果（第二行文本输出）。</p>

<pre><code class="language-python">for X, y in test_iter:
    break

true_labels = d2l.get_fashion_mnist_labels(y.asnumpy())
pred_labels = d2l.get_fashion_mnist_labels(net(X).argmax(axis=1).asnumpy())
titles = [true + &#39;\n&#39; + pred for true, pred in zip(true_labels, pred_labels)]

d2l.show_fashion_mnist(X[0:9], titles[0:9])
</code></pre>

<p><img src="media/15816738049513/output_31_0.svg" alt="output_31_0"/></p>

<h3 id="toc_42">小结</h3>

<ul>
<li>可以使用softmax回归做多类别分类。与训练线性回归相比，你会发现训练softmax回归的步骤和它非常相似：获取并读取数据、定义模型和损失函数并使用优化算法训练模型。事实上，绝大多数深度学习模型的训练都有着类似的步骤。</li>
</ul>

<h3 id="toc_43">练习</h3>

<ul>
<li>在本节中，我们直接按照softmax运算的数学定义来实现softmax函数。这可能会造成什么问题？（提示：试一试计算\(\exp(50)\)的大小。）</li>
<li>本节中的<code>cross_entropy</code>函数是按照<a href="softmax-regression.ipynb">“softmax回归”</a>一节中的交叉熵损失函数的数学定义实现的。这样的实现方式可能有什么问题？（提示：思考一下对数函数的定义域。）</li>
<li>你能想到哪些办法来解决上面的两个问题？</li>
</ul>

<h2 id="toc_44">模型选择、欠拟合和过拟合</h2>

<p>在前几节基于Fashion-MNIST数据集的实验中，我们评价了机器学习模型在训练数据集和测试数据集上的表现。如果你改变过实验中的模型结构或者超参数，你也许发现了：当模型在训练数据集上更准确时，它在测试数据集上却不一定更准确。这是为什么呢？</p>

<h3 id="toc_45">训练误差和泛化误差</h3>

<p>在解释上述现象之前，我们需要区分训练误差（training error）和泛化误差（generalization error）。通俗来讲，前者指模型在训练数据集上表现出的误差，后者指模型在任意一个测试数据样本上表现出的误差的期望，并常常通过测试数据集上的误差来近似。计算训练误差和泛化误差可以使用之前介绍过的损失函数，例如线性回归用到的平方损失函数和softmax回归用到的交叉熵损失函数。</p>

<p>让我们以高考为例来直观地解释训练误差和泛化误差这两个概念。训练误差可以认为是做往年高考试题（训练题）时的错误率，泛化误差则可以通过真正参加高考（测试题）时的答题错误率来近似。假设训练题和测试题都随机采样于一个未知的依照相同考纲的巨大试题库。如果让一名未学习中学知识的小学生去答题，那么测试题和训练题的答题错误率可能很相近。但如果换成一名反复练习训练题的高三备考生答题，即使在训练题上做到了错误率为0，也不代表真实的高考成绩会如此。</p>

<p>在机器学习里，我们通常假设训练数据集（训练题）和测试数据集（测试题）里的每一个样本都是从同一个概率分布中相互独立地生成的。基于该独立同分布假设，给定任意一个机器学习模型（含参数），它的训练误差的期望和泛化误差都是一样的。例如，如果我们将模型参数设成随机值（小学生），那么训练误差和泛化误差会非常相近。但我们从前面几节中已经了解到，模型的参数是通过在训练数据集上训练模型而学习出的，参数的选择依据了最小化训练误差（高三备考生）。所以，训练误差的期望小于或等于泛化误差。也就是说，一般情况下，由训练数据集学到的模型参数会使模型在训练数据集上的表现优于或等于在测试数据集上的表现。由于无法从训练误差估计泛化误差，一味地降低训练误差并不意味着泛化误差一定会降低。</p>

<p>机器学习模型应关注降低泛化误差。</p>

<h3 id="toc_46">模型选择</h3>

<p>在机器学习中，通常需要评估若干候选模型的表现并从中选择模型。这一过程称为模型选择（model selection）。可供选择的候选模型可以是有着不同超参数的同类模型。以多层感知机为例，我们可以选择隐藏层的个数，以及每个隐藏层中隐藏单元个数和激活函数。为了得到有效的模型，我们通常要在模型选择上下一番功夫。下面，我们来描述模型选择中经常使用的验证数据集（validation data set）。</p>

<h4 id="toc_47">验证数据集</h4>

<p>从严格意义上讲，测试集只能在所有超参数和模型参数选定后使用一次。不可以使用测试数据选择模型，如调参。由于无法从训练误差估计泛化误差，因此也不应只依赖训练数据选择模型。鉴于此，我们可以预留一部分在训练数据集和测试数据集以外的数据来进行模型选择。这部分数据被称为验证数据集，简称验证集（validation set）。例如，我们可以从给定的训练集中随机选取一小部分作为验证集，而将剩余部分作为真正的训练集。</p>

<p>然而在实际应用中，由于数据不容易获取，测试数据极少只使用一次就丢弃。因此，实践中验证数据集和测试数据集的界限可能比较模糊。从严格意义上讲，除非明确说明，否则本书中实验所使用的测试集应为验证集，实验报告的测试结果（如测试准确率）应为验证结果（如验证准确率）。</p>

<h4 id="toc_48">\(K\) 折交叉验证</h4>

<p>由于验证数据集不参与模型训练，当训练数据不够用时，预留大量的验证数据显得太奢侈。一种改善的方法是\(K\)折交叉验证（\(K\)-fold cross-validation）。在\(K\)折交叉验证中，我们把原始训练数据集分割成\(K\)个不重合的子数据集，然后我们做\(K\)次模型训练和验证。每一次，我们使用一个子数据集验证模型，并使用其他\(K-1\)个子数据集来训练模型。在这\(K\)次训练和验证中，每次用来验证模型的子数据集都不同。最后，我们对这\(K\)次训练误差和验证误差分别求平均。</p>

<h3 id="toc_49">欠拟合和过拟合</h3>

<p>接下来，我们将探究模型训练中经常出现的两类典型问题：一类是模型无法得到较低的训练误差，我们将这一现象称作欠拟合（underfitting）；另一类是模型的训练误差远小于它在测试数据集上的误差，我们称该现象为过拟合（overfitting）。在实践中，我们要尽可能同时应对欠拟合和过拟合。虽然有很多因素可能导致这两种拟合问题，在这里我们重点讨论两个因素：模型复杂度和训练数据集大小。</p>

<h4 id="toc_50">模型复杂度</h4>

<p>为了解释模型复杂度，我们以多项式函数拟合为例。给定一个由标量数据特征\(x\)和对应的标量标签\(y\)组成的训练数据集，多项式函数拟合的目标是找一个\(K\)阶多项式函数</p>

<p>\[\hat{y} = b + \sum_{k=1}^K x^k w_k\]</p>

<p>来近似\(y\)。在上式中，\(w_k\)是模型的权重参数，\(b\)是偏差参数。与线性回归相同，多项式函数拟合也使用平方损失函数。特别地，一阶多项式函数拟合又叫线性函数拟合。</p>

<p>因为高阶多项式函数模型参数更多，模型函数的选择空间更大，所以高阶多项式函数比低阶多项式函数的复杂度更高。因此，高阶多项式函数比低阶多项式函数更容易在相同的训练数据集上得到更低的训练误差。给定训练数据集，模型复杂度和误差之间的关系通常如图3.4所示。给定训练数据集，如果模型的复杂度过低，很容易出现欠拟合；如果模型复杂度过高，很容易出现过拟合。应对欠拟合和过拟合的一个办法是针对数据集选择合适复杂度的模型。</p>

<p><img src="../img/capacity_vs_error.svg" alt="模型复杂度对欠拟合和过拟合的影响"/></p>

<h4 id="toc_51">训练数据集大小</h4>

<p>影响欠拟合和过拟合的另一个重要因素是训练数据集的大小。一般来说，如果训练数据集中样本数过少，特别是比模型参数数量（按元素计）更少时，过拟合更容易发生。此外，泛化误差不会随训练数据集里样本数量增加而增大。因此，在计算资源允许的范围之内，我们通常希望训练数据集大一些，特别是在模型复杂度较高时，例如层数较多的深度学习模型。</p>

<h3 id="toc_52">多项式函数拟合实验</h3>

<p>为了理解模型复杂度和训练数据集大小对欠拟合和过拟合的影响，下面我们以多项式函数拟合为例来实验。首先导入实验需要的包或模块。</p>

<pre><code class="language-python">%matplotlib inline
import d2lzh as d2l
from mxnet import autograd, gluon, nd
from mxnet.gluon import data as gdata, loss as gloss, nn
</code></pre>

<h4 id="toc_53">生成数据集</h4>

<p>我们将生成一个人工数据集。在训练数据集和测试数据集中，给定样本特征\(x\)，我们使用如下的三阶多项式函数来生成该样本的标签：</p>

<p>\[y = 1.2x - 3.4x^2 + 5.6x^3 + 5 + \epsilon,\]</p>

<p>其中噪声项\(\epsilon\)服从均值为0、标准差为0.1的正态分布。训练数据集和测试数据集的样本数都设为100。</p>

<pre><code class="language-python">n_train, n_test, true_w, true_b = 100, 100, [1.2, -3.4, 5.6], 5
features = nd.random.normal(shape=(n_train + n_test, 1))
poly_features = nd.concat(features, nd.power(features, 2),
                          nd.power(features, 3))
labels = (true_w[0] * poly_features[:, 0] + true_w[1] * poly_features[:, 1]
          + true_w[2] * poly_features[:, 2] + true_b)
labels += nd.random.normal(scale=0.1, shape=labels.shape)
</code></pre>

<p>看一看生成的数据集的前两个样本。</p>

<pre><code class="language-python">features[:2], poly_features[:2], labels[:2]
</code></pre>

<pre><code class="language-text">(
 [[2.2122064]
  [0.7740038]]
 &lt;NDArray 2x1 @cpu(0)&gt;, 
 [[ 2.2122064   4.893857   10.826221  ]
  [ 0.7740038   0.5990819   0.46369165]]
 &lt;NDArray 2x3 @cpu(0)&gt;, 
 [51.674885   6.3585763]
 &lt;NDArray 2 @cpu(0)&gt;)
</code></pre>

<h4 id="toc_54">定义、训练和测试模型</h4>

<p>我们先定义作图函数<code>semilogy</code>，其中\(y\)轴使用了对数尺度。</p>

<pre><code class="language-python"># 本函数已保存在d2lzh包中方便以后使用
def semilogy(x_vals, y_vals, x_label, y_label, x2_vals=None, y2_vals=None,
             legend=None, figsize=(3.5, 2.5)):
    d2l.set_figsize(figsize)
    d2l.plt.xlabel(x_label)
    d2l.plt.ylabel(y_label)
    d2l.plt.semilogy(x_vals, y_vals)
    if x2_vals and y2_vals:
        d2l.plt.semilogy(x2_vals, y2_vals, linestyle=&#39;:&#39;)
        d2l.plt.legend(legend)
</code></pre>

<p>和线性回归一样，多项式函数拟合也使用平方损失函数。因为我们将尝试使用不同复杂度的模型来拟合生成的数据集，所以我们把模型定义部分放在<code>fit_and_plot</code>函数中。多项式函数拟合的训练和测试步骤与<a href="softmax-regression-scratch.ipynb">“softmax回归的从零开始实现”</a>一节介绍的softmax回归中的相关步骤类似。</p>

<pre><code class="language-python">num_epochs, loss = 100, gloss.L2Loss()

def fit_and_plot(train_features, test_features, train_labels, test_labels):
    net = nn.Sequential()
    net.add(nn.Dense(1))
    net.initialize()
    batch_size = min(10, train_labels.shape[0])
    train_iter = gdata.DataLoader(gdata.ArrayDataset(
        train_features, train_labels), batch_size, shuffle=True)
    trainer = gluon.Trainer(net.collect_params(), &#39;sgd&#39;,
                            {&#39;learning_rate&#39;: 0.01})
    train_ls, test_ls = [], []
    for _ in range(num_epochs):
        for X, y in train_iter:
            with autograd.record():
                l = loss(net(X), y)
            l.backward()
            trainer.step(batch_size)
        train_ls.append(loss(net(train_features),
                             train_labels).mean().asscalar())
        test_ls.append(loss(net(test_features),
                            test_labels).mean().asscalar())
    print(&#39;final epoch: train loss&#39;, train_ls[-1], &#39;test loss&#39;, test_ls[-1])
    semilogy(range(1, num_epochs + 1), train_ls, &#39;epochs&#39;, &#39;loss&#39;,
             range(1, num_epochs + 1), test_ls, [&#39;train&#39;, &#39;test&#39;])
    print(&#39;weight:&#39;, net[0].weight.data().asnumpy(),
          &#39;\nbias:&#39;, net[0].bias.data().asnumpy())
</code></pre>

<h4 id="toc_55">三阶多项式函数拟合（正常）</h4>

<p>我们先使用与数据生成函数同阶的三阶多项式函数拟合。实验表明，这个模型的训练误差和在测试数据集的误差都较低。训练出的模型参数也接近真实值：\(w_1 = 1.2, w_2=-3.4, w_3=5.6, b = 5\)。</p>

<pre><code class="language-python">fit_and_plot(poly_features[:n_train, :], poly_features[n_train:, :],
             labels[:n_train], labels[n_train:])
</code></pre>

<pre><code class="language-text">final epoch: train loss 0.006922996 test loss 0.011304822
weight: [[ 1.3227708 -3.3637862  5.56338  ]] 
bias: [4.952137]
</code></pre>

<p><img src="media/15816738049513/output_11_1.svg" alt="output_11_1"/></p>

<h4 id="toc_56">线性函数拟合（欠拟合）</h4>

<p>我们再试试线性函数拟合。很明显，该模型的训练误差在迭代早期下降后便很难继续降低。在完成最后一次迭代周期后，训练误差依旧很高。线性模型在非线性模型（如三阶多项式函数）生成的数据集上容易欠拟合。</p>

<pre><code class="language-python">fit_and_plot(features[:n_train, :], features[n_train:, :], labels[:n_train],
             labels[n_train:])
</code></pre>

<pre><code class="language-text">final epoch: train loss 43.99767 test loss 160.7238
weight: [[15.56557]] 
bias: [2.282313]
</code></pre>

<p><img src="media/15816738049513/output_13_1.svg" alt="output_13_1"/></p>

<h4 id="toc_57">训练样本不足（过拟合）</h4>

<p>事实上，即便使用与数据生成模型同阶的三阶多项式函数模型，如果训练样本不足，该模型依然容易过拟合。让我们只使用两个样本来训练模型。显然，训练样本过少了，甚至少于模型参数的数量。这使模型显得过于复杂，以至于容易被训练数据中的噪声影响。在迭代过程中，尽管训练误差较低，但是测试数据集上的误差却很高。这是典型的过拟合现象。</p>

<pre><code class="language-python">fit_and_plot(poly_features[0:2, :], poly_features[n_train:, :], labels[0:2],
             labels[n_train:])
</code></pre>

<pre><code class="language-text">final epoch: train loss 0.4027369 test loss 103.314186
weight: [[1.3872364 1.9376589 3.5085924]] 
bias: [1.2312856]
</code></pre>

<p><img src="media/15816738049513/output_15_1.svg" alt="output_15_1"/></p>

<p>我们将在接下来的两个小节继续讨论过拟合问题以及应对过拟合的方法。</p>

<h3 id="toc_58">小结</h3>

<ul>
<li>由于无法从训练误差估计泛化误差，一味地降低训练误差并不意味着泛化误差一定会降低。机器学习模型应关注降低泛化误差。</li>
<li>可以使用验证数据集来进行模型选择。</li>
<li>欠拟合指模型无法得到较低的训练误差，过拟合指模型的训练误差远小于它在测试数据集上的误差。</li>
<li>应选择复杂度合适的模型并避免使用过少的训练样本。</li>
</ul>

<h3 id="toc_59">练习</h3>

<ul>
<li>如果用一个三阶多项式模型来拟合一个线性模型生成的数据，可能会有什么问题？为什么？</li>
<li>在本节提到的三阶多项式拟合问题里，有没有可能把100个样本的训练误差的期望降到0，为什么？（提示：考虑噪声项的存在。）</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[深夜，来谈一谈未来]]></title>
    <link href="https://zning.me/15702894319169.html"/>
    <updated>2019-10-05T23:30:31+08:00</updated>
    <id>https://zning.me/15702894319169.html</id>
    <content type="html"><![CDATA[
<p>国庆已经过半，近期对于未来的科技发展，有些许想法从头脑中迸出。趁着假期这个机会分享给大家。</p>

<p>本文主要以脑洞为主，有些观点有些主观，如有想法请在留言区留言交流~</p>

<span id="more"></span><!-- more -->

<ul>
<li>
<a href="#toc_0">神经网络与深度学习的终结：始于硬件 败于“软件”</a>
</li>
<li>
<a href="#toc_1">弱化测试运维的开发的命运走向何方？</a>
</li>
<li>
<a href="#toc_2">算法工程师: 高精尖的岗位 稳就业的救星</a>
</li>
</ul>


<h2 id="toc_0">神经网络与深度学习的终结：始于硬件 败于“软件”</h2>

<p>这个话题，可能谈的有些异想天开：现在如此火热的人工智能的重点方向——神经网络与深度学习，怎么会败给软件？</p>

<p>当然，一眼关注到迥异的观点是人的天性。而对于“始于硬件”却没有多大关心。</p>

<p>机器智能领域(指机器学习、人工智能及其衍生学科领域)发展到现在，其实已经经历过至少两次的大起大落(具体的起落可阅读<a href="https://mp.weixin.qq.com/s/KP0cwA7ZkOW9PlHHJZJLaA">《智能时代、深度学习与未来：人类与人工智能的博弈之道》</a>)，而从本世纪第一个十年开始逐渐升温火热，并直至现在火爆。<strong>而这次成为热门的原因，归根结底并不是依靠机器智能领域有着什么基础理论的完美突破</strong>(当然近些年理论有进步是必然的)<strong>，</strong><strong>而是当下硬件性能的突飞猛进与成本的大大降低。</strong></p>

<p>可以这么说，现在的人们，只要你愿意去建立训练一个有关于机器智能领域的模型，都可以通过一台中高端游戏台式机来完成这项工作，甚至一些高配笔记本和商务机也可以完成较小模型的建立训练工作(这其中也得力于近些年割韭菜般的币圈，使得GPU硬件市场空前火热)。这就意味着，作为机器智能领域入门最最基础的生产工具，已经不像上世纪五十年代或者八十年代那样机器动辄百千万造价和占地几百平方，成本的降低必然带来大规模的兴起。即使现在两三万的N厂训练卡，对于些许个人来说也是咬牙节省半年也可承受的设备了。</p>

<p>因此，说机器智能领域本世纪头二十年的发展始于硬件一点都不为过。</p>

<p><img src="media/15702894319169/2019-10-05-001.jpeg" alt="2019-10-05-001" style="width:325px;"/></p>

<blockquote>
<p>始于硬件 败于“软件”，机器智能领域的神经网络与深度学习能否冲破牢笼？我们拭目以待<br/>
图片来自网络</p>
</blockquote>

<p>然而，领域内的“软件”，却没有如硬件般幸运。这里的“软件”之所以打引号，主要是其指的领域内基础理论与算法的发展。包括神经网络与传统机器学习在内，现在的理论与算法绝大部分都是以上世纪五十年代和八十年代的基础上进行改良和演化而来。<strong>而神经网络的内部研究，在进行了多年后仍未有进展。如此不禁令人担忧“软件”的老本还能支撑现在的机器智能领域发展多久。</strong></p>

<p>而至于深度学习，不论如何都可以看做是“通过一系列规则进行超多层(百层及以上)神经网络训练”的模型，即其“深度”体现在神经网络的层数之繁多、神经元个数之庞大、神经元互联之广泛。但是如果仔细考虑，我们可以发现，<strong>深度学习其实更多的是神经网络2.0版本，只不过这个版本靠的并不是理论与算法基础的创新，而是硬件的堆砌——毕竟，如此庞大的网络，必然需要更为多一个量级的硬件支持。</strong></p>

<p>于是，在当下，即使机器智能领域中的图形图像、计算机视觉和大数据机器学习算法——支持向量机、决策树、卷积神经网络、生成对抗网络等——比起上世纪来说有着令人瞩目的进步，<strong>但如若“软件”无法去更进一步的对基础理论和算法进行有益补充，可能接下来机器智能领域的火爆，也就达到了顶峰了。</strong></p>

<h2 id="toc_1">弱化测试运维的开发的命运走向何方？</h2>

<p>近些年，从自动化测试的提出、推广与普及，到近些年运维DevOps、AIOps的提出和推广。看上去确实是提高了生产效率，增加了程序的健壮性，进而推动了企业业务的迅速提高。不过，<strong>神奇之处在于，关于测试和运维的变革，从某种程度上来说并不是本工作岗位的人所做的创新，反而是由作为程序源动力的开发在主导着这些变革。</strong></p>

<p>很明显，自动化测试的出现与普及对于IT行业的测试岗位的影响，已经从岗位数量与岗位工资上体现的淋漓尽致。而且即使现有的测试岗，对于技术水平的要求上也逐步向开发研发看齐。而正在进行的DevOps运动更不在话下，从代码仓库自动化、持续交付持续集成(CI/CD)，到平台及服务、软件即服务，以及容器化实现的从开发到生产环境的无缝衔接，均体现了开发在融合开发与运维职责所做的努力。</p>

<p>首先声明一点: 我对这种发展推动IT从业者的生产效率提高和产出攀升上并无任何负面意见，甚至希冀这样的理想社会早日到来。不过，我发现，关于开发对测试和运维的边界模糊甚至权责接管的事情，正在我们身边发生。外行看热闹，内行看门道。<strong>同样都是IT领域，外行人以为啥都一样，其实各个领域的厮杀，现在已经到了新的阶段。</strong></p>

<p>所以，可以这么说: <strong>测试这个岗位，可以说已经被开发通过其聪敏智慧将绝大部分工作给接管了，而运维的工作也通过DevOps的运动逐渐在融合进开发的日常工作之中。</strong>而现在仍然有这些岗位的存在，很大程度上是为了开发人员解决不了的事情，可以让他们帮助去解决(没错，通俗来说这就叫“背锅”)。</p>

<p><img src="media/15702894319169/2019-10-05-002.jpeg" alt="2019-10-05-002" style="width:315px;"/></p>

<blockquote>
<p>还记得技术鄙视链吗？<br/>
图片来自网络</p>
</blockquote>

<p>这听起来很恐怖，不过这里我不想讨论测试与运维人员的出路，我想讨论的是: <strong>等待着开发的未来是什么？</strong></p>

<p>关注行业发展的人对于VS Code在近期上架的一个名叫TabNine插件会有所耳闻: 一款通过你打几个字母来猜测你所写的代码的智能代码补全工具。这个工具通过内置的一个神经网络模型(估计是自然语言处理领域的模型)，来判断你接下来会写什么样的代码。这听起来很棒很不错。</p>

<p><img src="media/15702894319169/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202019-10-05%2023.19.16.png" alt="屏幕截图 2019-10-05 23.19.16" style="width:400px;"/></p>

<blockquote>
<p>VS Code插件TabNine介绍截图</p>
</blockquote>

<p>不过这样的插件的出现，更应该让开发人员警惕: <strong>曾经所说的机器自己写代码，已不再是一个空想，正一步步成为现实。而能够做出这一步的，其实是开发背后的研发与算法工程师。</strong></p>

<p>没错，<strong>开发岗位的未来，或许真的可能会被研发与算法所研究出的自动化代码程序所替代</strong>，然后让开发去干研发与算法不能解决的事情(测试与运维: 嗯！？)。IT行业不再需要如此之多的开发岗位了，算法和研发就可以把一些小项目就给一条龙解决了。</p>

<p>正所谓: 螳螂捕蝉，黄雀在后。不到最后谁都不知道赢家是谁。</p>

<h2 id="toc_2">算法工程师: 高精尖的岗位 稳就业的救星</h2>

<p>从去年秋招开始，算法工程师这个受应届硕士研究生青睐的岗位突然在就业市场供过于求，找工作的学生纷纷大喊: 我太难了。</p>

<p>关于这个现象，在职场混迹了多年的老油条很客观的指出了一个非常现实的情况: <strong>算法岗本身就不需要这么多人，饱和也是正常现象。</strong></p>

<p>诚然，这个现实的观点是正确的，而且真真切切是现在岗位供过于求的直接原因。不过，对于算法岗位出现如此现象的其他原因，我却有很多不一样的观点。</p>

<p><img src="media/15702894319169/d923900bae3c0fd582f3d5664157f892.jpeg" alt="d923900bae3c0fd582f3d5664157f892"/></p>

<blockquote>
<p>算法既是能力的体现，又是挑战的开始</p>

<p>图片来自网络</p>
</blockquote>

<p><strong>首先，算法岗相比开发岗位对专业技能有着更高更专业的需求。</strong>在专业技术素质有着坚实基础的人更适合做算法，对于一些对算法有强依赖性的领域例如图形图像更甚。这就对我们的应届生的能力提出了很高的要求。但是，作为刚从象牙塔初入职场的学子，并不是所有人都能应付得了企业对于算法创新与改良的需求清单，反而在开发岗位长期与算法岗位交流且对算法有着熟识的人更适合转岗为算法。因此在企业需求上，应届生并不具有太多优势。</p>

<p><strong>不过至于现在的算法岗招聘，我认为很大程度上仅仅是让应届生听上去很好的岗位名称，让这个名字与高学历相称。</strong>不然岗位都叫程序员谁去应聘呢？于是，“算法工程师”这个岗位成了稳就业的救星，一剂猛药而已。初出茅庐的算法岗虽进入到工作岗位，但仍然是做着最基础的开发工作或者调包工作，对人的素质提高并没有什么帮助，唯一实在的估计就是工资了——当然这也是最关键的东西咯，不过想一想，工资高并不是岗位名称所带来的，而是你的学历和能力呀。</p>

<p>不过分析归分析，脑洞归脑洞，希望找到算法岗的小伙伴们能够加油把前两个论题所讨论的东东能够及早攻克或完成，我国社会主义现代化的实现靠你们继续啦～有意愿转算法岗的小伙伴也不必沮丧，现在的情况只能说明你的能力达不到算法岗的要求，<strong>毕竟算法岗并不是录用了就真的如传言所说高枕无忧不用996了，每一个岗位都有辛苦的一面。</strong>所以，<strong>在相近岗位汲取经验、技术与能力，再转向算法岗也是一个完美的路子。</strong></p>

<p>最后祝愿关注慧响公众号的盆友们人人早日喜提算法工程师～哈哈哈嗝。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[深度学习之《深度学习入门》学习笔记（四）神经网络的学习]]></title>
    <link href="https://zning.me/15691207315042.html"/>
    <updated>2019-09-22T10:52:11+08:00</updated>
    <id>https://zning.me/15691207315042.html</id>
    <content type="html"><![CDATA[
<p>最近学习吴恩达《Machine Learning》课程以及《深度学习入门：基于Python的理论与实现》书，一些东西总结了下。现就后者学习进行笔记总结。本文是本书的学习笔记（四）神经网络的学习。</p>

<span id="more"></span><!-- more -->

<ul>
<li><p><a href="./15656172902685.html">深度学习之《深度学习入门》学习笔记（一）Python入门</a></p></li>
<li><p><a href="./15656174993229.html">深度学习之《深度学习入门》学习笔记（二）感知机</a></p></li>
<li><p><a href="./15656175642754.html">深度学习之《深度学习入门》学习笔记（三）神经网络</a></p></li>
<li><p><a href="./15691207315042.html">深度学习之《深度学习入门》学习笔记（四）神经网络的学习</a></p>
<ul>
<li>
<a href="#toc_0">从数据中学习</a>
</li>
<li>
<a href="#toc_1">损失函数</a>
<ul>
<li>
<a href="#toc_2">均方误差</a>
</li>
<li>
<a href="#toc_3">交叉熵误差</a>
</li>
<li>
<a href="#toc_4">mini-batch学习</a>
</li>
<li>
<a href="#toc_5">为什么要设定损失函数</a>
</li>
</ul>
</li>
<li>
<a href="#toc_6">数值微分</a>
<ul>
<li>
<a href="#toc_7">导数</a>
</li>
<li>
<a href="#toc_8">数值微分的例子</a>
</li>
<li>
<a href="#toc_9">偏导数</a>
</li>
</ul>
</li>
<li>
<a href="#toc_10">梯度</a>
<ul>
<li>
<a href="#toc_11">梯度法</a>
</li>
<li>
<a href="#toc_12">神经网络的梯度</a>
</li>
</ul>
</li>
<li>
<a href="#toc_13">学习算法的实现</a>
<ul>
<li>
<a href="#toc_14">2层神经网络的类</a>
</li>
<li>
<a href="#toc_15">mini-batch的实现</a>
</li>
</ul>
</li>
</ul>
</li>
</ul>

<p>本章标题所说的“学习”是指从训练数据中自动获取最优权重参数的过程。学习的目的就是以损失函数为基准，找出能使它的值达到最小的权重参数。</p>

<h2 id="toc_0">从数据中学习</h2>

<p>神经网络的特征就是可以从数据中学习。<strong>所谓“从数据中学习”，是指可以由数据自动决定权重参数的值</strong>。</p>

<p>数据是机器学习的命根子。数据是机器学习的核心。这种数据驱动的方法，也可以说脱离了过往以人为中心的方法。</p>

<p>而机器学习的方法是极力避免人为介入的，尝试从收集到的数据中发现答案（模式）。<strong>神经网络或深度学习则比以往的机器学习方法更能避免人为介入</strong>。</p>

<p>例如手写数字识别，考虑通过有效利用数据来解决这个问题：先从图像中提取特征量，再用机器学习技术学习这些特征量的模式。“特征量”是指可以从输入数据（输入图像）中准确地提取本质数据（重要的数据）的转换器。图像的特征量通常表示为向量的形式。在计算机视觉领域，常用的特征量包括SIFT、SURF和HOG等。使用这些特征量将图像数据转换为向量，然后对转换后的向量使用机器学习中的SVM、KNN等分类器进行学习。</p>

<p>机器学习的方法中，由机器从收集到的数据中找出规律性。但是，<strong>将图像转换为向量时使用的特征量仍是由人来设计的</strong>。即使使用特征量和机器学习的方法，也需要针对不同的问题人工考虑合适的特征量。</p>

<blockquote>
<p>深度学习有时也称为端到端机器学习。</p>
</blockquote>

<p>神经网络的优点是对所有的问题都可以用同意的流程来解决。神经网络都是通过不断地学习所提供的数据，尝试发现带求解问题的模式。也就是说，与待处理的问题无关，神经网络可以 将数据直接作为原始数据，进行“端对端”的学习。</p>

<p>机器学习中，一般将数据分为<strong>训练数据</strong>和<strong>测试数据</strong>两部分来进行学习和实验等。未来正确评价模型的<strong>泛化能力</strong>，就必须划分训练数据和测试数据，训练数据也可以成为监督数据。</p>

<p>泛化能力是指处理未被观察过的数据（不包含在训练数据中的数据）的能力。获得泛化能力是机器学习的最终目标。</p>

<p>只对某个数据集过度拟合的状态称为<strong>过拟合（over fitting）</strong>。避免过拟合也是机器学习的一个重要课题。</p>

<h2 id="toc_1">损失函数</h2>

<p>神经网络以某个指标为线索寻找最优权重参数。神经网络的学习中所用的指标称为<strong>损失函数</strong>。这个损失函数可以使用任意函数，但<strong>一般用均方误差和交叉熵误差</strong>等。损失函数是表示神经网络性能的“恶劣程度”或者“性能有多好”的指标，即当前的神经网络对监督数据在多大程度上不拟合，在多大程度上不一致。</p>

<h3 id="toc_2">均方误差</h3>

<p>均方误差（mean squared error）由下式表示:</p>

<p>\[E = \frac{1}{2} \sum_{k}^{}(y_{k}-t_{k})^2\]</p>

<p>\(y_{k}\)表示神经网络的输出，\(t_{k}\)表示监督数据，\(k\)表示数据的维度。其中，\(t\)表示监督数据，将正确的解标签设为1，其他均设为0，<strong>其他标签表示为0的表示方法称为one-hot表示</strong>。</p>

<p>均方误差会计算神经网络的输出和正确解监督数据的各个元素之差的平方，再求总和。以下是代码实现：</p>

<pre><code class="language-python"># 均方误差定义函数
def mean_squared_error(y, t):
    return 0.5 * np.sum((y-t)**2)

# 设“2”为正解
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]

#例1：“2”的概率最高的情况（0.6）
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
mean_squared_error(np.array(y),np.array(t))
</code></pre>

<blockquote>
<p>0.09750000000000003</p>
</blockquote>

<pre><code class="language-python"># 例2：“7”的概率最高的情况（0.6）
y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]

mean_squared_error(np.array(y),np.array(t))
</code></pre>

<blockquote>
<p>0.5975</p>
</blockquote>

<p>很明显，均方误差显示第一个例子的输出结果与监督数据更加吻合。</p>

<h3 id="toc_3">交叉熵误差</h3>

<p>交叉熵误差（cross entropy error）由下式表示：</p>

<p>\[E=-\sum_{k}^{} t_{k} ln y_{k}\]</p>

<p>\(y_{k}\)是神经网络的输出，\(t_{k}\)是正确解标签。并且，中只有正确解标签的索引为1，其他均为0（one-hot表示）。交叉熵误差的值是由正确解标签所对应的输出结果决定的。</p>

<p>正确解标签对应的输出越大，上式的值越接近0；当输出为1时，交叉熵误差为0。此外，如果正确解标签对应的输出较小，则上式的值较大。以下是代码实现：</p>

<pre><code class="language-python"># 实现交叉熵误差
def cross_entropy_error0(y, t):
    delta = 1e-7
    return -np.sum(t * np.log(y + delta))
</code></pre>

<p>说明一下，函数内部在计算<code>np.log</code>时，加上了一个微小值delta。这是因为当出现<code>np.log(0)</code>时，<code>np.log(0)</code>会变为负无穷大的-inf，导致计算无法进行，所以作为保护性对策增加微小值。</p>

<pre><code class="language-python"># 进行简单计算
# 设“2”为正解
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
#例1：“2”的概率最高的情况（0.6）
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
cross_entropy_error0(np.array(y),np.array(t))
</code></pre>

<blockquote>
<p>0.510825457099338</p>
</blockquote>

<pre><code class="language-python"># 例2：“7”的概率最高的情况（0.6）
y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]
cross_entropy_error0(np.array(y),np.array(t))
</code></pre>

<blockquote>
<p>2.302584092994546</p>
</blockquote>

<p>上例结果可以看出与前文讨论是一致的。</p>

<h3 id="toc_4">mini-batch学习</h3>

<p>机器学习使用训练数据进行学习，严格来说，就是针对训练数据计算损失函数的值，找出使该值尽可能小的参数。因此，<strong>计算损失函数时必须将所有的训练数据作为对象</strong>。</p>

<p>以交叉熵误差为例，如果要求所有训练数据的损失函数的总和，可用如下式子：</p>

<p>\[E=-\frac{1}{N} \sum_{n}^{} \sum_{k}^{} t_{nk} ln y_{nk}\]</p>

<p>上式假设数据有\(N\)个，\(t_{nk}\)表示第\(n\)个数据的第\(k\)个元素的值（\(y_{nk}\)是神经网络的输出，\(t_{nk}\)是监督数据）。本式只是把求单个数据的损失函数的式子扩大到了\(N\)份数据。最后除以\(N\)进行正规化（归一化？），并求出单个数据的“平均损失函数”。</p>

<p>我们可以知道，许多数据分析是很大的数据量，这种情况不可能以全部数据为对象计算损失函数。因此，从全部数据选出一部分，作为全部数据的“近似”。神经网络的学习也是从训练数据中选出一批数据（称为mini-batch，小批量），然后对每个mini-batch进行学习。这种学习方式称为<strong>mini-batch学习</strong>。撰写读入MNIST数据集的代码：</p>

<pre><code class="language-python"># 读入MNIST 代码暂略
import sys, os
sys.path.append(os.pardir)
import numpy as np

# from dataset.mnist import load_mnist

# 使用np.random.choice进行随机选取
np.random.choice(60000, 10)
</code></pre>

<blockquote>
<p>array([ 5332, 11993, 16553, 47954, 31537,   4750, 52005,  1159,  6775,<br/>
           46043])</p>
</blockquote>

<p>实现一个可以同时处理单个数据和批量数据的交叉熵误差函数：</p>

<pre><code class="language-python"># 可同时处理单个和批量数据
def cross_entropy_error1(y, t):
    if y.nidm == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)

    delta = 1e-7
    batch_size = y.shape[0]
    return -np.sum(t * np.log(y + delta)) / batch_size
</code></pre>

<p>当监督数据是标签形式（非ont-hot表示，而是像“2”、“7”这样的具体标签）时，交叉熵误差可以通过如下代码实现：</p>

<pre><code class="language-python"># 可同时处理单个和批量数据
def cross_entropy_error2(y, t):
    if y.nidm == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)

    delta = 1e-7
    batch_size = y.shape[0]
    return -np.sum(np.log(y[np.arange(batch_size), t] + delta)) / batch_size
</code></pre>

<p>由于one-hot表示中\(t\)为0的元素的交叉熵误差也为0，因此针对这些元素的计算可以忽略。换言之，如果可以获得神经网络在正确解标签处的输出，就可以计算交叉熵误差。因此，\(t\)为one-hot表示时通过 <code>t * np.log(y)</code>计算的地方，在\(t\)为标签形式时，可用<code>np.log(y[np.arange(batch_size), t])</code>实现相同的处理。</p>

<p>介绍下<code>np.log(y[np.arange(batch_size), t])</code>：<code>np.arange(batch_size</code>会生产一个从0到batch_size-1的数组。因为t中标签是以<code>[2, 7, 0, 9, 4]</code>的形式存储的，所以<code>y[np.arange(batch_size), t</code>能抽出各个数据的正确解标签对应的神经网络的输出。</p>

<h3 id="toc_5">为什么要设定损失函数</h3>

<p>Q: 为什么要导入损失函数？既然我们的目标是获得识别精度尽可能高的神经网络，那不是应该把识别精度作为指标吗？</p>

<p>A: 在神经网络的学习中，寻找最优参数（权重和偏置）时，要寻找使损失函数的值尽可能小的参数。为了找到使损失函数的值尽可能小的地方，需要计算的参数的导数（确切的讲是梯度），然后以这个导数为指引，逐步更新参数的值。而对权重参数的损失函数求导，表示的是“如果稍微改变这个权重参数的值，损失函数的值会如何变化”。<strong>如果导数的值为负</strong>如，通过使该权重参数向正方向改变，可以减小损失函数的值；反过来，<strong>如果导数的值为正</strong>如，则通过使该权重参数向负方向改变，可以减小损失函数的值。<strong>当导数的值为0时</strong>如，无论权重参数向哪个方向变化，损失函数的值都不会改变，此时该权重参数的更新会停在此处。</p>

<p>总结一下：<strong>在进行神经网络的学习时，不能将识别精度作为指标。因为如果以识别精度为指标，则参数的导数在绝大多数地方都会变成0</strong>。</p>

<p>可以说，识别精度对微小的参数变化基本上没有什么反应，即便有反应，它的值也是不连续地、突然地变化。作为激活函数的阶跃函数也有同样的情况。出于相同的原因，如果使用阶跃函数作为激活函数，神经网络的学习将无法进行。众所周知，阶跃函数的斜率在绝大多数地方都为0，而sigmoid函数的斜率（切线）在任何地方都不为0。</p>

<h2 id="toc_6">数值微分</h2>

<p>梯度法使用梯度的信息决定前进的方向。</p>

<h3 id="toc_7">导数</h3>

<p>导数是某个瞬间的变化量。</p>

<p>Python中的舍入误差：</p>

<pre><code class="language-python">np.float32(1e-50)
</code></pre>

<blockquote>
<p>0.0</p>
</blockquote>

<pre><code class="language-python"># 对舍入误差减小与使用中心差分实现函数导数程序
def numerical_diff(f, x):
    h = 1e-4 #0.0001
    return (f(x+h) - f(x-h)) / (2*h)
</code></pre>

<p>利用微小的差分求导数的过程称为数值微分（numerical differentiation）。而基于数学式的推倒求导数的过程，则用解析性（analytic）一词，称为解析性求解或者解析性求导。</p>

<h3 id="toc_8">数值微分的例子</h3>

<pre><code class="language-python"># 实现例子
def function_1(x):
    return 0.01*x**2 + 0.1*x

import numpy as np
import matplotlib.pylab as plt

x = np.arange(0.0, 20.0, 0.1) #以0.1为单位，从0到20的数组x
y = function_1(x)
plt.xlabel(&quot;x&quot;)
plt.ylabel(&quot;y&quot;)
plt.plot(x, y)
plt.show()
</code></pre>

<blockquote>
<p><img src="media/15691207315042/output_118_0.png" alt="output_118_0"/></p>
</blockquote>

<pre><code class="language-python">#计算上面式子的5,10处导数
numerical_diff(function_1, 5)
</code></pre>

<blockquote>
<p>0.1999999999990898</p>
</blockquote>

<pre><code class="language-python">numerical_diff(function_1, 10)
</code></pre>

<blockquote>
<p>0.2999999999986347</p>
</blockquote>

<pre><code class="language-python">def tangent_line(f, x):
    d = numerical_diff(f, x)
    print(d)
    y = f(x) - d*x
    return lambda t: d*t + y
</code></pre>

<pre><code class="language-python">x = np.arange(0.0, 20.0, 0.1)
y = function_1(x)
plt.xlabel(&quot;x&quot;)
plt.ylabel(&quot;f(x)&quot;)

tf = tangent_line(function_1, 5)
tf2 = tangent_line(function_1, 10)
y2 = tf(x)
y22 = tf2(x)

plt.plot(x, y)
plt.plot(x, y2)
plt.show()
plt.plot(x, y)
plt.plot(x, y22)
plt.show()
</code></pre>

<blockquote>
<p>0.1999999999990898<br/>
   0.2999999999986347</p>

<p><img src="media/15691207315042/output_122_1.png" alt="output_122_1"/></p>

<p><img src="media/15691207315042/output_122_2.png" alt="output_122_2"/></p>
</blockquote>

<h3 id="toc_9">偏导数</h3>

<p>以下代码实现函数\(f(x_{0},x_{1}) = x^2_{0} + x^2_{1}\)：</p>

<pre><code class="language-python"># 实现上式的代码
def function_2(x):
    #或者return np.sum(x**2)
    return x[0]**2 + x[1]**2
</code></pre>

<pre><code class="language-python"># 求偏导1
def function_tmp1(x0):
    return x0*x0 + 4.0**2.0

numerical_diff(function_tmp1, 3.0)
</code></pre>

<blockquote>
<p>6.00000000000378</p>
</blockquote>

<pre><code class="language-python"># 求偏导2
def function_tmp2(x1):
    return 3.0**2.0 + x1*x1

numerical_diff(function_tmp1, 4.0)
</code></pre>

<blockquote>
<p>7.999999999999119</p>
</blockquote>

<p>如上两式，偏导数和单变量导数一样，都是求某个地方的斜率。不过，偏导数需要将多个变量中的某一个变量定位目标变量，并将其他变量固定位某个值。</p>

<h2 id="toc_10">梯度</h2>

<p>像\((\frac{\partial f}{\partial x_{0}},\frac{\partial f}{\partial x_{1}})\)这样的由全部变量的偏导数汇总而成的向量称为梯度（gradient）。</p>

<pre><code class="language-python"># 梯度的代码实现
def numerical_gradient(f, x):
    h = 1e-4 #0.001
    grad = np.zeros_like(x) #生成和x形状相同的数组
    
    for idx in range(x.size):
        tmp_val = x[idx]
        # f(x+h)的计算
        x[idx] = tmp_val + h
        fxh1 = f(x)
        
        # f(x-h)的计算
        x[idx] = tmp_val - h
        fxh2 = f(x)
        
        grad[idx] = (fxh1 - fxh2) / (2*h)
        x[idx] = tmp_val # 还原值
        
    return grad
</code></pre>

<p>上面的函数的实现看上去有些复杂，但它执行的处理和求单变量的数值微分基本没有区别。</p>

<pre><code class="language-python"># 求点(3,4) (0,2) (3,0)处的梯度
numerical_gradient(function_2, np.array([3.0, 4.0]))
</code></pre>

<blockquote>
<p>array([6., 8.])</p>
</blockquote>

<pre><code class="language-python">numerical_gradient(function_2, np.array([0.0, 2.0]))
</code></pre>

<blockquote>
<p>array([0., 4.])</p>
</blockquote>

<pre><code class="language-python">numerical_gradient(function_2, np.array([3.0, 0.0]))
</code></pre>

<blockquote>
<p>array([6., 0.])</p>
</blockquote>

<pre><code class="language-python">import numpy as np
import matplotlib.pylab as plt
from mpl_toolkits.mplot3d import Axes3D


def _numerical_gradient_no_batch(f, x):
    h = 1e-4 # 0.0001
    grad = np.zeros_like(x)
    
    for idx in range(x.size):
        tmp_val = x[idx]
        x[idx] = float(tmp_val) + h
        fxh1 = f(x) # f(x+h)
        
        x[idx] = tmp_val - h 
        fxh2 = f(x) # f(x-h)
        grad[idx] = (fxh1 - fxh2) / (2*h)
        
        x[idx] = tmp_val # 还原值
        
    return grad


def numerical_gradient(f, X):
    if X.ndim == 1:
        return _numerical_gradient_no_batch(f, X)
    else:
        grad = np.zeros_like(X)
        
        for idx, x in enumerate(X):
            grad[idx] = _numerical_gradient_no_batch(f, x)
        
        return grad


def function_2(x):
    if x.ndim == 1:
        return np.sum(x**2)
    else:
        return np.sum(x**2, axis=1)


def tangent_line(f, x):
    d = numerical_gradient(f, x)
    print(d)
    y = f(x) - d*x
    return lambda t: d*t + y
     
if __name__ == &#39;__main__&#39;:
    x0 = np.arange(-2, 2.5, 0.25)
    x1 = np.arange(-2, 2.5, 0.25)
    X, Y = np.meshgrid(x0, x1)
    
    X = X.flatten()
    Y = Y.flatten()
    
    grad = numerical_gradient(function_2, np.array([X, Y]) )
    
    plt.figure()
    plt.quiver(X, Y, -grad[0], -grad[1],  angles=&quot;xy&quot;,color=&quot;#666666&quot;)#,headwidth=10,scale=40,color=&quot;#444444&quot;)
    plt.xlim([-2, 2])
    plt.ylim([-2, 2])
    plt.xlabel(&#39;x0&#39;)
    plt.ylabel(&#39;x1&#39;)
    plt.grid()
    plt.legend()
    plt.draw()
    plt.show()
</code></pre>

<blockquote>
<p><img src="media/15691207315042/output_133_0.png" alt="output_133_0"/></p>
</blockquote>

<p>我们发现离最低处越远，箭头越大。梯度会指向各点处的函数值降低的方向，更严格的讲，梯度指示的方向是各点处的函数值减小最多的方向。</p>

<h3 id="toc_11">梯度法</h3>

<p>神经网络需在学习时找到最优参数（权重和偏置），这里所说的最优参数是指损失函数取最小值时的参数。通过巧妙地使用梯度来寻找函数最小值（或者尽可能小的值）的方法就是梯度法。</p>

<p>梯度表示的是各点处的函数值减小最多的方向。实际上，在复杂的函数中，梯度指示的方向基本上都不是函数值的最小处。</p>

<blockquote>
<p>函数的极小值、最小值以及被称为<strong>鞍点（saddle point）</strong>的地方，梯度为0。极小值是局部最小值，也就是限定在某个范围内的最小值。鞍点是从某个方向上看是极大值，从另一个方向上看则是极小值的点。</p>

<p>当函数很复杂且呈扁平状时，学习可能会进入一个（几乎）平坦的地区，陷入被称为“学习高原”的无法前进的停滞期。</p>
</blockquote>

<p>在寻找函数的最小值（或者尽可能小的值）的位置的任务中，要以梯度的信息为线索，决定前进的方向。</p>

<p>在梯度法中，函数的取值从当前位置沿着梯度方向前进一定距离，然后在新的地方重新求梯度，再沿着新梯度方向前进，如此反复，不断地沿梯度方向前进。像这样，<strong>通过不断地沿梯度方向前进，逐渐减小函数值的过程就是梯度法（gradient method）</strong>。</p>

<p>严格的讲，寻找最小值的梯度法称为<strong>梯度下降法（gradient descent method）</strong>，寻找最大值的梯度法称为<strong>梯度上升法（gradient ascent method）</strong>。</p>

<p>用数学式表示梯度法：</p>

<p>\[x_{0}=x_{0}-\eta \frac{\partial f}{\partial x_{0}}\]</p>

<p>\[x_{1}=x_{1}-\eta \frac{\partial f}{\partial x_{1}}\]</p>

<p>上式的\(\eta\)表示更新量，在神经网络的学习中，称为<strong>学习率（learning rate）</strong>。学习率决定在一次学习中，应该学习多少，以及在多大程度上更新参数。上式表示更新一次的式子，这个步骤会反复执行。</p>

<p>像学习率这样的参数成为<strong>超参数</strong>。学习率这样的超参数是人工设定的。一般来说，超参数需要尝试多个值，以便找到一种可以使学习顺利进行的设定。</p>

<p>而学习率需要事先确定为某个值，比如0.01或者0.001.一般而言，这个值过大或过小，都无法抵达一个“好的位置”。在神经网络的学习中，一般会一边改变学习率的值，一遍确认学习是否正确进行了。下面用python实现梯度下降法：</p>

<pre><code class="language-python">def gradient_descent(f, init_x, lr=0.01, step_num=100):
    &quot;&quot;&quot;
    参数f是要进行最优化的函数，init_x是初始值，lr是学习率learning rate，step_num是梯度法的重复次数。
    numerical_gradient(f,x)会求函数的梯度，用该梯度乘以学习率得到的值进行更新操作，由step_num指定重复的次数。
    &quot;&quot;&quot;
    x = init_x
    x_history = []

    for i in range(step_num):
        x_history.append( x.copy() )

        grad = numerical_gradient(f, x)
        x -= lr * grad

    return x, np.array(x_history)
</code></pre>

<pre><code class="language-python"># 用梯度法求f(x0+x1)=x0^2+x1^2的最小值
def function_2(x):
    return x[0]**2 + x[1]**2

init_x = np.array([-3.0, 4.0])
x, x_history = gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)
</code></pre>

<pre><code class="language-python"># 用图像表示上面的函数梯度下降法的步骤
plt.plot( [-5, 5], [0,0], &#39;--b&#39;)
plt.plot( [0,0], [-5, 5], &#39;--b&#39;)
plt.plot(x_history[:,0], x_history[:,1], &#39;o&#39;)

plt.xlim(-3.5, 3.5)
plt.ylim(-4.5, 4.5)
plt.xlabel(&quot;X0&quot;)
plt.ylabel(&quot;X1&quot;)
plt.show()
</code></pre>

<blockquote>
<p><img src="media/15691207315042/output_138_0.png" alt="output_138_0"/></p>
</blockquote>

<p>学习率过大或者过小都无法得到好的结果。</p>

<pre><code class="language-python"># 学习率过大的例子：lr=10.0
init_x = np.array([-3.0, 4.0])
x, x_history = gradient_descent(function_2, init_x=init_x, lr=10.0, step_num=100)
x
</code></pre>

<blockquote>
<p>array([-2.58983747e+13, -1.29524862e+12])</p>
</blockquote>

<pre><code class="language-python"># 学习率过小的例子：lr=1e-10
init_x = np.array([-3.0, 4.0])
x, x_history = gradient_descent(function_2, init_x=init_x, lr=1e-10, step_num=100)
x
</code></pre>

<blockquote>
<p>array([-2.99999994,  3.99999992])</p>
</blockquote>

<p>上面实验可以看出，学习率过大的话，会发散成一个很大的值；反过来，学习率过小的话，基本上没怎么更新就结束了。也就是说，设定合适的学习率是一个很重要的问题。</p>

<h3 id="toc_12">神经网络的梯度</h3>

<pre><code class="language-python"># 一个简单的神经网络
# coding: utf-8
import sys, os
sys.path.append(&#39;../input/deeplearningfromscratch/deeplearningfromscratch&#39;)  # 为了导入父目录中的文件而进行的设定
import numpy as np
from common.functions import softmax, cross_entropy_error
from common.gradient import numerical_gradient


class simpleNet:
    def __init__(self):
        self.W = np.random.randn(2,3)

    def predict(self, x):
        return np.dot(x, self.W)

    def loss(self, x, t):
        z = self.predict(x)
        y = softmax(z)
        loss = cross_entropy_error(y, t)

        return loss
</code></pre>

<pre><code class="language-python">net = simpleNet()
print(net.W)

x = np.array([0.6, 0.9])
p = net.predict(x)
print(p)

print(np.argmax(p))

t = np.array([0, 0, 1]) # 正确解标签

f = lambda w: net.loss(x, t)
dW = numerical_gradient(f, net.W)

print(dW)
</code></pre>

<blockquote>
<p>[[ 0.61060921  0.39278321 -0.19230499]<br/>
    [-0.57160886  0.59945886 -0.26529107]]<br/>
   [-0.14808245  0.7751829  -0.35414496]<br/>
   1<br/>
   [[ 0.13852712  0.34874166 -0.48726878]<br/>
    [ 0.20779067  0.5231125  -0.73090317]]</p>
</blockquote>

<h2 id="toc_13">学习算法的实现</h2>

<p>神经网络的学习步骤：</p>

<p><strong>前提：</strong>神经网络存在合适的权重和偏置，调整权重和偏置以便拟合训练数据的过程称为“学习”。</p>

<ol>
<li><strong>mini-batch</strong>：从训练数据中随机选出一部分数据，这部分数据称为mini-batch。我们的目标是减小mini-batch的损失函数的值。</li>
<li><strong>计算梯度</strong>：为了减小mini-batch的损失函数的值，需要求出各个权重参数的梯度。梯度表示损失函数的值减小最多的方向。</li>
<li><strong>更新参数</strong>：将权重参数沿梯度方向进行微小更新。</li>
<li><strong>重复</strong>：重复第1、2、3步。</li>
</ol>

<p>这里使用的数据是随机选择的mini batch数据，所以又被称为<strong>随机梯度下降法（stochastic gradient descent）</strong>。这里的随机指的是随机选择的意思。</p>

<p>随机梯度下降法是“对随机选择的数据进行的梯度下降法”。深度学习的很多框架中，随机梯度下降法一般由一个名为<strong>SGD</strong>的函数来实现。SGD来源于随机梯度下降法的英文名称的首字母。</p>

<h3 id="toc_14">2层神经网络的类</h3>

<pre><code class="language-python">from common.functions import *
from common.gradient import numerical_gradient


class TwoLayerNet:

    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):
        # 初始化权重
        self.params = {}
        self.params[&#39;W1&#39;] = weight_init_std * np.random.randn(input_size, hidden_size)
        self.params[&#39;b1&#39;] = np.zeros(hidden_size)
        self.params[&#39;W2&#39;] = weight_init_std * np.random.randn(hidden_size, output_size)
        self.params[&#39;b2&#39;] = np.zeros(output_size)

    def predict(self, x):
        W1, W2 = self.params[&#39;W1&#39;], self.params[&#39;W2&#39;]
        b1, b2 = self.params[&#39;b1&#39;], self.params[&#39;b2&#39;]
    
        a1 = np.dot(x, W1) + b1
        z1 = sigmoid(a1)
        a2 = np.dot(z1, W2) + b2
        y = softmax(a2)
        
        return y
        
    # x:输入数据, t:监督数据
    def loss(self, x, t):
        y = self.predict(x)
        
        return cross_entropy_error(y, t)
    
    def accuracy(self, x, t):
        y = self.predict(x)
        y = np.argmax(y, axis=1)
        t = np.argmax(t, axis=1)
        
        accuracy = np.sum(y == t) / float(x.shape[0])
        return accuracy
        
    # x:输入数据, t:监督数据
    def numerical_gradient(self, x, t):
        loss_W = lambda W: self.loss(x, t)
        
        grads = {}
        grads[&#39;W1&#39;] = numerical_gradient(loss_W, self.params[&#39;W1&#39;])
        grads[&#39;b1&#39;] = numerical_gradient(loss_W, self.params[&#39;b1&#39;])
        grads[&#39;W2&#39;] = numerical_gradient(loss_W, self.params[&#39;W2&#39;])
        grads[&#39;b2&#39;] = numerical_gradient(loss_W, self.params[&#39;b2&#39;])
        
        return grads
        
    def gradient(self, x, t):
        W1, W2 = self.params[&#39;W1&#39;], self.params[&#39;W2&#39;]
        b1, b2 = self.params[&#39;b1&#39;], self.params[&#39;b2&#39;]
        grads = {}
        
        batch_num = x.shape[0]
        
        # forward
        a1 = np.dot(x, W1) + b1
        z1 = sigmoid(a1)
        a2 = np.dot(z1, W2) + b2
        y = softmax(a2)
        
        # backward
        dy = (y - t) / batch_num
        grads[&#39;W2&#39;] = np.dot(z1.T, dy)
        grads[&#39;b2&#39;] = np.sum(dy, axis=0)
        
        da1 = np.dot(dy, W2.T)
        dz1 = sigmoid_grad(a1) * da1
        grads[&#39;W1&#39;] = np.dot(x.T, dz1)
        grads[&#39;b1&#39;] = np.sum(dz1, axis=0)

        return grads
</code></pre>

<pre><code class="language-python"># 二层神经网络例子
net = TwoLayerNet(input_size = 784, hidden_size = 100, output_size = 10)
print(net.params[&#39;W1&#39;].shape)
print(net.params[&#39;b1&#39;].shape)
print(net.params[&#39;W2&#39;].shape)
print(net.params[&#39;b2&#39;].shape)
</code></pre>

<blockquote>
<p>(784, 100)<br/>
   (100,)<br/>
   (100, 10)<br/>
   (10,)</p>
</blockquote>

<pre><code class="language-python">#推理处理的实现如下
x = np.random.rand(100, 784) # 伪输入数据100笔
y = net.predict(x)
t = np.random.rand(100, 10) # 伪正确解标签10笔

# grads = net.numerical_gradient(x, t) # 计算梯度，使用传统的基于数值微分计算参数的梯度
grads = net.gradient(x, t) # 计算梯度，使用误差反向传播算法

print(grads[&#39;W1&#39;].shape)
print(grads[&#39;b1&#39;].shape)
print(grads[&#39;W2&#39;].shape)
print(grads[&#39;b2&#39;].shape)
</code></pre>

<blockquote>
<p>(784, 100)<br/>
   (100,)<br/>
   (100, 10)<br/>
   (10,)</p>
</blockquote>

<p>解析<code>TwoLayerNet</code>的实现：</p>

<ol>
<li><p><code>__init__(self, input_size, hidden_size, output_size)</code>：类的初始化方法，参数依次表示输入层的神经元数、隐藏层的神经元数、输出层的神经元数。</p>
<p>因为进行手写数字识别时，输入图像的大小是784(28x28)，输出为10个类别，所以指定参数<code>input_size = 784, output_size = 10</code>，将隐藏层的个数<code>, hidden_size</code>设置为一个合适的值即可（例如这里是100）。</p></li>
<li><p><code>predict(self, x)、accuracy(self, x, t)</code>同上一章神经网络的推理处理基本一致。</p></li>
<li><p><code>loss(self, x, t)</code>是计算损失函数值的方法。这个方法会基于<code>predict()</code>的结果和正确解标签，计算交叉熵误差。</p></li>
<li><p><code>numerical_gradient(self, x, t)</code>是计算各个参数的梯度，而<code>gradient(self, x, t)</code>是使用误差反向传播法高效计算梯度的方法。</p>
<p><code>numerical_gradient(self, x, t)</code>是基于数值微分计算参数的梯度。而<code>gradient(self, x, t)</code>是使用误差反向传播法高速计算梯度，其求到的梯度和数值微分的结果基本一致，且速度比前者快。</p></li>
</ol>

<p>如何设置权重参数的初始值是关系到神经网络是否成功学习的重要问题。权重使用符合高斯分布的随机数进行初始化，偏置使用0进行初始化。</p>

<h3 id="toc_15">mini-batch的实现</h3>

<p>所谓mini-batch的学习，就是从训练数据中随机选择一部分数据，再以这些数据为对象(mini-batch)，使用梯度法更新参数的过程。</p>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from dataset.mnist import load_mnist
# from two_layer_net import TwoLayerNet

# 读入数据
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)

network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

# 超参数
iters_num = 10000  # 适当设定循环的次数
train_size = x_train.shape[0]
batch_size = 100
learning_rate = 0.1

train_loss_list = []
train_acc_list = []
test_acc_list = []

# 平均每个epoch的重复次数
iter_per_epoch = max(train_size / batch_size, 1)

for i in range(iters_num):
    # 获取mini-batch
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    
    # 计算梯度
    #grad = network.numerical_gradient(x_batch, t_batch)
    grad = network.gradient(x_batch, t_batch)
    
    # 更新参数
    for key in (&#39;W1&#39;, &#39;b1&#39;, &#39;W2&#39;, &#39;b2&#39;):
        network.params[key] -= learning_rate * grad[key]
    
    # 记录学习过程
    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)
    
    # 计算每个epoch的识别精度
    if i % iter_per_epoch == 0:
        train_acc = network.accuracy(x_train, t_train)
        test_acc = network.accuracy(x_test, t_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)
        print(&quot;train acc, test acc | &quot; + str(train_acc) + &quot;, &quot; + str(test_acc))

# 绘制图形
markers = {&#39;train&#39;: &#39;o&#39;, &#39;test&#39;: &#39;s&#39;}
x = np.arange(len(train_acc_list))
plt.plot(x, train_acc_list, label=&#39;train acc&#39;)
plt.plot(x, test_acc_list, label=&#39;test acc&#39;, linestyle=&#39;--&#39;)
plt.xlabel(&quot;epochs&quot;)
plt.ylabel(&quot;accuracy&quot;)
plt.ylim(0, 1.0)
plt.legend(loc=&#39;lower right&#39;)
plt.show()
</code></pre>

<blockquote>
<p>train acc, test acc | 0.10441666666666667, 0.1028<br/>
train acc, test acc | 0.7994333333333333, 0.8037<br/>
train acc, test acc | 0.8809833333333333, 0.8833<br/>
train acc, test acc | 0.9003666666666666, 0.9024<br/>
train acc, test acc | 0.9098833333333334, 0.9125<br/>
train acc, test acc | 0.9164, 0.9178<br/>
train acc, test acc | 0.9214, 0.9244<br/>
train acc, test acc | 0.92525, 0.9277<br/>
train acc, test acc | 0.92875, 0.9321<br/>
train acc, test acc | 0.9321166666666667, 0.9337<br/>
train acc, test acc | 0.9348333333333333, 0.9358<br/>
train acc, test acc | 0.93755, 0.9384<br/>
train acc, test acc | 0.9400833333333334, 0.9407<br/>
train acc, test acc | 0.9424166666666667, 0.9422<br/>
train acc, test acc | 0.9445666666666667, 0.9431<br/>
train acc, test acc | 0.9466666666666667, 0.9459<br/>
train acc, test acc | 0.9480833333333333, 0.9479</p>

<p><img src="media/15691207315042/output_151_1.png" alt="output_151_1"/></p>
</blockquote>

<p>随着学习的进行，损失函数的值在不断减小。这是学习正常进行的信号，表示神经网络的权重参数在逐渐拟合数据。通过反复地向它浇灌（输入）数据，神经网络整在逐渐向最优参数靠近。</p>

<p>不过损失函数的值，严格的讲是“对训练数据的某个mini-batch的损失函数”的值。训练数据的损失函数值减小的结果是不能说明该神经网络在其他数据集上也一定能有同等程度的表现。</p>

<p>神经网络的学习中，必须确认是否能够知确实别数据意外的其他数据，即虽然训练数据中的内容能够被正确识别，但是不在训练数据的内容却无法被识别，这种现象为<strong>过拟合</strong>。</p>

<p><strong>epoch</strong>是一个单位，一个epoch表示学习中所有训练数据均被使用过一次时的更新次数。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[架构学习书摘总结（五）架构实战]]></title>
    <link href="https://zning.me/15600074562241.html"/>
    <updated>2019-06-08T23:24:16+08:00</updated>
    <id>https://zning.me/15600074562241.html</id>
    <content type="html"><![CDATA[
<p>最近阅读了一本架构方面的入门图书叫《从零开始学架构：照着做，你也能成为架构师》，部分内容比较不错，先做书摘总结，以便加深印象与未来回顾学习。</p>

<p>本文是该书第五部分，是书中第十六至二十章，主要介绍消息队列设计、互联网架构演进及模板、架构重构和开源系统等架构实际案例内容。</p>

<span id="more"></span><!-- more -->

<p>连续阅读，请点击如下链接：</p>

<ul>
<li><p><a href="./15511868502157.html">架构学习书摘总结（一）概念和基础部分</a></p></li>
<li><p><a href="./15517030839592.html">架构学习书摘总结（二）高性能架构模式</a></p></li>
<li><p><a href="./15521364926344.html">架构学习书摘总结（三）高可用架构模式</a></p></li>
<li><p><a href="./15599222898499.html">架构学习书摘总结（四）可扩展架构模式</a></p></li>
<li><p><a href="./15600074562241.html">架构学习书摘总结（五）架构实战</a></p>
<ul>
<li>
<a href="#toc_0">第十六章 消息队列设计实战</a>
</li>
<li>
<a href="#toc_1">第十七章 互联网架构演进</a>
</li>
<li>
<a href="#toc_2">第十八章 互联网架构模板</a>
</li>
<li>
<a href="#toc_3">第十九章 架构重构</a>
</li>
<li>
<a href="#toc_4">第二十章 开源系统</a>
</li>
<li>
<a href="#toc_5">其他相关摘要</a>
</li>
</ul>
</li>
</ul>

<h2 id="toc_0">第十六章 消息队列设计实战</h2>

<ol>
<li><p>需求：</p>
<ol>
<li>用户发微博，通过审核子系统、统计子系统、广告子系统、消息子系统等，系统间通过接口调用，每通知一个新系统，微博子系统就要涉及接口进行测试，效率很低，问题定位很麻烦。还有等级子系统给VIP用户发放奖励、通知客服子系统安排专属服务人员……开发人员不胜其烦。</li>
<li>根因在于架构上各业务子系统强耦合，而消息队列正好可以完成子系统的解耦。经过一分析二讨论三开会四汇报五审批等一系列操作后，消息队列系统立项。</li>
</ol></li>
<li><p>设计流程：</p>
<ol>
<li><p>识别复杂度：对架构师来说是一项挑战，如果经验不足，那么只能采取“排查法”，从不同角度逐一进行分析。对于此项目具体分析过程如下：</p>
<ol>
<li><strong>这个消息队列是否需要高性能：</strong>关注每秒数据，即TPS和QPS。再考虑系统的读写并不是完全平均的，设计的目标应该以峰值来计算。峰值一般取平均值的3倍。例如，一天平均每秒写入消息为115条，每秒读取消息数是1150条，则消息对类系统的TPS是345，QPS是3450。另外由于现在基数较低，因此系统的设计目标按照峰值的四倍来计算，即TPS为1380，QPS为13800。TPS为1380并不高，但QPS为13800已经比较高了，因此高性能读取是复杂度之一，但这个高性能要求相比Kafka等系统来说也不是很高。</li>
<li><strong>这个消息队列是否需要高可用：</strong>消息队列需要高可用性，包括消息写入、消息存储、消息读取都需要保证高可用性。</li>
<li><strong>这个消息队列是否需要高可扩展性：</strong>消息队列的功能很明确，基本无需扩展，因此可扩展性不是这个消息队列的复杂度关键。</li>
</ol>
<p>综合分析下来，消息队列的复杂性主要体现在几个方面：高性能消息读取、高可用消息写入、高可用消息存储、高可用消息读取。</p></li>
<li><p>设计备选方案：采用开源Kafka、集群+MySQL存储、集群+自研存储方案。</p></li>
<li><p>评估和选择备选方案</p>
<table>
<thead>
<tr>
<th>质量属性</th>
<th>引入Kafka</th>
<th>MySQL存储</th>
<th>自研存储</th>
</tr>
</thead>
<tbody>
<tr>
<td>性能</td>
<td>高</td>
<td>中</td>
<td>高</td>
</tr>
<tr>
<td>复杂度</td>
<td>低，基本开箱可用</td>
<td>中，MySQL存储和复制，方案只需要开发服务器集群就可以</td>
<td>高，自研存储方案复杂度很高</td>
</tr>
<tr>
<td>硬件成本</td>
<td>低</td>
<td>高，一个分区就4台机器</td>
<td>低，和Kafka一样</td>
</tr>
<tr>
<td>可运维性</td>
<td>低，无法融入现有的运维体系，且运维团队无Scala经验</td>
<td>高，可以融入现有运维体系，MySQL运维很成熟</td>
<td>高，可以融入现有运维体系，并且只需要维护服务器即可，无需维护MySQL</td>
</tr>
<tr>
<td>可靠性</td>
<td>高，程序开源方案</td>
<td>高，MySQL存储很成熟</td>
<td>低，自研存储系统可靠性在最初阶段难以保证</td>
</tr>
<tr>
<td>人力投入</td>
<td>低，开箱即用</td>
<td>中，只需要开发服务器集群</td>
<td>高，需要开发服务器集群和存储系统</td>
</tr>
</tbody>
</table>
<p>最终根据讨论与架构师决策，选择了备选方案2。</p></li>
<li><p>细化方案：在备选方案的基础上进一步细化。</p></li>
</ol></li>
</ol>

<h2 id="toc_1">第十七章 互联网架构演进</h2>

<ol>
<li>什么样的策略才是真正有效的技术发展策略这个问题让人困惑关键的原因在于不管是潮流派（紧跟技术潮流）、保守派（新技术抱有戒备心，稳定压倒一切），还是跟分派（跟着竞争对手的步子走），都是站在技术本身的角度来考虑问题的，正所谓“不识庐山真面目，只缘身在此山中”。因此，要想看到“庐山真面目”，只有跳出技术的范畴，从一个更广更高的角度——企业的业务发展来考虑这个问题。</li>
<li>企业的发展，归根结底就是业务的发展，而影响一个企业的发展主要有3个因素：市场、技术、管理。在这个铁三角当中，业务处于三角形的中心，毫不夸张的说，市场、技术、管理都是为了支撑企业业务的发展。</li>
<li>我们可以简单地将企业的业务分为两类：产品类和服务类。</li>
<li>对于产品类服务，<strong>技术创新推动业务发展</strong>。对于服务类服务，<strong>业务发展推动技术发展</strong>。两类不同关系的原因在于用户选择服务的根本驱动力与选择产品的不同。用户选择一个产品的根本驱动力是其“功能”，而用户选择一个服务的根本驱动力却为“规模”。</li>
<li>当“规模”成为业务的决定因素后，服务模式的创新就成为业务发展的核心驱动力，而产品只是为了完成服务而提供给用户使用的一个载体。</li>
<li>服务类的业务发展路径：提出一种创新的服务模式→吸引了一批用户→业务开始发展→吸引了更多用户→服务模式不断完善和创新→吸引越来越多的用户。</li>
<li>即使是产品类业务，在技术创新开创了一个新的业务后，后续的业务发展也会反向推动技术的发展。</li>
<li>对于架构师来说，判断业务当前和接下来一段时间的主要复杂度是什么是非常关键的。架构师具体应该按照基于业务发展阶段的标准进行判断，这也是为什么架构师必须具备业务理解能力的原因。</li>
<li>互联网业务具有“规模决定一切”的相同点。</li>
<li>互联网业务发展一般分为几个时期：初创期、快速发展期、竞争期、成熟期。不同时期的差别主要体现在两个方面：复杂性、用户规模。</li>
<li><p>互联网发展的第一个主要方向是“业务越来越复杂”。</p>
<p><strong>初创期</strong>业务重点在于创新，只有随着越来越多的用户的使用，通过快速迭代试错、用户的反馈等手段，不断地在实践中去完善，去继续创新。</p>
<p><strong>发展期</strong>主要是将原来不完善的业务逐渐完善，因此会有越来越多的新功能不断地加入系统中。对于绝大部分技术团队来说，这个阶段技术的核心工作是快速地实现各种需求，只有这样才能满足业务发展的需要。</p>
<p><strong>架构期</strong>进行架构调整，架构期可以用的手段很多，但归根结底可以总结为一个字“拆”，功能、数据库、服务器等什么地方都可以拆。</p>
<p><strong>竞争期</strong>对技术的要求更上一层楼的“快”了，新业务的创新给技术带来的典型压力就是新的系统会更多，同时，原有的系统也会拆得越来越多。两者合力的一个典型后果就是系统数量在原来的基础上又增加了很多。而系统越来越多到了一个临界点后就产生到了质变，即系统数量的量变带来了技术工作的质变。主要有重复造轮子、系统交互一团麻。这个时期技术工作的主要解决手段有平台化（存储平台化、数据库平台化、缓存平台化）、服务化（消息队列、服务框架等）。</p>
<p><strong>成熟期</strong>进行逐项优化。</p></li>
<li><p>互联网发展的第二个主要方向是“用户量越来越”大。主要影响体现在：性能要求越来越高、可用性要求越来越高。</p>
<p>性能：再简单的查询，再高的硬件配置，单台MySQL机器支撑的TPS和QPS最高也就是万级，低的可能是几千，高的也不过几万。分布式将会带来复杂度的大幅度上升。以MySQL为例，分布式MySQL要考虑分库分表、读写分离、复制、同步等很多问题。</p>
<p>可用性：除了口碑，可用性对收入的影响也会随着用户量增大而增大。1万用户宕机1小时，可能才损失了几千元，100万用户宕机10分钟，损失可能就是几十万元了。</p></li>
</ol>

<h2 id="toc_2">第十八章 互联网架构模板</h2>

<p><img src="media/15600074562241/2019-9-15.jpg" alt="2019-9-15"/></p>

<p>上图基本涵盖互联网技术公司的大部分技术点，不同的公司只是在具体的技术实现上稍有差异，但不会跳出这个框架的范畴。</p>

<ol>
<li><p>存储层技术</p>
<ol>
<li><p>SQL</p>
<p>SQL即我们通常所说的关系型数据(库)。随着互联网业务的发展，性能要求越来越高，必然面对一个问题：将数据拆分到多个数据库实例才能满足业务的性能需求（其实Oracle也一样，只是时间早晚的问题）。数据库拆分满足了性能的要求，但带来了复杂度的问题：数据如何拆分、数据如何组合。所以互联网公司流行的做法是业务发展到一定阶段后，就会将这部分功能独立成中间件，例如淘宝的TDDL。再后期会导致新的复杂度问题：数据库资源使用率不高，比较浪费以及各SQL集群分开维护，投入的维护成本越来越高。因此，实力雄厚的大公司此时一般都会在SQL集群上构建SQL存储平台，以对业务透明的形式提供资源分配、数据备份、迁移、容灾、读写分离、分库分表等一系列服务。例如，淘宝的UMP(Unified MySQL Platform)系统。</p></li>
<li><p>NoSQL</p>
<p>NoSQL不是No SQL，而是Not Only SQL，即NoSQL是SQL的补充。首先NoSQL在数据结构上与传统的SQL的不同，例如典型的Memcache的key-value结构、Redis的复杂数据结构、MongoDB的文档数据结构；其次，NoSQL无一例外地都会将性能作为自己的一大卖点。</p>
<p>NoSQL的这两个特点很好地弥补了关系数据库的不足，因此在互联网行业NoSQL的应用基本上是基础要求。</p>
<p>由于NoSQL方案一般自己本身就提供集群功能，因此NoSQL在刚才是应用时很方便，不像SQL分库分表那么复杂。而NoSQL发展到一定规模后，通常都会在NoSQL集群的基础之上再实现统一存储平台。同意存储平台主要实现资源动态按需动态分配、资源自动化管理、故障自动化处理等几个功能。当然要发展到这个阶段，一般也是大公司才会这么做，简单来说就是如果只有几十台NoSQL服务器，那么做存储平台的收益不大；但如果有几千台NoSQL服务器，那么NoSQL存储平台就能够产生很大的收益。</p></li>
<li><p>小文件存储</p>
<p>除了关系型的业务数据，互联网行业还有很多用于展示的数据。这些数据具有数据量小一般在1M以下、数据量巨大、访问量巨大的典型特点。Facebook 2013年就达到每天上传3.5亿张照片，访问量超过10亿的规模。和SQL和NoSQL不同的是，小文件存储不一定需要公司或者业务规模很大，基本上认为业务在起步阶段就可以考虑做小文件统一存储。得益于开源与大数据，开源方案基础上封装一个小文件存储平台不太难。例如HBASE、Hadoop、Hypertable、FastDFS等都可以作为小文件存储的底层平台，只需要在这些开源方案上再包装一下基本上就可以用了。</p></li>
<li><p>大文件存储</p>
<p>互联网行业的大文件主要分为两类：一类是业务上的大数据，例如YouTube的视频、电影；一类是海量的日志数据，例如各种访问日志、操作日志、用户轨迹日志等。大文件没有小文件那么多，但每个文件都很大。这块开源方案也很成熟了，所以大数据存储和处理可以选择Hadoop、HBASE、Storm、Hive等。</p></li>
</ol></li>
<li><p>开发层技术</p>
<ol>
<li><p>开发框架</p>
<p>互联网公司都会指定一个大的技术方向，然后使用统一的开发框架。例如，Java相关的开发框架SSH、SpringMVC、SpringCloud、Play，Ruby的Ruby on Rails，PHP的ThinkPHP，Python的Django等等。使用统一的开发框架能够解决上面提到的各种问题，大大提升组织和团队的开发效率。</p>
<p>对于框架的选择，有一个总的原则：<strong>优选成熟的框架，避免盲目追逐新技术</strong>。首先，成熟的框架资料文档齐备；其次，成熟的框架受众更广，招聘时更容易招到合适的人才；最后，成熟的框架更加稳定，不会出现大的变动，适合长期发展。</p></li>
<li><p>Web服务器</p>
<p>开发框架只是负责完成业务功能的开发，真正能够运行起来，给用户提供服务，还需要服务器配合。选择一个服务器主要和开发语言相关，例如Java有Tomcat、JBoss、Resin等，PHP/Python的用Nginx，当然最保险的就是用Apache了，什么语言都支持。有人要担心Apache性能之类的问题，其实不用过早担心这个，等到业务真的发展到Apache撑不住的时候再考虑切换也不迟，那时候你有的是钱，有的是人，有的是时间。</p></li>
<li><p>容器</p>
<p>传统的虚拟化技术是虚拟机，解决了跨平台的问题，但由于虚拟机太庞大，启动慢，运行时太占资源，在互联网行业并没有大规模地应用；而Docker的容器技术，虽然没有跨平台，但启动快，几乎不占资源，推出后立刻就火起来了。不要以为Docker只是一个虚拟化或容器技术，它将在很大程度上改变目前的技术形势：</p>
<ol>
<li>运维方式会发生革命性的变化：Docker启动快，几乎不占资源，随时启动停止，基于Docker打造自动化运维、智能化运维将成为主流方式。</li>
<li>设计模式会发生本质上的变化：启动一个新的容器实例代价如此低，将鼓励设计思路朝“微服务”的方向发展。</li>
</ol>
<p>例如，一个传统的网站包括登录注册、页面访问、搜索等功能，没有用容器的情况下，除非有特别大的访问量，否则这些功能开始时都是集成在一个系统里面的；有了容器技术以后，一开始就可以将这些功能按照服务的方式设计，避免后续访问量增大时又要重构系统。</p></li>
</ol></li>
<li><p>服务层技术</p>
<p>服务层的主要目标是为了降低系统间相互关联的复杂度。</p>
<ol>
<li><p>配置中心</p>
<p>配置中心就是集中管理各个系统的配置。将配置中心做成通用的系统有如下好处：</p>
<ol>
<li>集中配置多个系统，操作效率高。</li>
<li>所有配置都在一个集中的地方，检查方便，协作效率高。</li>
<li>配置中心可以实现程序化的规则检查，避免常见的错误。</li>
<li>配置中心相当于备份了系统的配置，当某些情况下需要搭建新的环境时，能够快速搭建环境和恢复业务。</li>
</ol>
<p>配置中心通过“系统标示+host+port”来标示唯一一个系统运行实例是常见的设计方法。</p></li>
<li><p>服务中心</p>
<p>服务中心未来解决跨系统依赖的配置和调度问题。其实现一般来说有两种方式：服务名字系统和服务总线系统。</p>
<p>服务名字系统（Service Name System），类似于DNS，其是为了将Service名称解析为“host+port+接口名称”，但是和DNS一样，真正发起请求的还是请求方。</p>
<p>服务总线系统（Service Bus System），类似于计算机总线，其为了直接又总线系统完成调用，服务请求方不需要直接和服务提供方交互。</p>
<p>两者简单对比如下：</p>
<table>
<thead>
<tr>
<th></th>
<th>服务总线系统</th>
<th>服务名字系统</th>
</tr>
</thead>
<tbody>
<tr>
<td>复杂度</td>
<td>设计更加复杂，要同时完成配置和调度功能，且本身高性能和高可用的设计也更加复杂。</td>
<td>设计简单，基本类似一个服务配置中心，如果要做调度，需要独立的SDK包。</td>
</tr>
<tr>
<td>可用性</td>
<td>可用性的关键，它故障后所有业务间的访问都鼓掌，影响较大，但因为服务总线主要做调度，可以部署两套或多套并行系统。</td>
<td>仅仅保存配置，屌用还是由服务请求方发起，可用性要求没那么高，即使故障，各系统也可以使用本地缓存配置继续完成调用。</td>
</tr>
<tr>
<td>灵活性</td>
<td>控制所有的调度和配置，可以做得非常灵活。</td>
<td>仅仅有配置，即使提供独立的SDK支持调度，灵活性也要差一点，毕竟SDK只能获取静态的配置信息。</td>
</tr>
<tr>
<td>实时性</td>
<td>系统完成实际的调度，可以做到非常实时，例如某个服务及机器故障后立刻剔除故障节点。</td>
<td>提供调度的SDK包，也需要定时更新配置，不能每次请求都去获取一下最新的配置，实时性一般，这个问题和DNS类似。</td>
</tr>
<tr>
<td>可维护性</td>
<td>服务总线系统的修改和升级只需要自己完成即可。</td>
<td>修改和升级大部分情况下要修改SDK包（例如，调度算法变更），修改SDK包要求所有系统应用新SDK包才能生效。</td>
</tr>
<tr>
<td>多语言支持</td>
<td>服务总线系统支持通用的HTTP和TCP协议，和语言无关。</td>
<td>服务名字系统提供的SDK包需要适配多个语言，这个工作量也不小。</td>
</tr>
</tbody>
</table></li>
<li><p>消息队列</p>
<p>消息队列系统基本功能的实现比较简单，但要做到高性能、高可用、消息时序性、消息事务性则比较难。</p></li>
</ol></li>
<li><p>网络层技术</p>
<ol>
<li><p>负载均衡</p>
<p>负载均衡就是将请求均衡的分配到多个系统上。其实现方式很多，可大可小，可以软件实现，也可以硬件实现。</p>
<p>DNS是最简单也是最常见的负载均衡方式，一般用来实现地理级别的均衡。DNS负载均衡的有点事通用、成本低，但缺点也明显：DNS缓存时间比较长；DNS不能感知后端服务器的状态，只能根据配置策略进行负载均衡，无法做到更加灵活的负载均衡策略。所以对于时延和故障敏感的业务，有一些公司自己实现了HTTP-DNS的功能，即使用HTTP协议实现一个私有的DNS系统，这样的方案与通用DNS优缺点正好相反。</p>
<p>Nginx&amp;LVS&amp;F5用于同一地点内及其级别的负载均衡，其中Nginx是软件的7层负载均衡，LVS是内核的4层负载均衡，F5是硬件做4层负载均衡。</p>
<p>软件和硬件的区别就在于性能，硬件远远高于软件，Nginx的性能是万级，一般的Linux服务器上装个Nginx大概能到5万/秒；LVS的性能是十万级，没有具体测试过，据说可以达到80万/秒；F5性能是百万级，从200万/秒-800万/秒都有。</p>
<p>4层和7层的区别在于协议和灵活性。Nginx支持HTTP、E-mail等协议，而LVS和F5是4层负载均衡，和协议无关，几乎所有应用都可以做，例如聊天、数据库等。</p></li>
<li><p>CDN</p>
<p>CDN是为了解决用户忘了访问时的”最后一公里“效应，即将内容缓存在离用户最近的地方，用户访问的是缓存的内容，而不是站点实时内容。CDN经过多年的发展，已经变成了一个很庞大的体系：分布式存储、全局负载均衡、网络重定向、流量控制等都属于CDN的范畴。</p></li>
<li><p>多机房</p>
<p>多机房设计最核心的因素就是如何处理时延带来的影响。常见策略有：</p>
<p><strong>同城多机房：</strong>可搭建私有高速网络，基本做到与同机房一样的效果。对业务影响较小，但投入较大，而且遇到地震水灾等严重自然灾害也是有很大风险。</p>
<p><strong>跨城多机房：</strong>机房间通过网络进行数据复制，但由于跨城网络时延问题业务上需要做一定的妥协和兼容，不需要数据的实时强一致性，保证最终一致性即可。这种方式实现简单，但和业务有很强的相关性，微博可以这样做，支付宝就不能这样做。</p>
<p><strong>跨国多机房：</strong>同跨城多机房，只是地理分布更远、时延更大。所以一般仅用于备份和服务本国用户。</p></li>
<li><p>多中心</p>
<p>多中心要求每个中心都同时对外提供服务，且业务能够自动在多中心之间切换，故障后不需要人工干预或很少干预就能自动恢复。</p>
<p>多中心设计的关键就在于”数据一致性“和”数据事务性“如何保证，但这两个难点都和业务紧密相关，不存在通用的解决放哪，需要基于业务的特性进行详细的分析和设计。正因为多中心设计的复杂性，不一定所有业务都能实现多中心，目前国内的银行、支付宝这类系统就没有完全实现多中心。不然也不会出现挖掘机一铲子下去，支付宝中断4小时的故障。</p></li>
</ol></li>
<li><p>用户层技术</p>
<ol>
<li><p>用户管理</p>
<p>第一个目标：SSO，单点登录，又叫统一登录。单点登录的技术实现手段较多，例如cookie、token等，最有名的开源方案为CAS。</p>
<p>第二个目标：授权登录。现在最流行的授权登录就是OAuth 2.0协议，基本上已经成为事实上的标准，如果要做开放平台，则最好用这个协议，私有协议漏洞多，第三方介入也麻烦。</p>
<p>用户管理面临的主要问题是用户数巨大，一般至少千万级，但实现起来并不难，主要是因为不同用户之间没有关联。</p></li>
<li><p>消息推送</p>
<p>消息推送根据不同的途经，分为短信、邮件、站内信、App推送。App目前主要分为iOS和Android推送，iOS系统比较规范和封闭，基本上只能使用苹果的APNS了；但Android在国外用GCM和APNS差别不大，但在国内GCM不能用，另外各个厂商都有自己定制的Android，消息推送实现也不完全一样。通常情况下，对于中小公司，如果不涉及敏感数据，则Android系统上推荐使用第三方推送服务，因为这些第三方是专业做推送服务的，消息到达率是有一定保证的。</p>
<p>如果涉及敏感数据，则需要自己实现消息推送，这时就有一定的技术挑战了。消息推送主要包含3个功能：设备管理（唯一标识、注册、注销）、连接管理和消息管理，技术上面临的主要挑战有：海量设备和用户管理、连接保活、消息管理。</p></li>
<li><p>存储云与图片云</p>
<p>通常的实现都是”CDN+小文件存储“。由于图片业务的复杂性，普通的文件基本上提供存储和访问功能就够了，但图片涉及的业务可能包括裁剪、压缩、美化、审核、水印等处理，因此通常情况下图片云会拆分为独立的系统对用户提供服务。</p></li>
</ol></li>
<li><p>业务层技术</p>
<p>业务层面对的主要技术挑战是复杂性，而不管什么业务难题，用上”拆“问题一般都迎刃而解。究其原因，复杂性的一个主要原因就是系统越来越庞大，业务越来越多，降低复杂性最好的方式就是“拆”，化整为零、分而治之，将整体复杂性分散到多个子业务或子系统里面去。</p>
<p>而子系统数量过多，则需要“合”。按照“高内聚，低耦合”的原则，将职责关联比较强的子系统合成一个虚拟业务域，然后通过网关对外统一呈现，类似于设计模式中的Facade模式。</p></li>
<li><p>平台技术</p>
<ol>
<li><p>运维平台</p>
<p>运维平台核心的职责分为四大块：<strong>配置</strong>（资源管理：机器、IP、虚拟机）、<strong>部署</strong>（将系统发布到线上：包、灰度发布、回滚）、<strong>监控</strong>（收集系统上线运行后的相关数据并进行监控以便及时发现问题）、<strong>应急</strong>（系统出故障后的处理：停止程序、下线故障机器、切换IP）。</p>
<p>运维平台的核心设计要素：<strong>标准化、平台化、自动化、可视化</strong>。</p>
<p><strong>标准化：</strong>需要制定运维标准，规范配置管理、部署流程、监控指标、应急能力等，各系统按照运维标准来实现，避免不同的系统不同的处理方式。标准化是运维平台的基础，没有标准化就没有运维平台。</p>
<p><strong>平台化：</strong>在运维标准化的基础上，将运维的相关操作都集成到运维平台中，通过运维平台来完成运维工作。通过平台可将运维标准固化到平台中，且可以同简单方便的操作，可以复用支撑几百上千个业务系统。</p>
<p><strong>自动化：</strong>将重复操作固化，由系统自动完成。类似还有监控。</p>
<p><strong>可视化：</strong>提升数据查看效率，原理与汽车仪表盘类似。可直观地看到数据的相关属性，能够将数据的含义展示出来，能够将关联数据整合一起展示。</p></li>
<li><p>测试平台</p>
<p>核心职责是测试，包括单元测试、集成测试、接口测试、性能测试等。测试平台的核心目的是提升测试效率，从而提升产品质量，其设计关键就是自动化。为达到自动化，测试平台包括用例管理、资源管理、任务管理、数据管理等。</p></li>
<li><p>数据平台</p>
<p>核心职责包括数据管理、数据分析和数据应用。其中数据管理包括数据采集、数据存储、数据访问和数据安全四个核心职责，是数据平台的基础。数据分析包括数据统计、数据挖掘、机器学习、深度学习等几个细分领域。数据应用就广泛了，既包括在线业务，也包括利息按业务。例如推荐、广告等属于在线应用，报表、欺诈简称、异常检测等属于离线应用。</p></li>
<li><p>管理平台</p>
<p>核心职责是权限管理，无论业务系统（例如淘宝网）、中间件系统（例如消息队列Kafka），还是平台系统（例如运维平台）都需要进行管理。权限管理主要分为两部分：身份认证、权限控制。</p></li>
</ol></li>
</ol>

<h2 id="toc_3">第十九章 架构重构</h2>

<ol>
<li><p>系统架构是不断演化的，少部分架构演化可能需要推倒重来进行重写，但绝大部分的架构演化都是通过架构重构来实现的。架构重构对架构师的要求更高，主要体现在：</p>
<ol>
<li>业务已经上线，不能停下来</li>
<li>关联方众多，牵一发动全身</li>
<li>旧架构的约束</li>
</ol>
<p>因此架构重构对架构师的综合能力要求非常高，业务上要求架构师能够说服产品经理暂缓甚至暂停业务来进行架构重构；团队上需要架构师能够与其他团队达成一致的架构重构计划和步骤；技术上需要架构师给出让技术团队认可的架构重构方案。</p></li>
<li><p>有的放矢：期望通过架构重构来解决所有问题当然是不现实的，所以架构师的首要任务是从一大堆纷繁复杂的问题中识别出真正要通过架构重构来解决的问题，集中力量快速解决，而不是想着通过架构重构来解决所有的问题。尤其是对于刚接手一个新系统的架构师或技术主管来说，一定要控制住“新官上任三把火”的冲动，避免摊大饼式或运动式的重构和优化。架构师需要透过问题表象看到问题本质，找出真正需要通过架构重构解决的核心问题。</p></li>
<li><p>合纵连横：</p>
<ol>
<li>合纵：要想真正推动一个重构项目启动，需要花费大量的精力进行游说和沟通。一般技术人员谈到架构重构时，就会搬出一大堆技术术语，但如果和非技术人员这样沟通，效果如同鸡同鸭讲，没有技术背景的人员很难理解，甚至有可能担心我们是在忽悠人。所以在沟通协调时，将技术语言转换为通俗语言，以事实说话，以数据说话，是沟通的关键。例如某系统不用直接说可用性是几个9，而是整理线上故障的次数、每次影响的时长，影响的用户，客服的反馈意见等，然后再拿其他系统的数据进行对比，无论产品人员、项目人员，还是运营人员，明显就看出系统的可用性有问题了。</li>
<li>连横：有重构需要和其他相关或配合的系统的沟通协调。由于大家都是做技术的，所以这里沟通协调容易写，但是也有阻力，主要有“这对我有什么好处”和“这部分我这边现在不急”。前者会被误认为“不顾大局”，但其实首先这种拔高一般都比较虚，无法明确，不同人的理解不一样，无法达成共识；其次是如果对公司和部门有利，但对某个小组没用甚至不利，那么可能是因为目前的方案不够好，还可以考虑另外的方案。推动的有效策略是“换位思考、合作双赢、关注长期”，即站在对方角度思考，重构对他有什么好处，能够帮他解决什么问题，带来什么收益。当然有的时候有些情况需要协调更高层级的管理者才能推动，平级推动是比较难的。</li>
</ol></li>
<li><p>运筹帷幄：架构师在识别系统关键的复杂度问题后，还需要识别为了解决这个问题，需要做哪些准备事项，或者还要先解决哪些问题。其实就是<strong>分段实施</strong>，将要解决的问题根据优先级、重要性、实施难度等划分为不同的阶段，每个阶段聚焦于一个整体的目标，集中精力和资源解决一类问题。</p>
<p>制定分段实施策略有如下经验：</p>
<ol>
<li>划分优先级：将明显且有比较紧急的事项优先落地，解决目前遇到的主要问题。</li>
<li>问题分类：将问题按照性质分类，每个阶段集中解决一类问题。</li>
<li>先易后难：这点与很多人的直觉不太一样，有的人认为应该先攻克最难的问题，所谓的“擒贼先擒王”，解决最难的问题后其他问题就不在话下。这看起来很美好，但实际上不可行。采取先易后难的策略，能够很大程度上避免“先易后难”策略的问题。</li>
</ol></li>
<li><p>文武双全：项目管理+技术能力</p>
<p>真正的架构师，当然必须具备一定的项目经理技能，但更重要的还是技术能力，道理很简单：再好的饼，最后实现不了，都是空谈！“项目管理能力”是“文”的能力，“技术能力”是“武”的能力，架构师必须文武双全才能解决问题。</p>
<p>我们牢记架构设计的目的：<strong>架构设计的主要目的是为了解决系统的复杂性。</strong></p>
<p>某些系统关键问题是“不合理的耦合带来的复杂性”：将特定业务的数据和所有业务的公共数据耦合在一起，数据正确性难以保证，而且每次修改都是“牵一发动全身”，效率很高，所以重构的目标就是将“不合理的耦合”进行拆分。</p>
<p>而某些系统主要的问题就是有一个全局单点，一旦这个单点故障，就会导致所有业务全部不可用。所以我们重构的目标就是解决“全局唯一单点”的可用性问题。</p></li>
</ol>

<h2 id="toc_4">第二十章 开源系统</h2>

<ol>
<li>开源项目虽然节省了大量的人力和时间，但带来的踩坑（宕机、丢数据等）问题也不少。</li>
<li>架构师需要更加聪明的去选择和使用开源项目，即不要重复造轮子，但要找到合适的轮子。</li>
<li>选-如何选择一个开源项目：聚焦是否满足业务、聚焦是否成熟（可以看版本号、使用的公司数量、社区活跃度判断）、聚焦运维能力（可以从开源方案日志是否齐全、开源方案是否有命令行管理控制台等维护工具能看到系统运行时的情况、开源方案是否有故障检测和恢复的能力例如告警、倒换等）。</li>
<li>用-如何使用开源方案：
<ol>
<li>深入研究，仔细测试。可通过如下几个方面进行研究和测试：
<ol>
<li>通读开源项目的设计文档或白皮书，了解其设计原理。</li>
<li>核对没给配置项的作用和影响，识别出关键配置项。</li>
<li>进行多种场景的性能测试。</li>
<li>进行压力测试，连续跑几天，观察CPU、内存、磁盘I/O等指标波动。</li>
<li>进行故障测试，kill、断电、拔网线、重启100次以上、倒换等。</li>
</ol></li>
<li>小心应用，灰度发布：不管研究多深入、测试多仔细、自信心多爆棚，时刻对线上环境和风险要有敬畏之心，小心使得万年船。经验就是先在非核心的业务上应用，然后有经验后再慢慢扩展。</li>
<li>做好应急，以防万一：运气不好可能遇到一个之前全世界的使用者都从来没遇到的bug。</li>
</ol></li>
<li>改-如何基于开源项目做二次开发
<ol>
<li>保持纯洁，加以包装：建议不要改动原系统，而是要开发辅助系统：监控、告警、负载均衡、管理等。</li>
<li>发明你要的轮子：软件领域和硬件领域最大的不同就是前者没有绝对的工业标准，大家都很尽兴，想怎么玩就怎么玩。除此以外，开源项目未来能够大规模应用，考虑的是通用性的处理方案，而不同的业务其实差异较大，通用方案并不一定完美适合具体的某个业务。所以，如果你有钱有人有时间，投入人力去重复发明完美符合自己业务特点的轮子也是很好的选择。毕竟，很多财大气粗的公司（BAT们）都是这样做的，否则我们也就没有那么多好用的开源项目了。</li>
</ol></li>
</ol>

<h2 id="toc_5">其他相关摘要</h2>

<ol>
<li>实际中，不同的公司或团队，可能还有一些其他方面的复杂度分析。例如，金融系统可能需要考虑安全性，有的公司会考虑成本等。</li>
<li>通常情况下，成熟的团队不会轻易改变技术栈，反而是新成立的技术团队更加倾向于采用新技术。</li>
<li>架构师的技术储备越丰富，经验越多，备选方案也会更多，从而才能更好地涉及备选方案。</li>
<li>备选方案的选择和很多因素有关，并不单单考虑性能高低、技术是否优越这些纯技术因素。业务的需求特点、运维团队的经验、已有的技术体系、团队人员的技术水平都会影响备选方案的选择。</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[WAIC见闻与思考：AI应用落地热潮将至 深度学习的冬天要来？]]></title>
    <link href="https://zning.me/15673239600102.html"/>
    <updated>2019-09-01T15:46:00+08:00</updated>
    <id>https://zning.me/15673239600102.html</id>
    <content type="html"><![CDATA[
<p>8月29日-8月31日，2019第二届世界人工智能大会（WAIC）在上海召开，有幸能够近距离接触此次盛会，众多名家云集，深感看到的学到的东西很多，也深知自己的水平与各位高手望尘莫及，有很多需要努力的地方要紧跟脚步。</p>

<p>现就在WAIC中的见闻与思考做一些总结，以飨读者，也欢迎有不同看法的读者尽情留言讨论。</p>

<span id="more"></span><!-- more -->

<h2 id="toc_0">AI应用落地：热潮将至 机遇众多</h2>

<p>关于WAIC三天大会的精彩核心干货，我这里就不班门弄斧了，机器之心给各位都已经整理的非常全了（<a href="https://mp.weixin.qq.com/s/hoZGYibx8-p7HEPXp_ARlQ">第一天总结</a>、<a href="https://mp.weixin.qq.com/s/5885PH4Aj7oIsNb9vm261w">第二天总结</a>、<a href="https://mp.weixin.qq.com/s/_yM9s61zp8L3stBaJaL0Xg">第三天总结</a>）。这里我就我看到和听到的给出自己的分析与见解。</p>

<p>整体来看，WAIC整个会议的水平比日常各种技术分享活动要高很多，主要原因其实是AI不仅仅是IT工程师们的事业，更多的还有众多学科人才的加入，使得这个曾经是计算机及数学界独享的“高端”领域，逐步变为一处各个学科与各个行业都可以参与、使用与分享知识技术的天地。也由此使得AI领域发展到现在，已然成为了全人类憧憬下一个未来的新技术。</p>

<p><img src="media/15673239600102/IMG_5569.jpg" alt="IMG_5569" style="width:500px;"/></p>

<p><img src="media/15673239600102/IMG_5574.jpg" alt="IMG_5574" style="width:500px;"/></p>

<p><img src="media/15673239600102/IMG_5582.jpg" alt="IMG_5582" style="width:500px;"/></p>

<p>在第二天参加的“TensorFlow：智在，启无限”的宣讲论坛上，TensorFlow邀请了几位使用TensorFlow深度学习框架技术制作出应用的几个开发者同与会者分享（见上图，由上到下依次为安徽中医药大学胡继礼老师的《机器学习在中药饮片识别中的应用》、兰州大学武强老师的《DeepFlying——基于TensorFlow的飞天服饰创新》、Google AI研究员黄成之《通过机器学习让音乐创作简单易上手》）。其个中都有着超出技术本身的AI应用意味。而在WAIC召开期间，互联网上一个名叫“ZAO”的换脸APP也突然火了起来，这个以生成对抗网络等一系列深度学习算法为基础的AI应用出现所带给每个人的影响，不止是一个视频换脸或者隐私争议那么简单。<strong>更多的，由点及面的，我们看到的是每个人能够非常容易地开发或生成一个AI应用、每个人能够通过AI应用来获取到更便捷的生活方式和更丰富的文化娱乐情景。</strong></p>

<p><strong>这不禁令我们想到上一个改变我们生活的技术——互联网，而AI，极有可能是下一个与其有同样影响力的技术栈。</strong>而现在AI应用们的出现仅仅是个雏形，而且仅仅多是在文娱方面，在其他方面的发展仍是一片荒野——虽然仍有零星的绿洲存在。但无论现在是怎样的，未来AI应用走入现实生活的步伐将会越来越快，AI场景需求的多元化也会随着5G的商用推动物联网的发展而更加丰富。而什么是5G+AI+物联网场景下的“王者应用”呢？又有多少独角兽公司能够抓住这个时代的机遇迎风而上呢？我想在当下，每个人心中都有自己的答案。</p>

<p><img src="media/15673239600102/IMG_5615.jpg" alt="IMG_5615" style="width:500px;"/></p>

<p>第三天的开发者日是干货最为丰富的一天，关于NLP创新、AutoML、深度集成学习、Julia语言等内容，通过重磅嘉宾的分享掀起了会场内外的各种讨论。关于技术细节我这里也不班门弄斧了，只想说一个我注意到的细节：<strong>所有新技术的讨论都是在已有的框架基础上的革新，不论是效率上的提升、还是更好的易用性、更强大的功能，均是如此。</strong>我们可以看到近些年科研工作者和开发者们的努力，正让机器学习、神经网络与深度学习的未来应用前景更加明朗。</p>

<p>第三天下午会场中，我无意中逛到了WAIC的闭幕式会场，这个会场在日程表上压根就没有写（是的，你没看错，我至少翻了三遍参会指南都没看到）。而闭幕式上，包括市长应勇在内的市领导悉数到场，可见上海是十分重视AI的发展了。</p>

<p><em>(相比上海，最先发布各种关于人工智能政策文件的山东，一些东西还仍然在政策文件上，作为家乡人真的很为山东着急啊！)</em></p>

<p><img src="media/15673239600102/IMG_5624.jpg" alt="IMG_5624" style="width:500px;"/></p>

<p><img src="media/15673239600102/IMG_5625.jpg" alt="IMG_5625" style="width:500px;"/></p>

<p>当然，会上各种发榜与仪式，而关于上海市有关世界人工智能大会上发了哪些政策与名单，感兴趣的可以<a href="https://mp.weixin.qq.com/s/LMhjfT25dHJ-9FQXfyQFJA">点击这里查看官方的报道</a>。</p>

<h2 id="toc_1">深度学习的冬天：是噱头 还是危机信号？</h2>

<p>当然，这个节标题的起名跟WAIC本身会场议题没有关系。而是最近看到的CNCC 2019中第四天上午的一个会议议程。</p>

<p><img src="media/15673239600102/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202019-09-01%2016.35.14%E5%89%AF%E6%9C%AC.png" alt="屏幕截图 2019-09-01 16.35.14副本" style="width:500px;"/></p>

<p>说句实在话，这个议题着实让我对这个演讲和这个演讲者产生了浓厚的兴趣，甚至马上要掏出接近两千块钱来报名CNCC……（然而没钱）</p>

<p>不过仔细思索起来，这个议题也不是什么空穴来风，毕竟说句实在话，现在的深度学习是建立在神经网络的基础上，也就是说，深度网络可以理解为由多层且复杂的神经网络产生的模型，而通过深度网络进行的模型训练学习，即为深度学习。那么回溯技术源头，深度学习其实仅仅是将神经网络复杂化、规模化所创造的一个领域，其从神经网络的衍生完全取决于现代计算机性能与算力的提高。<strong>而深度学习本身并无特别多的新技术，只要机器多，深度大，绝大部分神经网络都可以成为深度学习的范畴。</strong></p>

<p>当然上面对深度学习的解释说法仅仅是一种通俗的说法，可能有些方面并不是特别准确。但是关于其依赖技术的无新颖性，我们可以深刻的想一想。深度学习本身贡献给神经网络领域的，也仅仅是其关于神经网络在发展到一定规模后如何高效的工作这些方面的算法，其在基本的算法方面，本身仍沿用“老一套”的算法底子。</p>

<p>这不禁让我们吸一口冷气。</p>

<p>而不仅是深度学习，包括机器学习、神经网络在内的很多人工智能领域基础学科，目前基础理论的研究情况不容乐观。<strong>一个是未见有更有创新性的算法在学科内出现，更多的是优化与改变；另一方面是神经网络对于人类仍然是一种黑盒，不知如何解释其成因是一个非常严肃的问题。</strong>人工智能从上世纪五十年代开始至今已然经历过至少两轮巨大的波折了，那么现在发展到这个程度的机器学习、神经网络、深度学习等人工智能领域，会不会到了现在这个阶段的天花板？AI跌落神坛的历史会不会重演？这一切都取决于这一代人的努力的情况。</p>

<p>不过，对于现阶段AI应用的落地，目前的技术已然足够。对于世界上的绝大部分人来说，现阶段AI的前景，至少还有十年期的发展机遇。然而，警惕仍不能放松，接下来人工智能领域基础学科的发展是什么样的，仍然是关键的风向标。可以说，<strong>如若AI应用落地能看到十年发展机遇，那么人工智能领域基础学科的发展，能让我们看到半个世纪后的未来</strong>。</p>

]]></content>
  </entry>
  
</feed>
